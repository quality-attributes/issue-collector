[
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jklaiho"
                },
                "number": 16068,
                "resourcePath": "/ansible/ansible/issues/16068",
                "state": "CLOSED",
                "publishedAt": "2016-05-31T12:00:14Z",
                "closedAt": "2019-03-25T11:40:21Z",
                "title": "Magic variable role_names to also list roles belonged to via dependencies?",
                "bodyText": "ISSUE TYPE\n\nFeature Idea\n\nANSIBLE VERSION\nansible 2.0.2.0 (not that relevant here though)\nCOMPONENT NAME\nmagic variables\nSUMMARY\nThe magic variable role_names, containing the list of all role names that the current target of the playbook belongs to, is quite useful in certain scenarios. For example, I use the same playbook to set up development vagrant VMs as well as test and production servers. A few tasks in certain roles are conditional on role membership, e.g. having when: \"'dev' in role_names\".\nHowever, it looks like roles that are included via role dependencies in meta/main.yml files are not included in the role_names list. You could make an argument either way, but it seems to me that they should be there for completeness' sake, since role membership via dependency is still a type of role membership.\nSeeing as role_names isn't really documented (\"role_names\" site:docs.ansible.com on Google returns nothing, at least), it probably isn't very widely used right now, and changing the behaviour probably won't break much. Is this something you could consider?"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jsmartin"
                },
                "number": 4578,
                "resourcePath": "/ansible/ansible/issues/4578",
                "state": "CLOSED",
                "publishedAt": "2013-10-18T02:51:50Z",
                "closedAt": "2014-09-29T20:46:26Z",
                "title": "SmartOS IP address fact",
                "bodyText": "On SmartOS, in contrast with the other OS's, it appears that the IP address object is an actual list of IPs.  Is that to be expected?\nfor example, in a smartos, I have to address it like this:\n{{ ansible_net1['ipv4'][0]['address'] }}\n\nwhere on linux\n{{ ansible_net1['ipv4']['address'] }}\n\nwill suffice."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tumbl3w33d"
                },
                "number": 22337,
                "resourcePath": "/ansible/ansible/issues/22337",
                "state": "CLOSED",
                "publishedAt": "2017-03-06T23:01:55Z",
                "closedAt": "2017-08-19T23:13:23Z",
                "title": "maven_artifact module should allow preserving the name of the downloaded artifact",
                "bodyText": "ISSUE TYPE\n\nFeature Idea\n\nCOMPONENT NAME\nmaven_artifact\nANSIBLE VERSION\nansible 2.3.0 (devel 3e4be156d7) last updated 2017/03/06 21:46:14 (GMT +200)\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nWhen downloading an artifact you can ask the module to determine the latest version automatically (it gets extracted from the repo's metadata). The artifact's name in the repository comes with a version string which currently gets overwritten with 'latest' when downloading.\nThe problem with this approach is that you sometimes need to know what version the downloaded artifact has for further processing, My use case is transferring the version to the RPM that I build from it.\nSTEPS TO REPRODUCE\nThe implementation should introduce an additional parameter which causes what I described above. This is only relevant for the constellation version: latest and dest being a directory, because else the filename chosen by dest or the version explicitly selected win.\n\n- hosts: localhost\n  gather_facts: no\n  tasks:\n    - maven_artifact:\n        version: latest\n        artifact_id: spring-core\n        group_id: org.springframework\n        dest: /tmp/\n        keep_name: yes\nEXPECTED RESULTS\nThe downloaded artifact contains the dynamically determined version string. In this case, the last version found in this list. At the moment of creating this issue this is /tmp/spring-core-4.3.7.RELEASE.jar\nACTUAL RESULTS\nThe downloaded artifact is renamed to /tmp/spring-core-latest.jar"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "zerkms"
                },
                "number": 10915,
                "resourcePath": "/ansible/ansible/issues/10915",
                "state": "CLOSED",
                "publishedAt": "2015-05-04T23:39:55Z",
                "closedAt": "2015-05-04T23:42:47Z",
                "title": "SSH connection is established for every command",
                "bodyText": "Here is the log of the run\n$ ansible-playbook deploy.yml -i hosts -vvvv -c ssh\n\nPLAY [app] ********************************************************************\n\nGATHERING FACTS ***************************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE setup\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=jzpkjtgucxrncjxcvemfmfxcxsgigepl] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-jzpkjtgucxrncjxcvemfmfxcxsgigepl; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname]\n\nTASK: [install app-pkg] ****************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE apt pkg=app-pkg state=latest\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=yyqqlkoftogxutbcuhdaczmvxpbeurqf] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-yyqqlkoftogxutbcuhdaczmvxpbeurqf; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname] => {\"changed\": false}\n\nTASK: [install nginx] *********************************************************\n<hostname> ESTABLISH CONNECTION FOR USER: xxxxxxxxx\n<hostname> REMOTE_MODULE apt pkg=nginx state=latest\n<hostname> EXEC ssh -C -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/me/.ansible/cp/ansible-ssh-%h-%p-%r\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=xxxxxxxxx -o ConnectTimeout=10 hostname /bin/sh -c 'sudo -k && sudo -H -S -p \"[sudo via ansible, key=hbtqwkkfqtfqkfdfcamgahpoycrmilqw] password: \" -u root /bin/sh -c '\"'\"'echo BECOME-SUCCESS-hbtqwkkfqtfqkfdfcamgahpoycrmilqw; LANG=C LC_CTYPE=C /usr/bin/python'\"'\"''\nok: [hostname] => {\"changed\": false}\n\nPLAY RECAP ********************************************************************\nhostname      : ok=3    changed=0    unreachable=0    failed=0\n\nThe deply.yml is as simple as\n\n---\n- hosts: app\n  vars: ~\n  remote_user: xxxxxxxxx\n  sudo: yes\n\nfollowed by 2 tasks to install packages via apt-get.\nThe OS on both machines is ubuntu trusty, the requiretty does not exist in sudoers (on the target machine) and setting Defaults !requiretty does not change anything.\nIn the ansible config I have made only these 2 changes:\n\n\nuncommented\nssh_args = -o ControlMaster=auto -o ControlPersist=60s\n\n\nenabled\npipelining = True\n\n\nIt's for ansible 1.9.1 installed from the PPA.\nWhat am I doing wrong?"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "claco"
                },
                "number": 7860,
                "resourcePath": "/ansible/ansible/issues/7860",
                "state": "CLOSED",
                "publishedAt": "2014-06-20T02:52:32Z",
                "closedAt": "2014-09-29T20:46:57Z",
                "title": "rax_identity stacktraces with pyrax >= 1.8.0",
                "bodyText": "Starting with pyrax 1.8.0 and greater installed, the rax_identity module now dies with a stacktrace trying to serialize the output:\nfailed: [localhost] => {\"failed\": true, \"parsed\": false}\ninvalid output was: Traceback (most recent call last):\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1438, in <module>\n    main()\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 110, in main\n    cloud_identity(module, state, pyrax.identity)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 82, in cloud_identity\n    module.exit_json(changed=changed, identity=instance)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1051, in exit_json\n    print self.jsonify(kwargs)\n  File \"/Users/claco/.ansible/tmp/ansible-tmp-1403232596.15-235494610913070/rax_identity\", line 1029, in jsonify\n    return json.dumps(data)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/__init__.py\", line 243, in dumps\n    return _default_encoder.encode(obj)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 207, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 270, in iterencode\n    return _iterencode(o, 0)\n  File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/json/encoder.py\", line 184, in default\n    raise TypeError(repr(o) + \" is not JSON serializable\")\nTypeError: <'load_balancer' Service object at 0x102bc23d0> is not JSON serializable\n\nAs a workaround, rolling back to < 1.8.0 fixes the problem."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ansible-bug-reporter"
                },
                "number": 24460,
                "resourcePath": "/ansible/ansible/issues/24460",
                "state": "CLOSED",
                "publishedAt": "2017-05-10T19:03:23Z",
                "closedAt": "2017-05-12T00:45:18Z",
                "title": "Force_handlers with_items does not execute handlers",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nForce handlers with_items\nANSIBLE VERSION\nansible 2.3.0.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = configured module search path = Default w/o overrides\npython version = 2.7.10 (default, May  1 2017, 19:24:18) [GCC 4.4.7 20120313 (Red Hat 4.4.7-16)]\nCONFIGURATION\nOS / ENVIRONMENT\nSUMMARY\nWe expect handlers to run when at least one item is changed when using with_items and force_handlers.\nSTEPS TO REPRODUCE\n---\n- hosts: localhost\n  gather_facts: no\n  force_handlers: True\n  handlers:\n    - name: hello_world\n      debug:\n  vars:\n    commands:\n      - ls\n      - pwd\n      - asdf\n  tasks:\n    - shell: \"{{ item }}\"\n      with_items: \"{{ commands }}\"\n      notify: hello_world\n\nEXPECTED RESULTS\nPLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************\nTASK [command] ****************************************************************************************************************************************************************************************************************************************************************\nchanged: [localhost] => (item=ls)\nchanged: [localhost] => (item=pwd)\nfailed: [localhost] (item=asdf) => {\"changed\": true, \"cmd\": \"asdf\", \"delta\": \"0:00:00.001902\", \"end\": \"2017-05-10 13:59:28.650743\", \"failed\": true, \"item\": \"asdf\", \"rc\": 127, \"start\": \"2017-05-10 13:59:28.648841\", \"stderr\": \"/bin/sh: asdf: command not found\", \"stderr_lines\": [\"/bin/sh: asdf: command not found\"], \"stdout\": \"\", \"stdout_lines\": []}\nRUNNING HANDLER [hello_world] *************************************************************************************************************************************************************************************************************************************************\nok: [localhost] => {\n\"changed\": false,\n\"msg\": \"Hello world!\"\n}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=1    changed=1    unreachable=0    failed=1\nACTUAL RESULTS\nPLAY [localhost] **************************************************************************************************************************************************************************************************************************************************************\nTASK [command] ****************************************************************************************************************************************************************************************************************************************************************\nchanged: [localhost] => (item=ls)\nchanged: [localhost] => (item=pwd)\nfailed: [localhost] (item=asdf) => {\"changed\": true, \"cmd\": \"asdf\", \"delta\": \"0:00:00.001916\", \"end\": \"2017-05-10 14:00:00.284031\", \"failed\": true, \"item\": \"asdf\", \"rc\": 127, \"start\": \"2017-05-10 14:00:00.282115\", \"stderr\": \"/bin/sh: asdf: command not found\", \"stderr_lines\": [\"/bin/sh: asdf: command not found\"], \"stdout\": \"\", \"stdout_lines\": []}\nPLAY RECAP ********************************************************************************************************************************************************************************************************************************************************************\nlocalhost                  : ok=0    changed=0    unreachable=0    failed=1"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "phillipuniverse"
                },
                "number": 4518,
                "resourcePath": "/ansible/ansible/issues/4518",
                "state": "CLOSED",
                "publishedAt": "2013-10-15T13:57:16Z",
                "closedAt": "2013-10-19T18:02:28Z",
                "title": "Stack trace in YAML Validation Code",
                "bodyText": "From the ansible-project list, the stack trace is as follows:\nTraceback (most recent call last):\n  File \"/usr/local/share/python/ansible-playbook\", line 268, in <module>\n    sys.exit(main(sys.argv[1:]))\n  File \"/usr/local/share/python/ansible-playbook\", line 208, in main\n    pb.run()\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/__init__.py\", line 228, in run\n    play = Play(self, play_ds, play_basedir)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 80, in __init__\n    ds = self._load_roles(self.roles, ds)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 288, in _load_roles\n    roles = self._build_role_dependencies(roles, [], self.vars)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/playbook/play.py\", line 178, in _build_role_dependencies\n    defaults_data = utils.parse_yaml_from_file(defaults)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 419, in parse_yaml_from_file\n    process_yaml_error(exc, data, path)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 397, in process_yaml_error\n    msg = process_common_errors(msg, probline, mark.column)\n  File \"/usr/local/lib/python2.7/site-packages/ansible/utils/__init__.py\", line 358, in process_common_errors\n    elif len(probline) and probline[column] == \":\" and probline.find(\"=\") != -1:\nIndexError: string index out of range\nThe issue ended up being that I missed a ':' between a variable key and value. I assume that this can be reproduced by leaving off a colon anywhere in a variable definition (or maybe even tasks). I specifically created this  it by creating a vars section and putting:\n---\nvariable value\nI fixed it by instead doing:\n---\nvariable: value\n@mpdehaan mentioned that this would be a pretty quick fix."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jlec"
                },
                "number": 15326,
                "resourcePath": "/ansible/ansible/issues/15326",
                "state": "CLOSED",
                "publishedAt": "2016-04-07T15:55:25Z",
                "closedAt": "2017-02-27T15:14:03Z",
                "title": "password_hash/get_encrypted_password uses passlib default of rounds=656000 which is 131 times glibc default",
                "bodyText": "In the password_hash filter function the underlying passlib call uses the default rounds parameter.\nThe default for glibc is 5000, the passlib default for sha512 is 656000. This means on a login in a linux account the hash calculation will take significantly longer.\nActually you get basically no rounds parameter when setting it to 5000\n$ python -c \"from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5000)\"\n$6$0zDM3P3/MbIoF0G0$pVmR8hc0ZNYdfGLnFhVKeCGhd32kDXo6ky83JmUWTZCfVQm2IrhnIbrrg5Vyl9XxE5aWzdmuYpvTA6gyOFc5Z.\n$ python -c \"from passlib.hash import sha512_crypt; print sha512_crypt.encrypt('foo', rounds=5001)\"\n$6$rounds=5001$VFyJ36YLb/RLhx0e$CAFCB/W7ebYEHIFsZtlJSdvuzEtsYOAvtOwCF7ahqpWqLj62TJp4LlzZ/FiW1B2U5kSKh4xibiJgZyd2ceVoI1\nCould a simple parameter be added to get_encrypted_password to set the value which has a preferable default what glibc does?"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "webarchitect609"
                },
                "number": 20547,
                "resourcePath": "/ansible/ansible/issues/20547",
                "state": "CLOSED",
                "publishedAt": "2017-01-22T10:15:57Z",
                "closedAt": "2017-01-22T17:48:50Z",
                "title": "Broken 'Edit on GitHub' link at http://docs.ansible.com/ansible/intro_adhoc.html",
                "bodyText": "ISSUE TYPE\n\nDocumentation Report\n\nSUMMARY\n'Edit on GitHub' http://joxi.ru/l2Z6VxFwlbVL2J?d=1 link leads to 404 error page http://joxi.ru/eAO14Wfxv1Gdmo?d=1\nSTEPS TO REPRODUCE\n1 Go to http://docs.ansible.com/ansible/intro_adhoc.html\n2 Press 'Edit on GitHub' at top right corner.\n3 See 404 error page.\nEXPECTED RESULTS\nSome kind of valid git hub page for editing related page.\nACTUAL RESULTS\n404 error page"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tzookb"
                },
                "number": 13103,
                "resourcePath": "/ansible/ansible/issues/13103",
                "state": "CLOSED",
                "publishedAt": "2015-11-09T22:18:55Z",
                "closedAt": "2015-11-19T22:41:48Z",
                "title": "ansible fails with an exception in python",
                "bodyText": "I sinply run command: \"forever stopall\"\nbut I get this output from stdout when it fails:\nfatal: [node1] => Traceback (most recent call last):\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 586, in _executor\nexec_rc = self._executor_internal(host, new_stdin)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 789, in _executor_internal\nreturn self._executor_internal_inner(host, self.module_name, self.module_args, inject, port, complex_args=complex_args)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 1005, in _executor_internal_inner\nnum_args_post = self._count_module_args(module_args)\nFile \"/Library/Python/2.7/site-packages/ansible/runner/init.py\", line 433, in _count_module_args\nvargs = split_args(args)\nFile \"/Library/Python/2.7/site-packages/ansible/module_utils/splitter.py\", line 73, in split_args\nargs = args.strip()\nAttributeError: 'dict' object has no attribute 'strip'"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "blakfeld"
                },
                "number": 14659,
                "resourcePath": "/ansible/ansible/issues/14659",
                "state": "CLOSED",
                "publishedAt": "2016-02-25T15:39:17Z",
                "closedAt": "2016-03-01T04:18:53Z",
                "title": "--diff Flag crashes on win_copy",
                "bodyText": "Issue Type:\n\nBug Report\n\nAnsible Version:\nansible 2.1.0 (devel cd51ba7965) last updated 2016/02/24 21:58:23 (GMT +000)\n  lib/ansible/modules/core: (detached HEAD e9454fa44f) last updated 2016/02/24 21:58:47 (GMT +000)\n  lib/ansible/modules/extras: (detached HEAD fade5b7936) last updated 2016/02/24 21:58:47 (GMT +000)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = /opt/maxpoint/ansible/release/library\n\nAnsible Configuration:\nStock\nEnvironment:\nRunning from: Ubuntu 14.04\nTargeting: Windows 2k12\nSummary:\nThe --diff flag fails when using the win_copy module.\nSteps To Reproduce:\nRun any playbook that contains the win_copy module while using the --diff flag against a Windows box.\n- name: Copy Over Pool Config Scripts\n  win_copy:\n    src: \"iis_pool_settings.ps1\"\n    dest: D:\\\\Temp\\\\iis_pool_settings.ps1\n\nExpected Results:\nI expect to see a diff between the file that was already on disk, and the one I just copied over.\nActual Results:\n\nHere's the entire output\nhttps://gist.github.com/blakfeld/542b191c85f4385e50bf\nHere's the relevant output\nTASK [company_bidder_deploy : Copy Over company Pool Config Scripts] *********\ntask path: /opt/company/ansible/release/roles/company_bidder_deploy/tasks/pre_install.yml:29\n<computername.companyinteractive.com> ESTABLISH WINRM CONNECTION FOR USER: Administrator on PORT 5986 TO computername.companyinteractive.com\n<computername.companyinteractive.com> WINRM CONNECT: transport=ssl endpoint=https://computername.companyinteractive.com:5986/wsman\n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414325.16-195672407265922\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM OPEN SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgAiACkALgBGAHUAbABsAE4AYQBtAGUAIAB8ACAAVwByAGkAdABlAC0ASABvAHMAdAAgAC0AUwBlAHAAYQByAGEAdABvAHIAIAAnACcAOwA=']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> PUT \"/tmp/tmpfOwmcO\" TO \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\"\n<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADUALgAxADYALQAxADkANQA2ADcAMgA0ADAANwAyADYANQA5ADIAMgBcAHMAdABhAHQALgBwAHMAMQAiAAoAJABEAGUAYgB1AGcAUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAEMAbwBuAHQAaQBuAHUAZQAiAAoAJABFAHIAcgBvAHIAQQBjAHQAaQBvAG4AUAByAGUAZgBlAHIAZQBuAGMAZQAgAD0AIAAiAFMAdABvAHAAIgAKAFMAZQB0AC0AUwB0AHIAaQBjAHQATQBvAGQAZQAgAC0AVgBlAHIAcwBpAG8AbgAgADIACgAkAGYAZAAgAD0AIABbAFMAeQBzAHQAZQBtAC4ASQBPAC4ARgBpAGwAZQBdADoAOgBDAHIAZQBhAHQAZQAoACQAcABhAHQAaAApAAoAJABzAGgAYQAxACAAPQAgAFsAUwB5AHMAdABlAG0ALgBTAGUAYwB1AHIAaQB0AHkALgBDAHIAeQBwAHQAbwBnAHIAYQBwAGgAeQAuAFMASABBADEAQwByAHkAcAB0AG8AUwBlAHIAdgBpAGMAZQBQAHIAbwB2AGkAZABlAHIAXQA6ADoAQwByAGUAYQB0AGUAKAApAAoAJABiAHkAdABlAHMAIAA9ACAAQAAoACkAIAAjAGkAbgBpAHQAaQBhAGwAaQB6AGUAIABmAG8AcgAgAGUAbQBwAHQAeQAgAGYAaQBsAGUAIABjAGEAcwBlAAoAfQAKAHAAcgBvAGMAZQBzAHMAIAB7AAoAJABiAHkAdABlAHMAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEMAbwBuAHYAZQByAHQAXQA6ADoARgByAG8AbQBCAGEAcwBlADYANABTAHQAcgBpAG4AZwAoACQAaQBuAHAAdQB0ACkACgAkAHMAaABhADEALgBUAHIAYQBuAHMAZgBvAHIAbQBCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAAsACAAJABiAHkAdABlAHMALAAgADAAKQAgAHwAIABPAHUAdAAtAE4AdQBsAGwACgAkAGYAZAAuAFcAcgBpAHQAZQAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAkAGIAeQB0AGUAcwAuAEwAZQBuAGcAdABoACkACgB9AAoAZQBuAGQAIAB7AAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0ARgBpAG4AYQBsAEIAbABvAGMAawAoACQAYgB5AHQAZQBzACwAIAAwACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABoAGEAcwBoACAAPQAgAFsAUwB5AHMAdABlAG0ALgBCAGkAdABDAG8AbgB2AGUAcgB0AGUAcgBdADoAOgBUAG8AUwB0AHIAaQBuAGcAKAAkAHMAaABhADEALgBIAGEAcwBoACkALgBSAGUAcABsAGEAYwBlACgAIgAtACIALAAgACIAIgApAC4AVABvAEwAbwB3AGUAcgBJAG4AdgBhAHIAaQBhAG4AdAAoACkACgAkAGYAZAAuAEMAbABvAHMAZQAoACkACgBXAHIAaQB0AGUALQBPAHUAdABwAHUAdAAgACIAewAiACIAcwBoAGEAMQAiACIAOgAiACIAJABoAGEAcwBoACIAIgB9ACIACgB9AA==']\n<computername.companyinteractive.com> WINRM PUT \"/tmp/tmpfOwmcO\" to \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\" (offset=10177 size=10177)\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"sha1\":\"4b5fc9661a0\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"sha1\":\"4b5fc9661a08b2372dc5efa608f719480e61502b\"}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\nTry\n{\n& \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\\stat.ps1\"\n}\nCatch\n{\n$_obj = @{ failed = $true }\nIf ($_.Exception.GetType)\n{\n$_obj.Add('msg', $_.Exception.Message)\n}\nElse\n{\n$_obj.Add('msg', $_.ToString())\n}\nIf ($_.InvocationInfo.PositionMessage)\n{\n$_obj.Add('exception', $_.InvocationInfo.PositionMessage)\n}\nElseIf ($_.ScriptStackTrace)\n{\n$_obj.Add('exception', $_.ScriptStackTrace)\n}\nTry\n{\n$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))\n}\nCatch\n{\n}\nEcho $_obj | ConvertTo-Json -Compress -Depth 99\nExit 1\n}\nFinally { Remove-Item \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414325.16-195672407265922\" -Force -Recurse -ErrorAction SilentlyContinue }\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'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']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"stat\":{\"exists\":fa\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"stat\":{\"exists\":false},\"changed\":false}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414326.29-74854495923127\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgAyADkALQA3ADQAOAA1ADQANAA5ADUAOQAyADMAMQAyADcAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.29-74854495923127\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\n(New-Item -Type Directory -Path $env:temp -Name \"ansible-tmp-1456414326.6-190477404467970\").FullName | Write-Host -Separator '';\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgAoAE4AZQB3AC0ASQB0AGUAbQAgAC0AVAB5AHAAZQAgAEQAaQByAGUAYwB0AG8AcgB5ACAALQBQAGEAdABoACAAJABlAG4AdgA6AHQAZQBtAHAAIAAtAE4AYQBtAGUAIAAiAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAIgApAC4ARgB1AGwAbABOAGEAbQBlACAAfAAgAFcAcgBpAHQAZQAtAEgAbwBzAHQAIAAtAFMAZQBwAGEAcgBhAHQAbwByACAAJwAnADsA']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"C:\\\\Users\\\\Administrat\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> PUT \"/tmp/tmpcEFZin\" TO \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\"\n<computername.companyinteractive.com> WINRM EXEC 'PowerShell' ['-NoProfile', '-NonInteractive', '-ExecutionPolicy', 'Unrestricted', '-EncodedCommand', 'YgBlAGcAaQBuACAAewAKACQAcABhAHQAaAAgAD0AIAAiAEMAOgBcAFUAcwBlAHIAcwBcAEEAZABtAGkAbgBpAHMAdAByAGEAdABvAHIAXABBAHAAcABEAGEAdABhAFwATABvAGMAYQBsAFwAVABlAG0AcABcAGEAbgBzAGkAYgBsAGUALQB0AG0AcAAtADEANAA1ADYANAAxADQAMwAyADYALgA2AC0AMQA5ADAANAA3ADcANAAwADQANAA2ADcAOQA3ADAAXABmAGkAbABlAC4AcABzADEAIgAKACQARABlAGIAdQBnAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBDAG8AbgB0AGkAbgB1AGUAIgAKACQARQByAHIAbwByAEEAYwB0AGkAbwBuAFAAcgBlAGYAZQByAGUAbgBjAGUAIAA9ACAAIgBTAHQAbwBwACIACgBTAGUAdAAtAFMAdAByAGkAYwB0AE0AbwBkAGUAIAAtAFYAZQByAHMAaQBvAG4AIAAyAAoAJABmAGQAIAA9ACAAWwBTAHkAcwB0AGUAbQAuAEkATwAuAEYAaQBsAGUAXQA6ADoAQwByAGUAYQB0AGUAKAAkAHAAYQB0AGgAKQAKACQAcwBoAGEAMQAgAD0AIABbAFMAeQBzAHQAZQBtAC4AUwBlAGMAdQByAGkAdAB5AC4AQwByAHkAcAB0AG8AZwByAGEAcABoAHkALgBTAEgAQQAxAEMAcgB5AHAAdABvAFMAZQByAHYAaQBjAGUAUAByAG8AdgBpAGQAZQByAF0AOgA6AEMAcgBlAGEAdABlACgAKQAKACQAYgB5AHQAZQBzACAAPQAgAEAAKAApACAAIwBpAG4AaQB0AGkAYQBsAGkAegBlACAAZgBvAHIAIABlAG0AcAB0AHkAIABmAGkAbABlACAAYwBhAHMAZQAKAH0ACgBwAHIAbwBjAGUAcwBzACAAewAKACQAYgB5AHQAZQBzACAAPQAgAFsAUwB5AHMAdABlAG0ALgBDAG8AbgB2AGUAcgB0AF0AOgA6AEYAcgBvAG0AQgBhAHMAZQA2ADQAUwB0AHIAaQBuAGcAKAAkAGkAbgBwAHUAdAApAAoAJABzAGgAYQAxAC4AVAByAGEAbgBzAGYAbwByAG0AQgBsAG8AYwBrACgAJABiAHkAdABlAHMALAAgADAALAAgACQAYgB5AHQAZQBzAC4ATABlAG4AZwB0AGgALAAgACQAYgB5AHQAZQBzACwAIAAwACkAIAB8ACAATwB1AHQALQBOAHUAbABsAAoAJABmAGQALgBXAHIAaQB0AGUAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAJABiAHkAdABlAHMALgBMAGUAbgBnAHQAaAApAAoAfQAKAGUAbgBkACAAewAKACQAcwBoAGEAMQAuAFQAcgBhAG4AcwBmAG8AcgBtAEYAaQBuAGEAbABCAGwAbwBjAGsAKAAkAGIAeQB0AGUAcwAsACAAMAAsACAAMAApACAAfAAgAE8AdQB0AC0ATgB1AGwAbAAKACQAaABhAHMAaAAgAD0AIABbAFMAeQBzAHQAZQBtAC4AQgBpAHQAQwBvAG4AdgBlAHIAdABlAHIAXQA6ADoAVABvAFMAdAByAGkAbgBnACgAJABzAGgAYQAxAC4ASABhAHMAaAApAC4AUgBlAHAAbABhAGMAZQAoACIALQAiACwAIAAiACIAKQAuAFQAbwBMAG8AdwBlAHIASQBuAHYAYQByAGkAYQBuAHQAKAApAAoAJABmAGQALgBDAGwAbwBzAGUAKAApAAoAVwByAGkAdABlAC0ATwB1AHQAcAB1AHQAIAAiAHsAIgAiAHMAaABhADEAIgAiADoAIgAiACQAaABhAHMAaAAiACIAfQAiAAoAfQA=']\n<computername.companyinteractive.com> WINRM PUT \"/tmp/tmpcEFZin\" to \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\" (offset=10656 size=10656)\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 0, out \"{\"sha1\":\"7f5d2fe5184\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"sha1\":\"7f5d2fe5184e28c170ff1de2a7b611897f896275\"}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> EXEC Set-StrictMode -Version Latest\nTry\n{\n& \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ansible-tmp-1456414326.6-190477404467970\\file.ps1\"\n}\nCatch\n{\n$_obj = @{ failed = $true }\nIf ($_.Exception.GetType)\n{\n$_obj.Add('msg', $_.Exception.Message)\n}\nElse\n{\n$_obj.Add('msg', $_.ToString())\n}\nIf ($_.InvocationInfo.PositionMessage)\n{\n$_obj.Add('exception', $_.InvocationInfo.PositionMessage)\n}\nElseIf ($_.ScriptStackTrace)\n{\n$_obj.Add('exception', $_.ScriptStackTrace)\n}\nTry\n{\n$_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))\n}\nCatch\n{\n}\nEcho $_obj | ConvertTo-Json -Compress -Depth 99\nExit 1\n}\n<computername.companyinteractive.com> WINRM EXEC u'PowerShell' [u'-NoProfile', u'-NonInteractive', u'-ExecutionPolicy', u'Unrestricted', u'-EncodedCommand', u'UwBlAHQALQBTAHQAcgBpAGMAdABNAG8AZABlACAALQBWAGUAcgBzAGkAbwBuACAATABhAHQAZQBzAHQACgBUAHIAeQAKAHsACgAmACAAIgBDADoAXABVAHMAZQByAHMAXABBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAFwAQQBwAHAARABhAHQAYQBcAEwAbwBjAGEAbABcAFQAZQBtAHAAXABhAG4AcwBpAGIAbABlAC0AdABtAHAALQAxADQANQA2ADQAMQA0ADMAMgA2AC4ANgAtADEAOQAwADQANwA3ADQAMAA0ADQANgA3ADkANwAwAFwAZgBpAGwAZQAuAHAAcwAxACIACgB9AAoAQwBhAHQAYwBoAAoAewAKACQAXwBvAGIAagAgAD0AIABAAHsAIABmAGEAaQBsAGUAZAAgAD0AIAAkAHQAcgB1AGUAIAB9AAoASQBmACAAKAAkAF8ALgBFAHgAYwBlAHAAdABpAG8AbgAuAEcAZQB0AFQAeQBwAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4ARQB4AGMAZQBwAHQAaQBvAG4ALgBNAGUAcwBzAGEAZwBlACkACgB9AAoARQBsAHMAZQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAbQBzAGcAJwAsACAAJABfAC4AVABvAFMAdAByAGkAbgBnACgAKQApAAoAfQAKAEkAZgAgACgAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAHsACgAkAF8AbwBiAGoALgBBAGQAZAAoACcAZQB4AGMAZQBwAHQAaQBvAG4AJwAsACAAJABfAC4ASQBuAHYAbwBjAGEAdABpAG8AbgBJAG4AZgBvAC4AUABvAHMAaQB0AGkAbwBuAE0AZQBzAHMAYQBnAGUAKQAKAH0ACgBFAGwAcwBlAEkAZgAgACgAJABfAC4AUwBjAHIAaQBwAHQAUwB0AGEAYwBrAFQAcgBhAGMAZQApAAoAewAKACQAXwBvAGIAagAuAEEAZABkACgAJwBlAHgAYwBlAHAAdABpAG8AbgAnACwAIAAkAF8ALgBTAGMAcgBpAHAAdABTAHQAYQBjAGsAVAByAGEAYwBlACkACgB9AAoAVAByAHkACgB7AAoAJABfAG8AYgBqAC4AQQBkAGQAKAAnAGUAcgByAG8AcgBfAHIAZQBjAG8AcgBkACcALAAgACgAJABfACAAfAAgAEMAbwBuAHYAZQByAHQAVABvAC0ASgBzAG8AbgAgAHwAIABDAG8AbgB2AGUAcgB0AEYAcgBvAG0ALQBKAHMAbwBuACkAKQAKAH0ACgBDAGEAdABjAGgACgB7AAoAfQAKAEUAYwBoAG8AIAAkAF8AbwBiAGoAIAB8ACAAQwBvAG4AdgBlAHIAdABUAG8ALQBKAHMAbwBuACAALQBDAG8AbQBwAHIAZQBzAHMAIAAtAEQAZQBwAHQAaAAgADkAOQAKAEUAeABpAHQAIAAxAAoAfQA=']\n<computername.companyinteractive.com> WINRM RESULT u'<Response code 1, out \"{\"msg\":\"path will no\", err \"\">'\n<computername.companyinteractive.com> WINRM STDOUT {\"msg\":\"path will not be created\",\"failed\":true}\n<computername.companyinteractive.com> WINRM STDERR \n<computername.companyinteractive.com> WINRM CLOSE SHELL: ABD20C31-4581-4A04-9E03-527A04A2F27F\nAn exception occurred during task execution. The full traceback is:\nTraceback (most recent call last):\n  File \"/opt/ansible-src/lib/ansible/executor/task_executor.py\", line 122, in run\n    res = self._execute()\n  File \"/opt/ansible-src/lib/ansible/executor/task_executor.py\", line 418, in _execute\n    result = self._handler.run(task_vars=variables)\n  File \"/opt/ansible-src/lib/ansible/plugins/action/copy.py\", line 202, in run\n    diffs.append(self._get_diff_data(dest_file, source_full, task_vars))\n  File \"/opt/ansible-src/lib/ansible/plugins/action/__init__.py\", line 614, in _get_diff_data\n    if peek_result['state'] == 'absent':\nKeyError: 'state'\n\nThe problem appears to be because the --diff command is relying upon the existence of a 'state' key in the returned JSON. This appears to be a key added in the module_utils.basic Python module, specifically the add_path_info method, which seems to be called by load_common_file_arguments. I was going to just hack this into win_copy, but it seems to me that the most reasonable solution would be to add load_common_file_arguments to powershell_common.ps1. I'm more than happy to start work on that if the community agrees that is the best answer."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "sebi-hgdata"
                },
                "number": 11881,
                "resourcePath": "/ansible/ansible/issues/11881",
                "state": "CLOSED",
                "publishedAt": "2015-08-06T19:53:25Z",
                "closedAt": "2015-08-12T14:50:19Z",
                "title": "Variable overding during nested includes issue",
                "bodyText": "It seems variables are not overridden when using nested includes (this works ok in v1 ) .This might be related to   #11353 . To reproduce the issue you can follow the steps from #11353 ...\nIn build.yml I have a variable docker_tags what is overriden in  tasks/docker/base_build.yml... instead of printing the overridden value it prints the value from build.yml. See debug statements after running  'ansible-playbook -v build_admin_ui.yml -i inventories/local/hosts --extra-vars \"admin_ui_version=1234\"'"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "BenMo158"
                },
                "number": 13390,
                "resourcePath": "/ansible/ansible/issues/13390",
                "state": "CLOSED",
                "publishedAt": "2015-12-02T06:13:11Z",
                "closedAt": "2016-04-29T00:13:51Z",
                "title": "[v2] \"win_iis_webbinding\" module require additional parameters to run?",
                "bodyText": "I was run\nansible iis01 -m win_iis_webbinding -a \"name=tiger\"\nAn exception occurred during task execution. To see the full traceback, use -vvv. The error was: +     ~~~~~~~~~~~~~~~~~~~\niis01 | FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"Property 'host_header' cannot be found on this object. Make sure that it exists.\"\n}\nI need run this,it's work\nansible iis01 -m win_iis_webbinding -a \"name=tiger host_header=www.tiger.com protocol=http port=8083 ip=127.0.0.1\"\niis01 | SUCCESS => {\n    \"added\": [], \n    \"changed\": false, \n    \"matched\": [], \n    \"parameters\": {\n        \"HostHeader\": \"www.tiger.com\", \n        \"IPAddress\": \"127.0.0.1\", \n        \"Name\": \"tiger\", \n        \"Port\": \"8083\", \n        \"Protocol\": \"http\"\n    }, \n    \"removed\": []\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jpcarey"
                },
                "number": 11362,
                "resourcePath": "/ansible/ansible/issues/11362",
                "state": "CLOSED",
                "publishedAt": "2015-06-23T23:33:17Z",
                "closedAt": "2015-06-24T18:42:38Z",
                "title": "ec2 dynamic_inventory tag issue",
                "bodyText": "The special characters escape to underscores in ec2.py does not appear to work poperly.\nI have the following tag for an ec2 instance:\naws:cloudformation:stack-name => imdev-flask-3-2-298-l1FiTlA\nPer the docs, this should translate to an underscore between stack and name: 'ec2_tag_aws_cloudformation_stack_name'\nThis does not work, and produced the following var undefined error:\nOne or more undefined variables: 'ec2_tag_aws_cloudformation_stack_name' is undefined\nrunning ec2.py, the actual output is \"ec2_tag_aws_cloudformation_stack-name\": \"imdev-flask-3-2-298-l1FiTlA\"\nIf I try to use in the ec2.py form, ansible will throw an error for the invalid character:\nFailed to template msg=\"{{ ec2_tag_aws_cloudformation_stack-name == removed_version }}\": Unable to look up a name or access an attribute in template string. Make sure your variable name does not contain invalid characters like '-'.\nHowever, accessing through the hostvars allows reference to the item with the \"-\". Ex:\n\"{{  hostvars[inventory_hostname]['ec2_tag_aws_cloudformation_stack-name'] }}\""
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tartansandal"
                },
                "number": 4608,
                "resourcePath": "/ansible/ansible/issues/4608",
                "state": "CLOSED",
                "publishedAt": "2013-10-20T23:05:27Z",
                "closedAt": "2013-10-22T16:01:21Z",
                "title": "documented used of 'lookup' now generates variable undefined error ",
                "bodyText": "The following documented use of the lookup plugin\n\n---\n- hosts: all\n  tasks:\n     - debug: msg=\"{{ lookup('env','HOME') }} is an environment variable\"\n\nnow generates the following error:\nTASK: [debug msg=\"{{lookup('env','HOME')}} is an environment variable\"] ******* \nfatal: [localhost] => One or more undefined variables: 'lookup' is undefined\n\nI've traced this back to commit 5031104."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "evgkrsk"
                },
                "number": 13175,
                "resourcePath": "/ansible/ansible/issues/13175",
                "state": "CLOSED",
                "publishedAt": "2015-11-16T01:06:20Z",
                "closedAt": "2019-01-25T16:25:15Z",
                "title": "Wrong diagnostics in ansible-galaxy on certificate issues",
                "bodyText": "Issue Type:\n\nBug Report\n\nComponent Name\n\nansible-galaxy\n\nAnsible Version:\nevg@thinkpad ~ $ansible --version\nansible 2.2.2.0\n  config file = /home/evg/.ansible.cfg\n  configured module search path = Default w/o overrides\n\nAnsible Configuration:\nClean out-of-box configuration (on SOME OS).\nEnvironment:\nN/A. Generic, non-mainstream OS. In my case, ALT Linux Sisypus (unstable branch).\nSummary:\nOn non-mainstream OS with non-standard SSL/TLS CA certificate paths (not in '/etc/ssl/certs', '/etc/pki/ca-trust/extracted/pem', '/etc/pki/tls/certs', '/usr/share/ca-certificates/cacert.org', '/etc/ansible' in my case) diagnostics on any problem with certificate check in ansible-galaxy is just plain wrong (ERROR! Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED).\nSteps To Reproduce:\nTry to install any galaxy role just after clean ansible install (ansible-galaxy will fail to check correct SSL/TLS certificate of galaxy.ansible.com with wrong error message).\nExpected Results:\nevg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent \n- downloading role 'zabbix-agent', owned by dj-wasabi\n- downloading role from https://github.com/dj-wasabi/ansible-zabbix-agent/archive/0.3.0.tar.gz\n- extracting dj-wasabi.zabbix-agent to /home/evg/.ansible/roles/dj-wasabi.zabbix-agent\n- dj-wasabi.zabbix-agent was installed successfully\n\nActual Results:\nevg@thinkpad ~ $ansible-galaxy install dj-wasabi.zabbix-agent   \n [WARNING]: - dj-wasabi.zabbix-agent was NOT installed successfully: Failed to get data from the API server (https://galaxy.ansible.com/api/): HTTP Error 401: UNAUTHORIZED\n\nERROR! - you can use --ignore-errors to skip failed roles and finish processing the list.\n\nSanity check:\nevg@thinkpad ~ $curl https://galaxy.ansible.com/api/\n{\"available_versions\":{\"v1\":\"/api/v1/\"},\"description\":\"GALAXY REST API\",\"current_version\":\"v1\"}\n\nWorkarounds:\nNone. Old workaround for ansible-1.9.x NOT working anymore:\nroot@thinkpad /etc/ansible #ln -s /usr/share/ca-certificates/ca-bundle.crt ."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": null,
                "number": 22531,
                "resourcePath": "/ansible/ansible/issues/22531",
                "state": "OPEN",
                "publishedAt": "2017-03-11T19:19:29Z",
                "closedAt": null,
                "title": "Trailing new lines aren't kept by default by template module",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nJinja\nANSIBLE VERSION\nansible 2.2.0.0\n\nCONFIGURATION\n[defaults]\nask_pass = False\nremote_user = root\ninventory = inventories\ntimeout = 5\nroles_path = roles\n\nansible_managed = This file is managed by DOCK42 Configuration Management, all manual changes will be lost.\n\n[ssh_connection]\ncontrol_path = %(directory)s/%%h-%%p-%%r\n\nOS / ENVIRONMENT\n\nSUMMARY\n(referring to commit 1998edd)\nWhen trying to wrap a conditional inside a template in an one liner, the line break will be skipped if there's no trailing character (e.g. like a whitespace to workaround this issue)\nSTEPS TO REPRODUCE\nCreate a template like this:\n# GSSAPI options\nGSSAPIAuthentication {% if openssh.gssapi_authentication %}yes{% else %}no{% endif %}\nGSSAPICleanupCredentials no\n#GSSAPIStrictAcceptorCheck yes\n\n openssh.gssapi_authentication: no\n\n- name: Configure sshd\n  template: \n    src: sshd_config.j2\n    dest: /etc/ssh/sshd_config\n    backup: yes\n    owner: root \n    group: root \n    mode: 0600\n    validate: '/usr/sbin/sshd -T -f %s'\n  notify:\n    - restart sshd\n  become: yes\n  tags:\n    - sshd_config\nEXPECTED RESULTS\n# GSSAPI options\nGSSAPIAuthentication no \nGSSAPICleanupCredentials no\n\nACTUAL RESULTS\n # GSSAPI options\nGSSAPIAuthentication noGSSAPICleanupCredentials no"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ahes"
                },
                "number": 14243,
                "resourcePath": "/ansible/ansible/issues/14243",
                "state": "CLOSED",
                "publishedAt": "2016-02-01T15:55:33Z",
                "closedAt": "2016-02-02T15:23:36Z",
                "title": "pre_tasks not working while gathering=smart in ansible.cfg",
                "bodyText": "Issue Type:\nBug report\nAnsible Version:\nansible 2.1.0 (devel 45e05ec072) last updated 2016/02/01 14:17:33 (GMT +200)\n  lib/ansible/modules/core: (detached HEAD 88e0bfd75d) last updated 2015/11/20 12:34:36 (GMT +200)\n  lib/ansible/modules/extras: (detached HEAD 7da1f8d4ca) last updated 2015/11/20 12:34:44 (GMT +200)\n  config file = ansible.cfg\n  configured module search path = Default w/o overrides\n\nAnsible Configuration:\n[defaults]\ngathering = smart\n\nEnvironment:\nMac OS X\nSummary:\nWhen gathering=smart used in ansible.cfg pre_tasks are not running.\nSteps To Reproduce:\nansible.cfg:\n[defaults]\ngathering = smart\n\nplay.yml:\n\n---\n- hosts: localhost\n\n  pre_tasks:\n    - name: pre_task\n      shell: echo 'hello'\n\n  tasks:\n    - name: task\n      shell: echo 'still busy'\n\n  post_tasks:\n    - name: post_task\n      shell: echo 'goodbye'\n\nExpected Results:\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [pre_task] ****************************************************************\nchanged: [localhost]\n\nTASK [task] ********************************************************************\nchanged: [localhost]\n\nTASK [post_task] ***************************************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=4    changed=3    unreachable=0    failed=0\n\nActual Results:\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nok: [localhost]\n\nTASK [task] ********************************************************************\nchanged: [localhost]\n\nTASK [post_task] ***************************************************************\nchanged: [localhost]\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=3    changed=2    unreachable=0    failed=0"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dp4qb"
                },
                "number": 10828,
                "resourcePath": "/ansible/ansible/issues/10828",
                "state": "CLOSED",
                "publishedAt": "2015-04-24T09:18:17Z",
                "closedAt": "2016-06-01T18:46:20Z",
                "title": "Issue with 'include' statements and role path",
                "bodyText": "My ansible's 'include' statement was working fine, but recently after including ymls in subfolder, it somehow brokes the role's path.\nHere's the tree of my project:\n.\n├── site.yml\n├── inventory.ini\n└── roles\n    └── webservers\n        ├── files\n        │   └── crt.crt\n        ├── tasks\n        │   ├── main.yml\n        │   ├── httpd.yml\n        │   ├── dev\n        │   │   ├── httpd.yml\n        │   │   └── main.yml\n        │   └── prod\n        │       ├── httpd.yml\n        │       └── main.yml\n        ├── templates\n        │   └── httpd_conf.j2\n        └── vars\n            └── main.yml\n\nsite.yml:\n - name: Install base software\n   sudo: yes\n   vars:\n      profile:    \"dev\"\n   roles:\n      - webservers\n\nroles/webservers/tasks/main.yml:\n - name: Install httpd\n   include: httpd.yml\n\n - name: Including specific tasks\n   include: \"{{ profile }}/main.yml\" \n\nPrior to this point, it works just fine. But the next step, after including the \"profile\"/main.yml, brokes the role's path.\nroles/webservers/tasks/dev/main.yml:\n - name:  Create httpd.conf from template\n   template: src=httpd_conf.j2 dest=/etc/httpd/conf/httpd.conf\n   with_items: webservers_httpd_vhosts\n\nresults an error:\nTASK: [webservers | Create httpd.conf from template] *********************\nfatal: [192.168.0.2] => input file not found at /home/me/git/repo/projects/ans/roles/webservers/tasks/templates/httpd_conf.j2 or /home/me/git/repo/projects/ans/httpd_conf.j2\n\nFATAL: all hosts have already failed -- aborting\n\nSo, after this 'include', it somehow thinks that role's root is located in 'webservers/tasks/', instead of 'webservers/'. But in the same time, it sees the variables in 'webservers/vars/' path.\nMoreover, it was working fine and recently just broke. I haven't updated ansible, nor edited it's parameters, just updated the playbook.\nIf I specify the 'src=../../templates/httpd_conf.j2' it works fine, but since this behaviour just appeared like from nowhere, I can't rely on this path.\nI've managed to use this workaround: template: src=\"{{ role_path }}/templates/httpd_conf.j2\" dest=/etc/httpd/conf/httpd.conf But that's a workaround. The reason of auto-pathing is broke is still undiscovered.\nAnsilbe's version: 1.9.0.1"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ankitmth"
                },
                "number": 14151,
                "resourcePath": "/ansible/ansible/issues/14151",
                "state": "CLOSED",
                "publishedAt": "2016-01-27T11:08:52Z",
                "closedAt": "2016-11-16T09:21:57Z",
                "title": "Ansible 2.0.0.2 : \"ERROR! file or module does not exist\" while running a playbook with script module",
                "bodyText": "After installing ansible 2.0.0.2 from rpm:\n[root@ansibletest setup]# ansible-playbook --version\nansible-playbook 2.0.0.2\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nI get the below error on running the playbook which uses the script module:\n[root@ansibletest setup]# ansible-playbook test.yml\n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nEnter passphrase for key '/root/.ssh/id_rsa':\nok: [192.168.2.101]\n\nTASK [Run test.sh script] ****************************************************\nfatal: [192.168.2.101]: FAILED! => {\"failed\": true, \"msg\": \"ERROR! file or module does not exist: /path/to/script/test.sh\"}\n\nPLAY RECAP *********************************************************************\n192.168.2.101              : ok=1    changed=0    unreachable=0    failed=1\n\nThe Playbook in question: -\n\n---\n- hosts: 192.168.2.101\n  remote_user: root\n  vars:\n    var1: data\n    param1: 1\n  tasks:\n    - name: Run test.sh script\n      become: yes\n      script: /{{ var1 }}/path/to/test.sh {{ param1 }} creates=/{{ var1 }}/{{ param1 }}/executed.txt"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ktosiek"
                },
                "number": 8213,
                "resourcePath": "/ansible/ansible/issues/8213",
                "state": "CLOSED",
                "publishedAt": "2014-07-20T18:52:22Z",
                "closedAt": "2015-06-28T01:05:40Z",
                "title": "Variable interpolation in hostvars",
                "bodyText": "Issue Type: Feature Idea\nAnsible Version: ansible 1.6.6\nEnvironment: N/A\nSummary:\nWhen variables are accessed through hostvars, jinja2 expressions inside those variables should be interpolated.\nThere was discussion about this issue on ansible-project mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/ansible-project/pdWFR2Q8U8E/m6sgB1C5K1YJ\nAnd here is a filter that shows simple implementation (but might be pretty slow, and I think it should be in hostvars not a filter): https://groups.google.com/group/ansible-project/attach/16318558e74073ae/hostvars.py?part=0.1&view=1\nSteps To Reproduce:\nGiven vars like:\nfoo: y\nbar: \"x is {{ foo }}\"\n\nThe values for both {{ bar }} and {{ hostvars[some_host].bar }} should be \"x is y\".\nCurrently the value of {{ hostvars[some_host].bar }} would be (literally) \"x is {{ foo }}\"."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "basictheprogram"
                },
                "number": 23249,
                "resourcePath": "/ansible/ansible/issues/23249",
                "state": "CLOSED",
                "publishedAt": "2017-04-04T02:23:33Z",
                "closedAt": "2017-04-04T17:31:07Z",
                "title": "win_chocolatey installing powershell when powershell4 is installed and state: latest",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\nwin_chocolatey\nANSIBLE VERSION\n\n$ ansible --version\nansible 2.4.0 (devel cc50b803df) last updated 2017/03/22 14:15:18 (GMT -500)\n  config file =\n  configured module search path = Default w/o overrides\n  python version = 2.7.10 (default, Jul 30 2016, 19:40:32) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.34)]\n\nCONFIGURATION\nAnsible configuration from git clone\nOS / ENVIRONMENT\nControl host macOS 10.12.3\nManaged host Windows 7\nSUMMARY\nAttempting to upgrade powershell4 to powershell5 using the win_chocolatey module with state: latest\nMight be related to #21873\nRelated to #22892\nSTEPS TO REPRODUCE\n\nHave chocolatey installed\nInstall powershell4\n\n\n- name: install powershell \n  win_chocolatey:\n    name: \"{{ item }}\"\n    state: latest\n  with_items:\n    - \"powershell\"\n  register: check_powershell5\nEXPECTED RESULTS\nPowershell4 would be upgraded to Powershell5\nACTUAL RESULTS\nhttps://gist.github.com/basictheprogram/74b48320e386d308a69f82f7c90019b5\nchoco_summary.log\nhttps://gist.github.com/basictheprogram/de146516fe292a43630c10e7df205f38\nchocolatey.log\nhttps://gist.github.com/basictheprogram/5cc240c1f89fba6ef7ac769ff5e41a01"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "michalzubkowicz"
                },
                "number": 16045,
                "resourcePath": "/ansible/ansible/issues/16045",
                "state": "CLOSED",
                "publishedAt": "2016-05-30T08:07:08Z",
                "closedAt": "2016-05-30T15:43:10Z",
                "title": "Docker module: argument memory_limit is of type <type 'str'> and we were unable to convert to int\" on  Ansible 2.0.2.0-1.el7",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nANSIBLE VERSION\n\nansible 2.0.2.0\n\nOS / ENVIRONMENT\nCentos 7\nSUMMARY\nAfter upgrade to ansible 2.0.2.0 it's not possible to enter memory_limit as human readable string (ie. 265MB) only bytes are accepted.\nSTEPS TO REPRODUCE\ntry set memory_limit:  256MB\n\n- name: sphinx container\n  docker:\n        name: sphinx\n        image: michalzubkowicz/docker-sphinxsearch\n        state: started\n        restart_policy: always\n        memory_limit: 256MB\n\n\nEXPECTED RESULTS\n\nShould accept string as early versions\nACTUAL RESULTS\n\nIs showing error\nargument memory_limit is of type <type 'str'> and we were unable to convert to int"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "landryb"
                },
                "number": 25910,
                "resourcePath": "/ansible/ansible/issues/25910",
                "state": "CLOSED",
                "publishedAt": "2017-06-20T15:19:14Z",
                "closedAt": "2017-06-23T16:25:42Z",
                "title": "openbsd_pkg: add support for pkgname%branch syntax",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\nopenbsd_pkg\nANSIBLE VERSION\nansible 2.3.1.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Jun 17 2017, 15:16:02) [GCC 4.2.1 20070719 ]\n\n\nCONFIGURATION\nOS / ENVIRONMENT\nOpenBSD -current or 6.1\nSUMMARY\nTrying to use the pkgname%branch syntax fails\nSTEPS TO REPRODUCE\n\n\nansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'\n\n\nEXPECTED RESULTS\ninstalling openldap-server 2.44 from the databases/openldap branch. works fine in a shell:\n#pkg_add -n openldap-server--%openldap\nquirks-2.304 signed on 2017-04-02T15:01:33Z\nopenldap-server-2.4.44p3:icu4c-58.2p0: ok\nopenldap-server-2.4.44p3:openldap-client-2.4.44p3: ok\nopenldap-server-2.4.44p3:db-4.6.21p3v0: ok\nopenldap-server-2.4.44p3:e2fsprogs-1.42.12p4: ok\nopenldap-server-2.4.44p3: ok\n\n\nACTUAL RESULTS\n\nThe openbsd_pkg module errors out.\n\n$ansible -C localhost -m openbsd_pkg -a 'name=openldap-server--%openldap state=installed'\n\nlocalhost | FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to parse package name at versionless_match: openldap-server--%openldap\"\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jean-christophe-manciot"
                },
                "number": 26146,
                "resourcePath": "/ansible/ansible/issues/26146",
                "state": "CLOSED",
                "publishedAt": "2017-06-27T15:21:12Z",
                "closedAt": "2017-09-15T14:49:22Z",
                "title": "Error with paramiko 2.1.0 <--> 2.2.1: Unicode-objects must be encoded before hashing",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nios_command\nbut I suspect all commands are impacted\nANSIBLE VERSION\nBoth latest stable & unstable versions:\nansible 2.3.1.0 (detached HEAD ecb38fdf73) last updated 2017/06/27 16:57:40 (GMT +200)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']\n  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]\n\nansible 2.4.0 (devel 9f7fcf15be) last updated 2017/06/27 16:41:44 (GMT +200)\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = [u'/home/actionmystique/Ansible/git-yang-networkop/ansible-101/library']\n  ansible python module location = /home/actionmystique/src/Ansible/git-ansible/lib/ansible\n  executable location = /home/actionmystique/src/Ansible/git-ansible/bin/ansible\n  python version = 2.7.13 (default, Jan 19 2017, 14:48:08) [GCC 6.3.0 20170118]\n\nCONFIGURATION\ninventory   = ./hosts\nlibrary        = /home/actionmystique/Ansible/git-yang-networkop/ansible-101/library\nforks = 1000\ngathering = explicit\ngather_timeout = 30\nroles_path = /home/actionmystique/Ansible/Roles/roles\nprivate_role_vars = yes\nhash_behaviour = merge\nlog_path = /var/log/ansible.log\nretry_files_enabled = False\nshow_custom_stats = True\ntimeout = 60\npipelining = True\nconnect_timeout = 60\nconnect_retries = 30\nconnect_interval = 1\nOS / ENVIRONMENT\n\nhost: Ubuntu 17.04 4.10\ntarget:\n\nIOS-XEv 16.4.1\n\n\nparamiko: 2.2.1\n\nSUMMARY\ncf. title, for instance with a show running-config command.\nI am able to manually ssh into the remote device.\nIs the paramiko version too recent?\nSTEPS TO REPRODUCE\nStructure passed as \"provider\": connections.ssh\nconnections\n...\n        ssh:\n          transport: cli \n          host: \"{{ ansible_host }}\"\n          # ansible_port\n          port: 22\n          # ansible_user\n          username: admin\n          # ansible_ssh_pass\n          password: xxxxxxxxxxx\n          authorize: yes\n          # enable_secret_password\n          auth_pass: xxxxxxxxxxx\n          # private_key_file\n          ssh_keyfile: \"~/.ssh/id_rsa\"\n          version: 2\n          timeout: 10\n\nRole: ios_pull_config:\n- include_vars: \"../defaults/{{ os_family }}/connections.yml\"\n  when: (connections is undefined)\n\n- name: Fetching config from the remote node\n  ios_command:\n        provider: \"{{ connections.ssh }}\"\n        commands:\n          - \"show {{ config }}\"\n  register: configuration\n\nPlaybook:\n- name: Pulling IOS/IOSv/IOSv-L2/IOS-XE/IOS-XEv (CSR-1000v) startup and running configs\n  hosts:\n    - iosv\n    - iosv_l2\n    - ios_xev\n  gather_facts: no\n  roles:\n    - { role: ios_pull_config, config: startup-config, with_date_time: 'no' }\n    - { role: ios_pull_config, config: running-config, with_date_time: 'no' }\n\nEXPECTED RESULTS\nStartup & running configurations from the target IOSv node.\nACTUAL RESULTS: CLI\n...\n<172.21.100.111> using connection plugin network_cli\n<172.21.100.111> socket_path: \nfatal: [XEv_Spine_11]: FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell\"\n}\n...\n\nACTUAL RESULTS: log\n...\n2017-06-27 16:48:19,004 p=27279 u=root |  task path: /home/actionmystique/Ansible/Roles/roles/ios_pull_config/tasks/main.yml:77\n2017-06-27 16:48:21,476 p=27374 u=root |  creating new control socket for host 172.21.100.111:22 as user admin\n2017-06-27 16:48:21,476 p=27374 u=root |  control socket path is /root/.ansible/pc/4746f9877e\n2017-06-27 16:48:21,477 p=27374 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible\n2017-06-27 16:48:21,477 p=27374 u=root |  using connection plugin network_cli\n2017-06-27 16:48:21,495 p=27375 u=root |  creating new control socket for host 172.21.100.112:22 as user admin\n2017-06-27 16:48:21,496 p=27375 u=root |  control socket path is /root/.ansible/pc/96255242f1\n2017-06-27 16:48:21,496 p=27375 u=root |  current working directory is /media/actionmystique/SAMSUNG-850-Ext4/Labs/GNS3/git-Public-Labs-Collection/CCNP/ROUTE/VRF/Jean-Christophe_Manciot/1+/AD between VRFs in Leaf & Spine DC-linux/IOS-XE 16.5.1/Ansible\n2017-06-27 16:48:21,496 p=27375 u=root |  using connection plugin network_cli\n2017-06-27 16:48:21,585 paramiko.transport starting thread (client mode): 0x9f70b2d0L\n2017-06-27 16:48:21,586 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1\n2017-06-27 16:48:21,586 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25\n2017-06-27 16:48:21,586 paramiko.transport Connected (version 2.0, client Cisco-1.25)\n2017-06-27 16:48:21,587 paramiko.transport starting thread (client mode): 0xb9ea82d0L\n2017-06-27 16:48:21,587 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False\n2017-06-27 16:48:21,587 paramiko.transport Local version/idstring: SSH-2.0-paramiko_2.2.1\n2017-06-27 16:48:21,587 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1\n2017-06-27 16:48:21,587 paramiko.transport HostKey agreed: ssh-rsa\n2017-06-27 16:48:21,587 paramiko.transport Remote version/idstring: SSH-2.0-Cisco-1.25\n2017-06-27 16:48:21,587 paramiko.transport Cipher agreed: aes128-ctr\n2017-06-27 16:48:21,587 paramiko.transport MAC agreed: hmac-sha1\n2017-06-27 16:48:21,587 paramiko.transport Connected (version 2.0, client Cisco-1.25)\n2017-06-27 16:48:21,587 paramiko.transport Compression agreed: none\n2017-06-27 16:48:21,588 paramiko.transport kex algos:[u'diffie-hellman-group-exchange-sha1', u'diffie-hellman-group14-sha1'] server key:[u'ssh-rsa'] client encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] server encrypt:[u'aes128-ctr', u'aes192-ctr', u'aes256-ctr', u'aes128-cbc', u'3des-cbc', u'aes192-cbc', u'aes256-cbc'] client mac:[u'hmac-sha1', u'hmac-sha1-96'] server mac:[u'hmac-sha1', u'hmac-sha1-96'] client compress:[u'none'] server compress:[u'none'] client lang:[u''] server lang:[u''] kex follows?False\n2017-06-27 16:48:21,588 paramiko.transport Kex agreed: diffie-hellman-group-exchange-sha1\n2017-06-27 16:48:21,589 paramiko.transport HostKey agreed: ssh-rsa\n2017-06-27 16:48:21,589 paramiko.transport Cipher agreed: aes128-ctr\n2017-06-27 16:48:21,589 paramiko.transport MAC agreed: hmac-sha1\n2017-06-27 16:48:21,589 paramiko.transport Compression agreed: none\n2017-06-27 16:48:21,789 paramiko.transport Got server p (2048 bits)\n2017-06-27 16:48:21,790 paramiko.transport Got server p (2048 bits)\n2017-06-27 16:48:21,884 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>\n2017-06-27 16:48:21,885 paramiko.transport Switch to new keys ...\n2017-06-27 16:48:21,890 p=27374 u=root |  connecting to host 172.21.100.111 returned an error\n2017-06-27 16:48:21,890 p=27374 u=root |  Unicode-objects must be encoded before hashing\n2017-06-27 16:48:21,911 paramiko.transport kex engine KexGex specified hash_algo <built-in function openssl_sha1>\n2017-06-27 16:48:21,912 paramiko.transport Switch to new keys ...\n2017-06-27 16:48:21,915 p=27375 u=root |  connecting to host 172.21.100.112 returned an error\n2017-06-27 16:48:21,915 p=27375 u=root |  Unicode-objects must be encoded before hashing\n2017-06-27 16:48:21,986 paramiko.transport EOF in transport thread\n2017-06-27 16:48:22,012 paramiko.transport EOF in transport thread\n2017-06-27 16:48:31,524 p=27279 u=root |  fatal: [XEv_Spine_11]: FAILED! => {\n    \"changed\": false, \n    \"failed\": true, \n    \"msg\": \"unable to open shell. Please see: https://docs.ansible.com/ansible/network_debug_troubleshooting.html#unable-to-open-shell\"\n}\n...\n\nACTUAL RESULTS: Manual SSH\n# ssh admin@172.21.100.111\nCC\n**************************************************************************\n* IOSv is strictly limited to use for evaluation, demonstration and IOS  *\n* education. IOSv is provided as-is and is not supported by Cisco's      *\n* Technical Advisory Center. Any use or disclosure, in whole or in part, *\n* of the IOSv Software or Documentation to any third party for any       *\n* purposes is expressly prohibited except as otherwise authorized by     *\n* Cisco in writing.                                                      *\n**************************************************************************CC\nXEv_Spine_11# #sh run\nBuilding configuration...\nCurrent configuration : 12818 bytes\n!\n! Last configuration change at 14:35:32 UTC Tue Jun 27 2017\n!\nversion 16.4\n...\n\nWORKAROUND: older paramiko\nThe latest paramiko version is 2.2.1.\nWhen downgrading to paramiko 2.0.6, this issue is ... gone.\nI recommend to put a more stringent requirement on paramiko in requirements.txt until this issue is solved with more recent paramiko, for instance:\nparamiko <= 2.0.6\n\nThe issue begins with paramiko 2.1.0+"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "svpace"
                },
                "number": 15864,
                "resourcePath": "/ansible/ansible/issues/15864",
                "state": "CLOSED",
                "publishedAt": "2016-05-15T16:19:55Z",
                "closedAt": "2016-05-16T23:18:13Z",
                "title": "[modules-core: file] Allow for \"directory\" or \"link\" in place of \"file\" with state ",
                "bodyText": "Feature Idea\n\nANSIBLE VERSION\nansible 2.0.2.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nSUMMARY\nusing \"file\" to check for directories (or links) is not very intuitive.\nIt would be great if\ndirectory: path=x/y/z\n\ncould be used as an alias for\nfile: path=x/y/z state=directory"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "psi-4ward"
                },
                "number": 13887,
                "resourcePath": "/ansible/ansible/issues/13887",
                "state": "CLOSED",
                "publishedAt": "2016-01-14T13:32:19Z",
                "closedAt": "2017-11-22T00:36:04Z",
                "title": "GATHERING FACTS fails",
                "bodyText": "$ ansible --version\nansible 1.9.4\n\n$ uname -a\nLinux psi-nb 4.3.3-2-ARCH #1 SMP PREEMPT Wed Dec 23 20:09:18 CET 2015 x86_64 GNU/Linux\n\nGuest is CentOS 7 Vagrant BOX\n$ uname -a\nLinux jenkins 4.4.0-1.el7.elrepo.x86_64 #1 SMP Sun Jan 10 21:17:16 EST 2016 x86_64 x86_64 x86_64 GNU/Linux\n\nWhen i run vagrant provision or ansible-playbook ... gathering facts fails MOST time:\nPLAY [all] ******************************************************************** \n\nGATHERING FACTS *************************************************************** \n<127.0.0.1> ESTABLISH CONNECTION FOR USER: vagrant\n<127.0.0.1> REMOTE_MODULE setup\n<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r\" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=\"/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && chmod a+rx $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191 && echo $HOME/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191'                                                                                      \n<127.0.0.1> PUT /tmp/tmpOf1gPr TO /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup\n<127.0.0.1> EXEC ssh -C -tt -v -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/psi/.ansible/cp/ansible-ssh-%h-%p-%r\" -o StrictHostKeyChecking=no -o Port=2222 -o IdentityFile=\"/home/psi/o2/ansible/vagrant/jenkins/.vagrant/machines/jenkins/virtualbox/private_key\" -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=vagrant -o ConnectTimeout=10 127.0.0.1 /bin/sh -c 'LANG=C LC_CTYPE=C /usr/bin/python /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/setup; rm -rf /home/vagrant/.ansible/tmp/ansible-tmp-1452776926.62-156382542588191/ >/dev/null 2>&1'\nfailed: [jenkins] => {\"failed\": true, \"parsed\": false}\n{\"verbose_override\": true, \"changed\": false, \"ansible_facts\": {\"ansible_product_serial\": \"NA\", \"ansible_form_factor\": \"Other\", \"ansible_product_version\": \"1.2\", \"ansible_fips\": false, \"ansible_swaptotal_mb\": 1535, \"ansible_user_id\": \"vagrant\", \"module_setup\": true, \"ansible_userspace_bits\": \"64\", \"ansible_architecture\": \"x86_64\", \"ansible_distribution_version\": \"7.1.1503\", \"ansible_domain\": \"localdomain\", \"ansible_date_time\": {\"tz\": \"EST\", \"hour\": \"08\", \"time\": \"08:08:47\", \"epoch\": \"1452776927\", \"month\": \"01\", \"tz_offset\": \"-0500\", \"second\": \"47\", \"iso8601_micro\": \"2016-01-14T13:08:47.283819Z\", \"weekday\": \"Thursday\", \"year\": \"2016\", \"date\": \"2016-01-14\", \"iso8601\": \"2016-01-14T13:08:47Z\", \"day\": \"14\", \"minute\": \"08\"}, \"ansible_python_version\": \"2.7.5\", \"ansible_processor_cores\": 1, \"ansible_virtualization_role\": \"guest\", \"ansible_env\": {\"LANG\": \"C\", \"TERM\": \"xterm-256color\", \"SHELL\": \"/bin/bash\", \"XDG_RUNTIME_DIR\": \"/run/user/1000\", \"SHLVL\": \"2\", \"SSH_TTY\": \"/dev/pts/1\", \"LC_CTYPE\": \"C\", \"LESSOPEN\": \"||/usr/bin/lesspipe.sh %s\", \"PATH\": \"/usr/local/bin:/usr/bin\", \"PWD\": \"/home/vagrant\", \"LOGNAME\": \"vagrant\", \"USER\": \"vagrant\", \"MAIL\": \"/var/mail/vagrant\", \"HOME\": \"/home/vagrant\", \"SSH_CONNECTION\": \"10.0.2.2 41752 10.0.2.15 22\", \"XDG_SESSION_ID\": \"9\", \"SSH_CLIENT\": \"10.0.2.2 41752 22\", \"_\": \"/usr/bin/python\"}, \"ansible_processor_vcpus\": 1, \"ansible_docker0\": {\"macaddress\": \"02:42:33:3c:c5:e0\", \"interfaces\": [], \"mtu\": 1500, \"active\": false, \"promisc\": false, \"stp\": false, \"ipv4\": {\"netmask\": \"255.255.0.0\", \"network\": \"172.17.0.0\", \"address\": \"172.17.0.1\"}, \"device\": \"docker0\", \"type\": \"bridge\", \"id\": \"8000.0242333cc5e0\"}, \"ansible_bios_version\": \"VirtualBox\", \"ansible_processor\": [\"GenuineIntel\", \"Intel(R) Core(TM) i5-5200U CPU @ 2.20GHz\"], \"ansible_virtualization_type\": \"virtualbox\", \"ansible_lo\": {\"mtu\": 65536, \"active\": true, \"promisc\": false, \"ipv4\": {\"netmask\": \"255.0.0.0\", \"network\": \"127.0.0.0\", \"address\": \"127.0.0.1\"}, \"ipv6\": [{\"scope\": \"host\", \"prefix\": \"128\", \"address\": \"::1\"}], \"device\": \"lo\", \"type\": \"loopback\"}, \"ansible_memtotal_mb\": 2000, \"ansible_ssh_host_key_ecdsa_public\": \"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBK5Jp5g/+qSXGzxlC6ZlEQfUpiAuZRcXv1gFPe9h1f9T/yVCBqZs2fM37/6cIljYzSwxiJVSveGiomGLxyba9G0=\", \"ansible_default_ipv4\": {\"macaddress\": \"52:54:00:d3:eb:a4\", \"network\": \"10.0.2.0\", \"mtu\": 1500, \"alias\": \"eth0\", \"netmask\": \"255.255.255.0\", \"address\": \"10.0.2.15\", \"interface\": \"eth0\", \"type\": \"ether\", \"gateway\": \"10.0.2.2\"}, \"ansible_swapfree_mb\": 1535, \"ansible_default_ipv6\": {}, \"ansible_distribution_release\": \"Core\", \"ansible_system_vendor\": \"innotek GmbH\", \"ansible_os_family\": \"RedHat\", \"ansible_cmdline\": {\"LANG\": \"en_US.UTF-8\", \"systemd.debug\": true, \"BOOT_IMAGE\": \"/vmlinuz-4.4.0-1.el7.elrepo.x86_64\", \"biosdevname\": \"0\", \"quiet\": true, \"net.ifnames\": \"0\", \"rhgb\": true, \"rd.lvm.lv\": \"VolGroup00/LogVol00\", \"crashkernel\": \"auto\", \"console\": \"ttyS0,115200\", \"ro\": true, \"root\": \"/dev/mapper/VolGroup00-LogVol00\"}, \"ansible_user_gid\": 1000, \"ansible_selinux\": {\"status\": \"disabled\"}, \"ansible_userspace_architecture\": \"x86_64\", \"ansible_product_uuid\": \"NA\", \"ansible_system\": \"Linux\", \"ansible_pkg_mgr\": \"yum\", \"ansible_memfree_mb\": 1333, \"ansible_devices\": {\"sda\": {\"scheduler_mode\": \"deadline\", \"rotational\": \"1\", \"vendor\": \"ATA\", \"sectors\": \"83886080\", \"host\": \"\", \"sectorsize\": \"512\", \"removable\": \"0\", \"support_discard\": \"0\", \"model\": \"VBOX HARDDISK\", \"size\": \"40.00 GB\", \"holders\": [], \"partitions\": {\"sda2\": {\"start\": \"4096\", \"sectorsize\": 512, \"sectors\": \"409600\", \"size\": \"200.00 MB\"}, \"sda3\": {\"start\": \"413696\", \"sectorsize\": 512, \"sectors\": \"83472384\", \"size\": \"39.80 GB\"}, \"sda1\": {\"start\": \"2048\", \"sectorsize\": 512, \"sectors\": \"2048\", \"size\": \"1.00 MB\"}}}}, \"ansible_user_uid\": 1000, \"ansible_memory_mb\": {\"real\": {\"total\": 2000, \"free\": 1333, \"used\": 667}, \"swap\": {\"cached\": 0, \"total\": 1535, \"used\": 0, \"free\": 1535}, \"nocache\": {\"used\": 300, \"free\": 1700}}, \"ansible_distribution\": \"CentOS\", \"ansible_distribution_major_version\": \"7\", \"ansible_user_dir\": \"/home/vagrant\", \"ansible_processor_countOpenSSH_7.1p1, OpenSSL 1.0.2e 3 Dec 2015\ndebug1: Reading configuration data /home/psi/.ssh/config\ndebug1: Reading configuration data /etc/ssh/ssh_config\ndebug1: /etc/ssh/ssh_config line 20: Applying options for *\ndebug1: auto-mux: Trying existing master\ndebug1: mux_client_request_session: master session id: 2\nShared connection to 127.0.0.1 closed.\n\nanyone konws why?\nFor me the output looks like incomplete JSON"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "philltomlinson"
                },
                "number": 13062,
                "resourcePath": "/ansible/ansible/issues/13062",
                "state": "CLOSED",
                "publishedAt": "2015-11-06T16:24:37Z",
                "closedAt": "2015-11-11T12:00:28Z",
                "title": "Shell with_items Prompts for ssh key password on second command",
                "bodyText": "I have seen something when running the shell module along with a with_items where it now prompts for a key pass phrase. It runs the first command with no password prompt but then on the second one it prompts for a key password and fails the task:\n-name: Shell commands\n shell: \"{{item}}\"\n with_items:\n  - echo \"test 1\"\n  - echo \"test 2\"\n\nWhen running the playbook you get prompted for a key passphrase:\nEnter passphrase for key '/home/vagrant/.ssh/id_rsa':\n\nThis previously worked on the v2.0.0-0.3.beta1 tag but no longer works on later tags and on the dev branch."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "gholms"
                },
                "number": 3978,
                "resourcePath": "/ansible/ansible/issues/3978",
                "state": "CLOSED",
                "publishedAt": "2013-08-30T23:20:12Z",
                "closedAt": "2014-02-13T20:42:30Z",
                "title": "Add a way to make boto not verify SSL certs",
                "bodyText": "Starting a year ago with version 2.6.0, boto began verifying servers' SSL certificates by default.  Since most Eucalyptus and Nova installs have self-signed certs, this breaks Ansible's AWS-related modules when they're pointed at one of those clouds using a relatively current version of boto.\nThey added a validate_certs=False arg to calls like boto.connect_ec2_endpoint one can use to disable this behavior, but right now Ansible doesn't have a way to trigger that.  Modules like ec2, ec2_elb, and so on would benefit from a parameter that lets one turn cert verification off when they need to talk to services with self-signed certs.\nEPEL bug that triggered this report:  https://bugzilla.redhat.com/show_bug.cgi?id=1003105"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "deimosfr"
                },
                "number": 9811,
                "resourcePath": "/ansible/ansible/issues/9811",
                "state": "CLOSED",
                "publishedAt": "2014-12-14T18:46:45Z",
                "closedAt": "2016-07-12T18:17:33Z",
                "title": "ansible-galaxy file parameter full path",
                "bodyText": "Hi,\nIt would be nice when you setup a galaxy dependencies file like this:\njdauphant.nginx,v1.1.1,nginx\n\nto be able to specify the name of the final module path. The goal is when i launch it like this:\nansible-galaxy install -r galaxy.txt -p roles\n\nI get a folder 'roles/nginx' which is better than having 'roles/jdauphant.nginx'\nThanks"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "bobrik"
                },
                "number": 10673,
                "resourcePath": "/ansible/ansible/issues/10673",
                "state": "CLOSED",
                "publishedAt": "2015-04-13T10:01:05Z",
                "closedAt": "2015-04-13T17:38:43Z",
                "title": "JSON output gets truncated on spaces",
                "bodyText": "Issue Type:\nBug report\nAnsible Version:\nλ ansible --version\nansible 2.0.0\n  configured module search path = ../library\n\nand\nλ ansible --version\nansible 1.9.0.1\n  configured module search path = ../library\n\nEnvironment:\nN/A\nSummary:\ncopy: content=\"{{ some_data|to_json }}\\n\" dest=/tmp/wtf.yaml results in truncated json.\nSteps To Reproduce:\nExample playbook:\n- hosts: web414\n  vars:\n    some_data:\n      something:\n        wow: \"hey ho\"\n  tasks:\n    - name: write some json\n      copy: content=\"{{ some_data|to_json }}\\n\" dest=/tmp/wtf.yaml\nResulting file:\nweb414 ~ # cat /tmp/wtf.yaml\n\"{\"something\": {\"wow\": \"heyweb414 ~ #\n\nExpected Results:\nFile should not be truncated.\nActual Results:\nFile gets truncated."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "omame"
                },
                "number": 14148,
                "resourcePath": "/ansible/ansible/issues/14148",
                "state": "CLOSED",
                "publishedAt": "2016-01-27T09:50:24Z",
                "closedAt": "2017-04-12T07:10:59Z",
                "title": "Misleading error when a file isn't found in a role",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\n\nansible 2.0.0.2\nconfig file = /home/user/ansible/ansible.cfg\nconfigured module search path = Default w/o overrides\n\nAnsible Configuration:\nClean.\nEnvironment:\nUbuntu 14.04.\nSummary:\nWhen a copy or assemble operation can't access the src file in a role a misleading error suggests to look at the playbooks directory.\nSteps To Reproduce:\nuser@server:~/ansible$ touch roles/test/files/snari\nuser@server:~/ansible$ cat playbooks/test.yml\n\n---\n- hosts: all\n  roles:\n    - test\n\nuser@server:~/ansible$ cat roles/test/tasks/main.yml\n\n---\n- copy:\n    src: snari\n    dest: /tmp/snari\n\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nok: [server]\n\nPLAY RECAP *********************************************************************\nserver                     : ok=1    changed=0    unreachable=0    failed=0\n\nuser@server:~/ansible$ mv roles/test/files/snari{,_}\n\nExpected Results:\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nfatal: [server]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"could not find snari in the search path: /home/user/ansible/roles/test/files, /home/user/ansible/playbooks\"}\n\nPLAY RECAP *********************************************************************\nserver                     : ok=0    changed=0    unreachable=0    failed=1\n\nActual Results:\nuser@server:~/ansible$ ansible-playbook -i inventory/local playbooks/test.yml\n\nPLAY ***************************************************************************\n\nTASK [test : copy] *************************************************************\nfatal: [server]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"could not find src=/home/user/ansible/playbooks/snari\"}\n\nPLAY RECAP *********************************************************************\nserver                     : ok=0    changed=0    unreachable=0    failed=1"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "profhase"
                },
                "number": 23955,
                "resourcePath": "/ansible/ansible/issues/23955",
                "state": "CLOSED",
                "publishedAt": "2017-04-25T09:35:05Z",
                "closedAt": "2017-04-25T14:49:20Z",
                "title": "Copy module shows 'changed' in check mode",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\ncopy\nANSIBLE VERSION\n2.3.0.0\n\nCONFIGURATION\nNA\nOS / ENVIRONMENT\nNA\nSUMMARY\nWhen using the copy module to copy a file into a directory on the target system, the check mode yields changed though there is no change on the target system. The problem only occurs if the path\nis given without trailing slash.\nSTEPS TO REPRODUCE\nCall in check mode after file exists:\n- name: copy file\n  copy:\n    src=myfile\n    dest=/path/to/folder\nEXPECTED RESULTS\nchanged: false\nACTUAL RESULTS\nchanged: true\nThis effect does not occur if dest is given with a trailing slash:\n- name: copy file\n  copy:\n    src=myfile\n    dest=/path/to/folder/"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "sascha-egerer"
                },
                "number": 12412,
                "resourcePath": "/ansible/ansible/issues/12412",
                "state": "CLOSED",
                "publishedAt": "2015-09-17T08:42:11Z",
                "closedAt": "2015-09-17T15:13:35Z",
                "title": "Ansible fails to load Yaml file with equal-sign as value",
                "bodyText": "Ansible fails when loading a Yaml file like this:\n--- \n  foo: \n    - =\nThe Yaml file is valid (see http://www.yamllint.com/) but ansible fails with ERROR: Syntax Error while loading YAML script ...\nPutting quotes around the equal-sign works but i can't do that in my case as this file will be generated by a Yaml parser. So using a workaround is not an option."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "fulminemizzega"
                },
                "number": 22320,
                "resourcePath": "/ansible/ansible/issues/22320",
                "state": "CLOSED",
                "publishedAt": "2017-03-06T16:06:43Z",
                "closedAt": "2018-09-25T20:09:44Z",
                "title": "fedora 25 dnf installed packages not marked as user installed",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\ndnf\nANSIBLE VERSION\n\nansible 2.2.1.0\n  config file = /home/edo/Sviluppo/fedora-vmware/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\nansible.cfg\n[defaults]\ninventory=inventory\n\n[privilege_escalation]\nbecome_method=sudo\nbecome_user=root\n\nOS / ENVIRONMENT\nFedora 25\nSUMMARY\n\nWhen running the playbook below not every package is marked as user installed, this means for example that running \"dnf autoremove\" afterwards will remove packages installed by the playbook.\nSTEPS TO REPRODUCE\nRun the following playbook, then run \"dnf history userinstalled\":\n\n---\n- name: Fedora on VMware post-install\n  hosts: localhost\n  become: yes\n  tasks:\n    - name: Update everything\n      dnf: name=\"*\" state=latest\n    - name: Install packages\n      dnf: name={{ item }} state=latest\n      with_items:\n        - vim-enhanced\n        - emacs\n        - tmux\n        - powertop\n        - gnome-tweak-tool\n        - libselinux-python\n        - dconf-editor\n        - open-vm-tools-desktop\n        - git\n...\ndnf history userinstalled output:\nPackages installed by user\nanaconda-25.20.9-1.fc25.x86_64\nansible-2.2.1.0-1.fc25.noarch\ndocker-2:1.12.6-6.gitae7d637.fc25.x86_64\ndracut-live-044-78.fc25.x86_64\ngnome-tweak-tool-3.22.0-1.fc25.noarch\ngoogle-chrome-stable-56.0.2924.87-1.x86_64\nkernel-modules-extra-4.9.12-200.fc25.x86_64\nlangpacks-en-1.0-8.fc25.noarch\nmemtest86+-5.01-15.fc25.x86_64\npython2-dnf-1.1.10-5.fc25.noarch\nsyslinux-6.04-0.1.fc25.x86_64\ntmux-2.2-3.fc25.x86_64\n\nEXPECTED RESULTS\nAll the packages in the playbook should be listed as user installed, so that 'dnf autoremove' will not remove them. A possible workaround is running 'dnf mark '.\nPlus, if you support also dnf autoremove #20333, then interesting things will happen."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "bcomnes"
                },
                "number": 10077,
                "resourcePath": "/ansible/ansible/issues/10077",
                "state": "CLOSED",
                "publishedAt": "2015-01-23T20:12:23Z",
                "closedAt": "2015-07-02T13:38:15Z",
                "title": "`defaults/main.yml` overriding variables `include_vars:` in roles",
                "bodyText": "In defaults/main.yml:\n---\nfoo: bar\nIn vars/Darwin.yml:\n---\nfoo: baz\nIn tasks/main.yml:\n- name: include env specific vars\n  include_vars: \"{{ item }}\"\n  with_first_found:\n    - \"{{ ansible_distribution }}.yml\"\n    - \"{{ ansible_os_family }}.yml\"\n- debug: var=foo\nAssume this is run on a Darwin machine we get\nfoo: bar\nwhere I would expect to get baz.\nThe work around is to ditch the defaults folder and instead load it with a defaults file with   with_first_found:\nMaybe I am doing something backwards here, but the defaults folder seem to take higher precedence over including variables selectively included in the task."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "samvarankashyap"
                },
                "number": 17033,
                "resourcePath": "/ansible/ansible/issues/17033",
                "state": "CLOSED",
                "publishedAt": "2016-08-10T17:33:25Z",
                "closedAt": "2016-08-10T18:35:42Z",
                "title": "ansible os_server module doesnot work with async_status",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\n\nmodule : os_server\nANSIBLE VERSION\n\nansible 2.1.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\n\nCONFIGURATION\n\nOS / ENVIRONMENT\n\nN/A\nSUMMARY\n\nAnsible is unable to parse the os_server module output using async_status .\nSTEPS TO REPRODUCE\n\nThe following tasks are to be written\n\nname: \"Async:: provision/deprovision os_server resources by looping on count\"\nos_server:\nstate: \"{{ instance.4 }}\"\nauth:\nauth_url: \"{{ instance.0 }}\"\nusername: \"{{ instance.1 }}\"\npassword: \"{{ instance.2 }}\"\nproject_name: \"{{ instance.3 }}\"\nname: \"{{ instance.9 }}{{ instance.10 }}{{ instance.11 }}\"\nimage: \"{{ instance.5 }}\"\nkey_name: \"{{ instance.6  }}\"\napi_timeout: 99999\nflavor: \"{{ instance.7 }}\"\nnetwork: \"{{ instance.8 }}\"\nwith_nested:\n\n[\"{{ endpoint }}\"]\n[\"{{ username }}\"]\n[\"{{ password }}\"]\n[\"{{ project }}\"]\n[\"{{ state }}\"]\n[\"{{ res_def['image'] }}\"]\n[\"{{ res_def['keypair']  }}\"]\n[\"{{ res_def['flavor']  }}\"]\n[\"{{ res_def['networks'][0] }}\"]\n[\"{{ res_grp_name }}\"]\n[\"{{ res_def['res_name'] }}\"]\n\"{{ res_count.stdout }}\"\nloop_control:\nloop_var: instance\nasync: 1000\npoll: 0\nregister: res_def_output\nwhen: async == true\n\n\nname: 'check on fire and forget task'\nasync_status: jid={{ item.ansible_job_id }}\nregister: job_result\nuntil: job_result.finished\nretries: 30\nwith_items: \"{{ res_def_output['results'] }}\"\n\nGives following error :\nEXPECTED RESULTS\n\nexpected results are the output of the booted instance\nACTUAL RESULTS\n\nAnsible is unable to parse the output of the job\n\n<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: root\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `\" && echo ansible-tmp-1470849705.2-237917709656041=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041 `\" ) && sleep 0'                                                                                              \n<127.0.0.1> PUT /tmp/tmpXkXV6P TO /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849705.2-237917709656041/\" > /dev/null 2>&1 && sleep 0'                                                                         \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (29 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 1, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `\" && echo ansible-tmp-1470849710.29-212087273581974=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974 `\" ) && sleep 0'                                                                                           \n<127.0.0.1> PUT /tmp/tmpvCVj1J TO /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849710.29-212087273581974/\" > /dev/null 2>&1 && sleep 0'                                                                       \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (28 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 2, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `\" && echo ansible-tmp-1470849715.39-35966468008926=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926 `\" ) && sleep 0'                                                                                              \n<127.0.0.1> PUT /tmp/tmpj1oc9X TO /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849715.39-35966468008926/\" > /dev/null 2>&1 && sleep 0'                                                                         \nFAILED - RETRYING: TASK: openstack : check on fire and forget task (27 retries left).Result was: {\"ansible_job_id\": \"309653937203.28231\", \"attempts\": 3, \"changed\": false, \"finished\": 0, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"retries\": 30, \"started\": 1}\n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `\" && echo ansible-tmp-1470849720.47-221980927570550=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550 `\" ) && sleep 0'                                                                                           \n<127.0.0.1> PUT /tmp/tmpUBrX3j TO /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849720.47-221980927570550/\" > /dev/null 2>&1 && sleep 0'                                                                       \nfailed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'309653937203.28231', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 0], u'results_file': u'/root/.ansible_async/309653937203.28231'}) => {\"ansible_job_id\": \"309653937203.28231\", \"failed\": true, \"finished\": 1, \"invocation\": {\"module_args\": {\"jid\": \"309653937203.28231\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"item\": {\"ansible_job_id\": \"309653937203.28231\", \"instance\": [\"http://localhost:5000/v2.0\", \"e2e-openstack\", \"rgHdUfMqshXWSBYdlfIp\", \"e2e-openstack\", \"present\", \"rhel-6.5_jeos\", \"ci-factory\", \"m1.small\", \"e2e-openstack\", \"testgroup1\", \"ano_inst\", 0], \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"started\": 1}, \"msg\": \"Could not parse job output: No handlers could be found for logger \\\"keystoneauth.identity.base\\\"\\n\\n{\\\"invocation\\\": {\\\"module_args\\\": {\\\"auth_type\\\": null, \\\"availability_zone\\\": null, \\\"image\\\": \\\"rhel-6.5_jeos\\\", \\\"image_exclude\\\": \\\"(deprecated)\\\", \\\"flavor_include\\\": null, \\\"meta\\\": null, \\\"flavor\\\": \\\"m1.small\\\", \\\"cloud\\\": null, \\\"scheduler_hints\\\": null, \\\"boot_from_volume\\\": false, \\\"userdata\\\": null, \\\"network\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"nics\\\": [], \\\"floating_ips\\\": null, \\\"flavor_ram\\\": null, \\\"volume_size\\\": false, \\\"state\\\": \\\"present\\\", \\\"auto_ip\\\": true, \\\"security_groups\\\": [\\\"default\\\"], \\\"config_drive\\\": false, \\\"volumes\\\": [], \\\"key_name\\\": \\\"ci-factory\\\", \\\"api_timeout\\\": 99999, \\\"auth\\\": {\\\"username\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"project_name\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"password\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"auth_url\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"}, \\\"endpoint_type\\\": \\\"public\\\", \\\"boot_volume\\\": null, \\\"key\\\": null, \\\"cacert\\\": null, \\\"wait\\\": true, \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"region_name\\\": null, \\\"timeout\\\": 180, \\\"cert\\\": null, \\\"terminate_volume\\\": false, \\\"verify\\\": true, \\\"floating_ip_pools\\\": null}}, \\\"openstack\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.91\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.183.233\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.91\\\", \\\"10.8.183.233\\\"]}, \\\"accessIPv4\\\": \\\"10.8.183.233\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.183.233\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.91\\\", \\\"interface_ip\\\": \\\"10.8.183.233\\\", \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"created\\\": \\\"2016-08-10T17:21:46Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"B2zNdwbcP8ha\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_0\\\"}, \\\"changed\\\": true, \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"server\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.91\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:da:36:f5\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.183.233\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.91\\\", \\\"10.8.183.233\\\"]}, \\\"accessIPv4\\\": \\\"10.8.183.233\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.183.233\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.91\\\", \\\"interface_ip\\\": \\\"10.8.183.233\\\", \\\"name\\\": \\\"testgroup1_ano_inst_0\\\", \\\"created\\\": \\\"2016-08-10T17:21:46Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"B2zNdwbcP8ha\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_0\\\"}}\\n{\\\"msg\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/async_wrapper\\\\\\\", line 89, in _run_module\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/__init__.py\\\\\\\", line 339, in loads\\\\n    return _default_decoder.decode(s)\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 364, in decode\\\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 382, in raw_decode\\\\n    raise ValueError(\\\\\\\"No JSON object could be decoded\\\\\\\")\\\\nValueError: No JSON object could be decoded\\\\n\\\", \\\"failed\\\": 1, \\\"cmd\\\": \\\"/root/.ansible/tmp/ansible-tmp-1470849702.44-113930776716519/os_server\\\", \\\"data\\\": \\\"No handlers could be found for logger \\\\\\\"keystoneauth.identity.base\\\\\\\"\\\\n\\\\n{\\\\\\\"invocation\\\\\\\": {\\\\\\\"module_args\\\\\\\": {\\\\\\\"auth_type\\\\\\\": null, \\\\\\\"availability_zone\\\\\\\": null, \\\\\\\"image\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\", \\\\\\\"image_exclude\\\\\\\": \\\\\\\"(deprecated)\\\\\\\", \\\\\\\"flavor_include\\\\\\\": null, \\\\\\\"meta\\\\\\\": null, \\\\\\\"flavor\\\\\\\": \\\\\\\"m1.small\\\\\\\", \\\\\\\"cloud\\\\\\\": null, \\\\\\\"scheduler_hints\\\\\\\": null, \\\\\\\"boot_from_volume\\\\\\\": false, \\\\\\\"userdata\\\\\\\": null, \\\\\\\"network\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"nics\\\\\\\": [], \\\\\\\"floating_ips\\\\\\\": null, \\\\\\\"flavor_ram\\\\\\\": null, \\\\\\\"volume_size\\\\\\\": false, \\\\\\\"state\\\\\\\": \\\\\\\"present\\\\\\\", \\\\\\\"auto_ip\\\\\\\": true, \\\\\\\"security_groups\\\\\\\": [\\\\\\\"default\\\\\\\"], \\\\\\\"config_drive\\\\\\\": false, \\\\\\\"volumes\\\\\\\": [], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"api_timeout\\\\\\\": 99999, \\\\\\\"auth\\\\\\\": {\\\\\\\"username\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"project_name\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"password\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"auth_url\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\"}, \\\\\\\"endpoint_type\\\\\\\": \\\\\\\"public\\\\\\\", \\\\\\\"boot_volume\\\\\\\": null, \\\\\\\"key\\\\\\\": null, \\\\\\\"cacert\\\\\\\": null, \\\\\\\"wait\\\\\\\": true, \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"region_name\\\\\\\": null, \\\\\\\"timeout\\\\\\\": 180, \\\\\\\"cert\\\\\\\": null, \\\\\\\"terminate_volume\\\\\\\": false, \\\\\\\"verify\\\\\\\": true, \\\\\\\"floating_ip_pools\\\\\\\": null}}, \\\\\\\"openstack\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"10.8.183.233\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\", \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, {\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Default security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"metadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\\\\\", \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:46Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754241a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"B2zNdwbcP8ha\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_drive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\"}, \\\\\\\"changed\\\\\\\": true, \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"server\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:da:36:f5\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d83a4842-b362-44fe-8f73-dc5f8ee47df5\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"10.8.183.233\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\", \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, {\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Default security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"metadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"45cbafb4a5df6c815398bc435ef016c872014a6bb6008d544875210f\\\\\\\", \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.91\\\\\\\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.183.233\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:46Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754241a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"B2zNdwbcP8ha\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_drive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_0\\\\\\\"}}\\\\n\\\", \\\"ansible_job_id\\\": \\\"309653937203.28231\\\"}\", \"results_file\": \"/root/.ansible_async/309653937203.28231\", \"started\": 1}                                                                                                                                               \n<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p \"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `\" && echo ansible-tmp-1470849720.76-738324392915=\"` echo $HOME/.ansible/tmp/ansible-tmp-1470849720.76-738324392915 `\" ) && sleep 0'                                                                                                    \n<127.0.0.1> PUT /tmp/tmpvHYKYF TO /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status\n<127.0.0.1> EXEC /bin/sh -c 'LANG=en_US.UTF-8 LC_ALL=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 /usr/bin/python /root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/async_status; rm -rf \"/root/.ansible/tmp/ansible-tmp-1470849720.76-738324392915/\" > /dev/null 2>&1 && sleep 0'                                                                             \nfailed: [localhost] (item={'_ansible_no_log': False, u'ansible_job_id': u'557360551178.28260', u'started': 1, '_ansible_item_result': True, u'instance': [u'http://localhost:5000/v2.0', u'e2e-openstack', u'rgHdUfMqshXWSBYdlfIp', u'e2e-openstack', u'present', u'rhel-6.5_jeos', u'ci-factory', u'm1.small', u'e2e-openstack', u'testgroup1', u'ano_inst', 1], u'results_file': u'/root/.ansible_async/557360551178.28260'}) => {\"ansible_job_id\": \"557360551178.28260\", \"failed\": true, \"finished\": 1, \"invocation\": {\"module_args\": {\"jid\": \"557360551178.28260\", \"mode\": \"status\"}, \"module_name\": \"async_status\"}, \"item\": {\"ansible_job_id\": \"557360551178.28260\", \"instance\": [\"http://localhost:5000/v2.0\", \"e2e-openstack\", \"rgHdUfMqshXWSBYdlfIp\", \"e2e-openstack\", \"present\", \"rhel-6.5_jeos\", \"ci-factory\", \"m1.small\", \"e2e-openstack\", \"testgroup1\", \"ano_inst\", 1], \"results_file\": \"/root/.ansible_async/557360551178.28260\", \"started\": 1}, \"msg\": \"Could not parse job output: No handlers could be found for logger \\\"keystoneauth.identity.base\\\"\\n\\n{\\\"invocation\\\": {\\\"module_args\\\": {\\\"auth_type\\\": null, \\\"availability_zone\\\": null, \\\"image\\\": \\\"rhel-6.5_jeos\\\", \\\"image_exclude\\\": \\\"(deprecated)\\\", \\\"flavor_include\\\": null, \\\"meta\\\": null, \\\"flavor\\\": \\\"m1.small\\\", \\\"cloud\\\": null, \\\"scheduler_hints\\\": null, \\\"boot_from_volume\\\": false, \\\"userdata\\\": null, \\\"network\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"nics\\\": [], \\\"floating_ips\\\": null, \\\"flavor_ram\\\": null, \\\"volume_size\\\": false, \\\"state\\\": \\\"present\\\", \\\"auto_ip\\\": true, \\\"security_groups\\\": [\\\"default\\\"], \\\"config_drive\\\": false, \\\"volumes\\\": [], \\\"key_name\\\": \\\"ci-factory\\\", \\\"api_timeout\\\": 99999, \\\"auth\\\": {\\\"username\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"project_name\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"password\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\", \\\"auth_url\\\": \\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\"}, \\\"endpoint_type\\\": \\\"public\\\", \\\"boot_volume\\\": null, \\\"key\\\": null, \\\"cacert\\\": null, \\\"wait\\\": true, \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"region_name\\\": null, \\\"timeout\\\": 180, \\\"cert\\\": null, \\\"terminate_volume\\\": false, \\\"verify\\\": true, \\\"floating_ip_pools\\\": null}}, \\\"openstack\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.92\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.182.45\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.92\\\", \\\"10.8.182.45\\\"]}, \\\"accessIPv4\\\": \\\"10.8.182.45\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.182.45\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.92\\\", \\\"interface_ip\\\": \\\"10.8.182.45\\\", \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"created\\\": \\\"2016-08-10T17:21:48Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"iRvzMzj2Q33e\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_1\\\"}, \\\"changed\\\": true, \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"server\\\": {\\\"OS-EXT-STS:task_state\\\": null, \\\"addresses\\\": {\\\"e2e-openstack\\\": [{\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"172.16.100.92\\\", \\\"OS-EXT-IPS:type\\\": \\\"fixed\\\"}, {\\\"OS-EXT-IPS-MAC:mac_addr\\\": \\\"fa:16:3e:89:04:7c\\\", \\\"version\\\": 4, \\\"addr\\\": \\\"10.8.182.45\\\", \\\"OS-EXT-IPS:type\\\": \\\"floating\\\"}]}, \\\"image\\\": {\\\"id\\\": \\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\", \\\"name\\\": \\\"rhel-6.5_jeos\\\"}, \\\"OS-EXT-STS:vm_state\\\": \\\"active\\\", \\\"OS-SRV-USG:launched_at\\\": \\\"2016-08-10T17:21:52.000000\\\", \\\"NAME_ATTR\\\": \\\"name\\\", \\\"flavor\\\": {\\\"id\\\": \\\"2\\\", \\\"name\\\": \\\"m1.small\\\"}, \\\"az\\\": \\\"nova\\\", \\\"id\\\": \\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\", \\\"cloud\\\": \\\"defaults\\\", \\\"user_id\\\": \\\"9c770dbddda444799e627004fee26e0a\\\", \\\"OS-DCF:diskConfig\\\": \\\"MANUAL\\\", \\\"networks\\\": {\\\"e2e-openstack\\\": [\\\"172.16.100.92\\\", \\\"10.8.182.45\\\"]}, \\\"accessIPv4\\\": \\\"10.8.182.45\\\", \\\"accessIPv6\\\": \\\"\\\", \\\"security_groups\\\": [{\\\"id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"name\\\": \\\"default\\\", \\\"security_group_rules\\\": [{\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\"}, {\\\"direction\\\": \\\"ingress\\\", \\\"protocol\\\": null, \\\"remote_ip_prefix\\\": null, \\\"port_range_max\\\": null, \\\"security_group_id\\\": \\\"df1a797b-009c-4685-a7c9-43863c36d653\\\", \\\"port_range_min\\\": null, \\\"ethertype\\\": \\\"IPv4\\\", \\\"id\\\": \\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\"}], \\\"description\\\": \\\"Default security group\\\"}], \\\"key_name\\\": \\\"ci-factory\\\", \\\"progress\\\": 0, \\\"OS-EXT-STS:power_state\\\": 1, \\\"OS-EXT-AZ:availability_zone\\\": \\\"nova\\\", \\\"metadata\\\": {}, \\\"status\\\": \\\"ACTIVE\\\", \\\"updated\\\": \\\"2016-08-10T17:21:52Z\\\", \\\"hostId\\\": \\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\", \\\"HUMAN_ID\\\": true, \\\"OS-SRV-USG:terminated_at\\\": null, \\\"public_v4\\\": \\\"10.8.182.45\\\", \\\"public_v6\\\": \\\"\\\", \\\"private_v4\\\": \\\"172.16.100.92\\\", \\\"interface_ip\\\": \\\"10.8.182.45\\\", \\\"name\\\": \\\"testgroup1_ano_inst_1\\\", \\\"created\\\": \\\"2016-08-10T17:21:48Z\\\", \\\"tenant_id\\\": \\\"f1dda47890754241a3e111f9b7394707\\\", \\\"region\\\": \\\"\\\", \\\"adminPass\\\": \\\"iRvzMzj2Q33e\\\", \\\"os-extended-volumes:volumes_attached\\\": [], \\\"volumes\\\": [], \\\"config_drive\\\": \\\"\\\", \\\"human_id\\\": \\\"testgroup1_ano_inst_1\\\"}}\\n{\\\"msg\\\": \\\"Traceback (most recent call last):\\\\n  File \\\\\\\"/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/async_wrapper\\\\\\\", line 89, in _run_module\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/__init__.py\\\\\\\", line 339, in loads\\\\n    return _default_decoder.decode(s)\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 364, in decode\\\\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\\\\n  File \\\\\\\"/usr/lib64/python2.7/json/decoder.py\\\\\\\", line 382, in raw_decode\\\\n    raise ValueError(\\\\\\\"No JSON object could be decoded\\\\\\\")\\\\nValueError: No JSON object could be decoded\\\\n\\\", \\\"failed\\\": 1, \\\"cmd\\\": \\\"/root/.ansible/tmp/ansible-tmp-1470849703.74-111819930279072/os_server\\\", \\\"data\\\": \\\"No handlers could be found for logger \\\\\\\"keystoneauth.identity.base\\\\\\\"\\\\n\\\\n{\\\\\\\"invocation\\\\\\\": {\\\\\\\"module_args\\\\\\\": {\\\\\\\"auth_type\\\\\\\": null, \\\\\\\"availability_zone\\\\\\\": null, \\\\\\\"image\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\", \\\\\\\"image_exclude\\\\\\\": \\\\\\\"(deprecated)\\\\\\\", \\\\\\\"flavor_include\\\\\\\": null, \\\\\\\"meta\\\\\\\": null, \\\\\\\"flavor\\\\\\\": \\\\\\\"m1.small\\\\\\\", \\\\\\\"cloud\\\\\\\": null, \\\\\\\"scheduler_hints\\\\\\\": null, \\\\\\\"boot_from_volume\\\\\\\": false, \\\\\\\"userdata\\\\\\\": null, \\\\\\\"network\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"nics\\\\\\\": [], \\\\\\\"floating_ips\\\\\\\": null, \\\\\\\"flavor_ram\\\\\\\": null, \\\\\\\"volume_size\\\\\\\": false, \\\\\\\"state\\\\\\\": \\\\\\\"present\\\\\\\", \\\\\\\"auto_ip\\\\\\\": true, \\\\\\\"security_groups\\\\\\\": [\\\\\\\"default\\\\\\\"], \\\\\\\"config_drive\\\\\\\": false, \\\\\\\"volumes\\\\\\\": [], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"api_timeout\\\\\\\": 99999, \\\\\\\"auth\\\\\\\": {\\\\\\\"username\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"project_name\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"password\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\", \\\\\\\"auth_url\\\\\\\": \\\\\\\"VALUE_SPECIFIED_IN_NO_LOG_PARAMETER\\\\\\\"}, \\\\\\\"endpoint_type\\\\\\\": \\\\\\\"public\\\\\\\", \\\\\\\"boot_volume\\\\\\\": null, \\\\\\\"key\\\\\\\": null, \\\\\\\"cacert\\\\\\\": null, \\\\\\\"wait\\\\\\\": true, \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"region_name\\\\\\\": null, \\\\\\\"timeout\\\\\\\": 180, \\\\\\\"cert\\\\\\\": null, \\\\\\\"terminate_volume\\\\\\\": false, \\\\\\\"verify\\\\\\\": true, \\\\\\\"floating_ip_pools\\\\\\\": null}}, \\\\\\\"openstack\\\\\\\": {\\\\\\\"OS-EXT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STS:vm_state\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770dbddda444799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"10.8.182.45\\\\\\\"]}, \\\\\\\"accessIPv4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"defaul \\\\\\\"security_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_gd\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"t security group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", tadata\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67, \\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100\", \\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:48Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47841a3e111f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"iRvzMzj2Q33e\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"conive\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\"}, \\\\\\\"changed\\\\\\\": true, \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"server\\\\\\\": {\\\\XT-STS:task_state\\\\\\\": null, \\\\\\\"addresses\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [{\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\.16.100.92\\\\\\\", \\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"fixed\\\\\\\"}, {\\\\\\\"OS-EXT-IPS-MAC:mac_addr\\\\\\\": \\\\\\\"fa:16:3e:89:04:7c\\\\\\\", \\\\\\\"version\\\\\\\": 4, \\\\\\\"addr\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\"OS-EXT-IPS:type\\\\\\\": \\\\\\\"floating\\\\\\\"}]}, \\\\\\\"image\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"3bcfd17c-6bf0-4134-ae7f-80bded8b46fd\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"rhel-6.5_jeos\\\\\\\"}, \\\\\\\"OS-EXT-STtate\\\\\\\": \\\\\\\"active\\\\\\\", \\\\\\\"OS-SRV-USG:launched_at\\\\\\\": \\\\\\\"2016-08-10T17:21:52.000000\\\\\\\", \\\\\\\"NAME_ATTR\\\\\\\": \\\\\\\"name\\\\\\\", \\\\\\\"flavor\\\\\\\": {\\\\\\\"id\\\\\\\": \\\\\\\"2\\\\\\\", \\\\\\\\\\\": \\\\\\\"m1.small\\\\\\\"}, \\\\\\\"az\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d658f3fa-92f6-4e30-b0fa-db6c59771d1a\\\\\\\", \\\\\\\"cloud\\\\\\\": \\\\\\\"defaults\\\\\\\", \\\\\\\"user_id\\\\\\\": \\\\\\\"9c770d44799e627004fee26e0a\\\\\\\", \\\\\\\"OS-DCF:diskConfig\\\\\\\": \\\\\\\"MANUAL\\\\\\\", \\\\\\\"networks\\\\\\\": {\\\\\\\"e2e-openstack\\\\\\\": [\\\\\\\"172.16.100.92\\\\\\\", \\\\\\\"10.8.182.45\\\\\\\"]}, \\\\\\\"accessI\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"accessIPv6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"security_groups\\\\\\\": [{\\\\\\\"id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"default\\\\\\\",ecurity_group_rules\\\\\\\": [{\\\\\\\"direction\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_i \\\\\\\"df1a797b-009c-4685-a7c9-43863c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"ade9fcb9-14c1-4975-a04d-6007f80005c1\\\\\\\"}, irection\\\\\\\": \\\\\\\"ingress\\\\\\\", \\\\\\\"protocol\\\\\\\": null, \\\\\\\"remote_ip_prefix\\\\\\\": null, \\\\\\\"port_range_max\\\\\\\": null, \\\\\\\"security_group_id\\\\\\\": \\\\\\\"df1a797b-009c-4685-a763c36d653\\\\\\\", \\\\\\\"port_range_min\\\\\\\": null, \\\\\\\"ethertype\\\\\\\": \\\\\\\"IPv4\\\\\\\", \\\\\\\"id\\\\\\\": \\\\\\\"d03e4bae-24b6-415a-a30c-ee0d060f566f\\\\\\\"}], \\\\\\\"description\\\\\\\": \\\\\\\"Defaulrity group\\\\\\\"}], \\\\\\\"key_name\\\\\\\": \\\\\\\"ci-factory\\\\\\\", \\\\\\\"progress\\\\\\\": 0, \\\\\\\"OS-EXT-STS:power_state\\\\\\\": 1, \\\\\\\"OS-EXT-AZ:availability_zone\\\\\\\": \\\\\\\"nova\\\\\\\", \\\\\\\"me\\\\\\\": {}, \\\\\\\"status\\\\\\\": \\\\\\\"ACTIVE\\\\\\\", \\\\\\\"updated\\\\\\\": \\\\\\\"2016-08-10T17:21:52Z\\\\\\\", \\\\\\\"hostId\\\\\\\": \\\\\\\"bd7d90d8ca358f34673eb32d9471d4d768480b46d4af9b933eca67e8\\\\\\\"HUMAN_ID\\\\\\\": true, \\\\\\\"OS-SRV-USG:terminated_at\\\\\\\": null, \\\\\\\"public_v4\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"public_v6\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"private_v4\\\\\\\": \\\\\\\"172.16.100.92\\\\\\\"interface_ip\\\\\\\": \\\\\\\"10.8.182.45\\\\\\\", \\\\\\\"name\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\", \\\\\\\"created\\\\\\\": \\\\\\\"2016-08-10T17:21:48Z\\\\\\\", \\\\\\\"tenant_id\\\\\\\": \\\\\\\"f1dda47890754211f9b7394707\\\\\\\", \\\\\\\"region\\\\\\\": \\\\\\\"\\\\\\\", \\\\\\\"adminPass\\\\\\\": \\\\\\\"iRvzMzj2Q33e\\\\\\\", \\\\\\\"os-extended-volumes:volumes_attached\\\\\\\": [], \\\\\\\"volumes\\\\\\\": [], \\\\\\\"config_dr\": \\\\\\\"\\\\\\\", \\\\\\\"human_id\\\\\\\": \\\\\\\"testgroup1_ano_inst_1\\\\\\\"}}\\\\n\\\", \\\"ansible_job_id\\\": \\\"557360551178.28260\\\"}\", \"results_file\": \"/root/.ansible_async/557360551178.282started\": 1}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "asad-at-srt"
                },
                "number": 2585,
                "resourcePath": "/ansible/ansible/issues/2585",
                "state": "CLOSED",
                "publishedAt": "2013-04-06T14:39:44Z",
                "closedAt": "2013-04-06T21:42:10Z",
                "title": "Please enhance when: to accept a list",
                "bodyText": "The semantics should be an AND of the items in the list\nwhen:\n\nexpression-1\nexpression-2\n\nshould be equivalent to:\nwhen: expression-1 and expression-2\nJustification:\nLong/complex expressions are made eminently more readable by breaking them down into components.\nThis is quite general, any boolean expression can be transformed into an AND-form with negation: a or b = ^(^a and ^b)"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "mmoya"
                },
                "number": 2290,
                "resourcePath": "/ansible/ansible/issues/2290",
                "state": "CLOSED",
                "publishedAt": "2013-03-03T20:48:14Z",
                "closedAt": "2013-03-04T04:39:39Z",
                "title": "include fails to expand host variables",
                "bodyText": "The playbook:\n\n---\n- hosts: localhost\n  vars:\n    - othertasks:\n       - othertask1\n       - othertask2\n  tasks:\n    - name: debug\n      debug: msg=\"tasks/$item.yml\"\n      with_items: $othertasks\n\n    - include: tasks/$item.yml\n      with_items: $othertasks\n\nthis works as long as othertasks is defined inline.\nWhen othertasks is removed from playbook and defined in the inventory file or in host_vars, debug continues working but include fails.\nError is ERROR: file not found: ./tasks/$othertasks.yml."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "garyrutland"
                },
                "number": 13225,
                "resourcePath": "/ansible/ansible/issues/13225",
                "state": "CLOSED",
                "publishedAt": "2015-11-20T09:27:24Z",
                "closedAt": "2015-11-20T13:13:37Z",
                "title": "Copying files with mode 400 uploads with a mode of 620",
                "bodyText": "Hi,\nI did post this in the Google forum, but my post never seemed to appear so asking here now instead.\nThis is probably something that isn't recommended but I'm trying to copy some PEM keys to a destination server so that they can be used by that server at a later time.\nI need to make sure that they are set to a mode of 400 as well, so using the following code:\n- name: add default ssh keys\n  become: true\n  copy:\n    src: ./.ssh/\n    dest: ~/.ssh\n    mode: 400\n    force: true\n\nI would hope and expect to see the following when listing the files on the destination server:\n-r--------\n\nBut instead I'm seeing the following:\n-rw--w----\n\nAny particular reason why the files being uploaded are actually getting the incorrect permissions?\nOr is there a different approach to making sure these files have the correct permissions?\nThanks,\nGary"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "zshamrock"
                },
                "number": 13674,
                "resourcePath": "/ansible/ansible/issues/13674",
                "state": "CLOSED",
                "publishedAt": "2015-12-26T12:50:57Z",
                "closedAt": "2017-11-22T00:36:51Z",
                "title": "tags are not inherented by multiple include levels",
                "bodyText": "Issue Type:\n\nBug Report (or feature request, depends on how you see it)\n\nAnsible Version:\n1.9.4\nAnsible Configuration:\nDefault\nEnvironment:\nDebian 8.2 (jessie)\nSummary:\nSee the project structure below\nSteps To Reproduce:\nI have the following project structure:\nroles/\n  local/   \n    tasks/\n      main.yml [1]\n      media/\n        main.yml [2]\n        hipchat.yml\n        slack.yml\n       ...\n\n[1] main.yml has content:\n- include: media/main.yml tags=media\n[2] main.yml has content:\n- include: hipchat.yml tags=hipchat\n- include: slack.yml tags=slack\n...\n\nExpected Results:\nI expect all tasks from media/main.yml will inherit all tags specified in tasks/main.yml. So, if I run ansible-playbook -i hosts site.yml --tags media, it will run all tasks specified in media/main.yml, and $ ansible-playbook --list-tags -i hosts site.yml reports this tag as well\nplaybook: site.yml\n\n  play #1 (localhost):  TAGS: []\n    TASK TAGS: [alternatives, apt, atom, chrome, clojure, docker, dot-files, dropbox, git, gnome, go, hipchat, idea, jdk, maven, packages, permissions, productivity, sdkman, skype, slack, spotify, system, tools, viber, vim]\n\nActual Results:\nIt fails with the following error:\nERROR: tag(s) not found in playbook: media.  possible values: alternatives,apt,atom,chrome,clojure,docker,dot-files,dropbox,git,gnome,go,hipchat,idea,jdk,maven,packages,permissions,productivity,sdkman,skype,slack,spotify,system,tools,viber,vim\n\nP.S.1: The whole source code of the project is available here https://github.com/zshamrock/ididitagain\nP.S.2: As a workaround (or the right way to do?), move specific subdirectories, like media, dev, dot-files, etc, in its own roles, and assign tags with role instead, as mentioned here http://docs.ansible.com/ansible/playbooks_tags.html in my site.yml?\nroles:\n  - { role: webserver, port: 5000, tags: [ 'web', 'foo' ] }"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "zentavr"
                },
                "number": 13547,
                "resourcePath": "/ansible/ansible/issues/13547",
                "state": "CLOSED",
                "publishedAt": "2015-12-14T20:52:06Z",
                "closedAt": "2015-12-14T21:00:05Z",
                "title": "Seems like default() filter does not work with ansible 2.0.0@devel",
                "bodyText": "Issue Type: Bug Report\nAnsible Version: 2.0.0 devel@8d16638\nAnsible Configuration:\n[defaults]\nhost_key_checking = False\nforks=20\n\nEnvironment: Mac OS X El Capitan 10.11.1\nSummary\nI have a task like this:\n    - name: Spinning up EC2 instances\n      ec2:\n        key_name: \"{{ instances_keypair }}\"\n        instance_type: \"{{ item.0.instance_type }}\"\n        image: \"{{ item.0.image }}\"\n        instance_tags: \"{{ item.0.instance_tags }}\"\n        exact_count: \"{{ item.1.exact_count }}\"\n        count_tag: \"{{ item.0.count_tag }}\"\n        vpc_subnet_id: \"{{ item.1.vpc_subnet_id }}\"\n        group: \"{{ item.0.group }}\"\n        volumes: \"{{ item.0.volumes }}\"\n        zone: \"{{ ec2_region + item.1.az }}\"\n        region: \"{{ ec2_region }}\"\n        wait: \"{{ item.0.wait|default(yes) }}\"\n        wait_timeout: \"{{ item.0.wait_timeout|default(300) }}\"\n      register: ec2\n      with_subelements:\n        - ec2_instances\n        - azs\n\n...where ec2_instances is:\nec2_instances:\n  # Memcache Servers\n  - type: memcache\n    instance_type: m3.medium\n    image: \"{{ image_id }}\"\n    group: ['private']\n    instance_tags:\n      Name: \"memcache-{{ env }}\"\n      role: memcache\n      environment: \"{{ env }}\"\n      memcache_environment: \"{{ env }}\"\n      deployment: ansible\n    volumes:\n      - device_name: /dev/sdp\n        # For any volume, a volume size less than 1 will be interpreted as a request not to create the volume.\n        volume_size: 0\n        delete_on_termination: true\n    count_tag:\n      role: memcache\n      environment: \"{{ env }}\"\n      memcache_environment: \"{{ env }}\"\n      deployment: ansible\n    azs:\n      - az: a\n        # Private Subnet ID in \"a\"\n        vpc_subnet_id: \"{{ subnets['a']['private'] }}\"\n        exact_count: 0\n      - az: b\n        # Private Subnet ID in \"b\"\n        vpc_subnet_id: \"{{ subnets['b']['private'] }}\"\n        exact_count: 1\n      - az: d\n        # Private Subnet ID in \"d\"\n        vpc_subnet_id: \"{{ subnets['d']['private'] }}\"\n        exact_count: 0\n      - az: e\n        # Private Subnet ID in \"e\"\n        vpc_subnet_id: \"{{ subnets['e']['private'] }}\"\n        exact_count: 0\n    wait: yes\n    wait_timeout: 360\n\n  #1 redis cluster (2 nodes)\n  - type: redis\n    instance_type: m3.medium\n    image: \"{{ image_id }}\"\n    group: ['private']\n    instance_tags:\n      Name: \"redis-{{ env }}\"\n      role: redis\n      environment: \"{{ env }}\"\n      redis_environment: \"{{ env }}\"\n      deployment: ansible\n    count_tag:\n      role: redis\n      environment: \"{{ env }}\"\n      redis_environment: \"{{ env }}\"\n      deployment: ansible\n    volumes:\n      - device_name: /dev/sdp\n        volume_size: 10\n        delete_on_termination: true\n    azs:\n      - az: a\n        # Private Subnet ID in \"a\"\n        vpc_subnet_id: \"{{ subnets['a']['private'] }}\"\n        exact_count: 1\n      - az: b\n        # Private Subnet ID in \"b\"\n        vpc_subnet_id: \"{{ subnets['b']['private'] }}\"\n        exact_count: 1\n      - az: d\n        # Private Subnet ID in \"d\"\n        vpc_subnet_id: \"{{ subnets['d']['private'] }}\"\n        exact_count: 0\n      - az: e\n        # Private Subnet ID in \"e\"\n        vpc_subnet_id: \"{{ subnets['e']['private'] }}\"\n        exact_count: 0\n\nWhen I execute the task - I face an error when we reach the block without defined (that redis cluster in my case):\n    wait: yes\n    wait_timeout: 360\n\nThe error is:\nfatal: [localhost]: FAILED! => {\"failed\": true, \"msg\": \"ERROR! 'yes' is undefined\"}\n\nExpected Results: workeable default() filter"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jkramarz"
                },
                "number": 6314,
                "resourcePath": "/ansible/ansible/issues/6314",
                "state": "CLOSED",
                "publishedAt": "2014-03-06T15:15:34Z",
                "closedAt": "2014-03-19T15:35:49Z",
                "title": "Playbooks are runned over other group of hosts than listed by list-hosts",
                "bodyText": "Issue Type:\nBug report\nAnsible Version:\nstarting from commit ae9843f\nansible 1.5 is affected\nEnvironment:\nat last Debian 6\nSummary:\nPlease summarize your request in this space.  You will earn bonus points for being succinct, but please add enough detail so we can understand the request.\nSteps To Reproduce:\nrun\n\nansible-playbook -C site.yml -i hosts --list-hosts\nansible-playbook -C site.yml -i hosts\n\nusing files\nhosts:\n[physical]\nphost1\n\n[phost1]\nvm1.phost1\n\nsite.yml:\n\n---\n- hosts: physical\n  tasks:\n    - ping:\n\nExpected Results:\nGroups not declared as children of group 'physical' should not be involved.\nplaybook: site.yml\n\n  play #1 (physical): host count=1\n    phost1\n\n\nPLAY [physical] *************************************************************** \n\nTASK: [ping ] ***************************************************************** \nok: [phost1]\n\nPLAY RECAP ******************************************************************** \nphost1                     : ok=1    changed=0    unreachable=0    failed=0   \n\nActual Results:\nHosts from groups named after hosts included in group 'physical' (not declared as group's children) are involved by play.\nplaybook: site.yml\n\n  play #1 (physical): host count=1\n    phost1\n\n\nPLAY [physical] *************************************************************** \n\nTASK: [ping ] ***************************************************************** \nok: [vm1.phost1]\nok: [phost1]\n\nPLAY RECAP ******************************************************************** \nphost1                     : ok=1    changed=0    unreachable=0    failed=0   \nvm1.phost1                 : ok=1    changed=0    unreachable=0    failed=0"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dagwieers"
                },
                "number": 1664,
                "resourcePath": "/ansible/ansible/issues/1664",
                "state": "CLOSED",
                "publishedAt": "2012-11-23T15:16:29Z",
                "closedAt": "2012-11-24T23:19:22Z",
                "title": "Variables from group_vars/all are not being picked up when using inventory script",
                "bodyText": "In our case we have seen the above, I still need a minimal test-case to demonstrate this."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "pfigue"
                },
                "number": 9321,
                "resourcePath": "/ansible/ansible/issues/9321",
                "state": "CLOSED",
                "publishedAt": "2014-10-13T11:33:44Z",
                "closedAt": "2014-10-13T22:55:22Z",
                "title": "Duplicated newline in Jinja2 variable when copy module is used",
                "bodyText": "Issue Type: Bug Report\nAnsible Version: ansible 1.7.2\nEnvironment: Arch Linux\nSummary: Variable content gets newlines duplicated when pasted into a file via the copy module\nSteps To Reproduce:\n\nAdd a step copy: content=\"{{ ssl_private_key }}\" dest=/etc/ssl/private/foo.pem in a playbook\nWrite the content of ssl_private_key in an ansible-vault vars file\nRun the playbook\nCheck if /etc/ssl/private/foo.pem will have an extra newline for each line in the ssl_private_key variable\n\nExpected Results: if the variable ssl_private_key is defined like this:\nssl_private_key: |\n   -----BEGIN PRIVATE KEY-----\n   MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\n   MQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\n   ZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n   -----END PRIVATE KEY-----\n\nI expect to have this in /etc/ssl/private/foo.pem:\n-----BEGIN PRIVATE KEY-----\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\nMQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\nZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n-----END PRIVATE KEY-----\n\nAnd this is what I actually get with Ansible 1.7.1 and some previous versions.\nActual Results: But instead, with Ansible 1.7.2 the result will have duplicated \\n:\n-----BEGIN PRIVATE KEY-----\n\nMIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDHI2RnhLTBOuZ0\n\nMQswCQYDVQQGEwJVUzEQMA4GA1UEChMHU1NMLmNvbTEcMBoGA1UEAxMTU1NMLmNv\n\nZW50aWFsU1NMIFdpbGRjYXJkMRQwEgYDVQQDFAsqLnJlbGF5ci5pbzCCASIwDQYJ\n\n-----END PRIVATE KEY-----"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jorgenschaefer"
                },
                "number": 6840,
                "resourcePath": "/ansible/ansible/issues/6840",
                "state": "CLOSED",
                "publishedAt": "2014-04-03T11:02:56Z",
                "closedAt": "2014-09-29T20:36:03Z",
                "title": "synchronize action should maybe force the connection for hosts named localhost (edited title)",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\nansible 1.6 (devel 317c2f4) last updated 2014/04/03 12:41:29 (GMT +200)\nEnvironment:\nopenSUSE 12.2 (x86_64)\nSummary:\nUsing the synchronize action while deploying to a VM listening on localhost confuses ansible as it tries to unnecessarily delegate to localhost, and also forgets to clear the port.\nSteps To Reproduce:\nI'm running a simple playbook to deploy some software to a VM running on localhost:2222.\nInventory file:\n[localvm]\nlocalhost ansible_ssh_user=root ansible_ssh_port=2222\n\ntasks/main.yml:\n- name: Install via rsync\n  synchronize: src=../pkg dest=/srv/pkg/\n\nThis fails because it tries to connect to 127.0.0.1:2222 with my current username. What is happening, apparently, is that it assumes I'm doing a delegate action and tries to connect to localhost with my current username, but retains the faulty port. There is no need to delegate at all.\nExpected Results:\nI'd assume it would simply run rsync locally, without delegation, to deploy to the VM.\nActual Results:\nIt fails because it tries to run an ssh connection to <username>@127.0.0.1 port 2222, which does not work because <username> does not exist on the VM, and port 2222 is the VM. It would succeed on port 22, but that's not necessary.\nThe following change also makes the above work for me, so it is indeed some bogus delegation business. (The second hunk is only necessary because the first change removes the conn.delegate attribute.)\n--- a/lib/ansible/runner/action_plugins/synchronize.py\n+++ b/lib/ansible/runner/action_plugins/synchronize.py\n@@ -81,7 +81,7 @@ class ActionModule(object):\n         self.transport_overridden = False\n\n         if inject.get('delegate_to') is None:\n-            inject['delegate_to'] = '127.0.0.1'\n+            # inject['delegate_to'] = '127.0.0.1'\n             # IF original transport is not local, override transport and disable sudo.\n             if self.original_transport != 'local':\n                 inject['ansible_connection'] = 'local'\n@@ -136,11 +136,11 @@ class ActionModule(object):\n\n         # CHECK DELEGATE HOST INFO\n         use_delegate = False\n-        if conn.delegate != conn.host:\n-            if 'hostvars' in inject:\n-                if conn.delegate in inject['hostvars'] and self.original_transport != 'local':\n-                    # use a delegate host instead of localhost\n-                    use_delegate = True\n+        # if conn.delegate != conn.host:\n+        #     if 'hostvars' in inject:\n+        #         if conn.delegate in inject['hostvars'] and self.original_transport != 'local':\n+        #             # use a delegate host instead of localhost\n+        #             use_delegate = True\n\n         # COMPARE DELEGATE, HOST AND TRANSPORT\n         process_args = False"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "huguesalary"
                },
                "number": 7360,
                "resourcePath": "/ansible/ansible/issues/7360",
                "state": "CLOSED",
                "publishedAt": "2014-05-11T04:48:06Z",
                "closedAt": "2014-08-09T02:06:16Z",
                "title": "Docker - dns argument not working",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\nAnsible 1.7 (but had the same issue with 1.6.1 and 1.6)\nEnvironment:\nRunning ansible from Mac OS\nManaging Debian\nSummary:\nWith the docker module, using the dns argument does not add the dns to the container. (nothing's added to the container's /etc/resolv.conf)\nSteps To Reproduce:\n    - name: Create docker proxy\n      docker: image=ubuntu name=test hostname=test count=1 privileged=yes dns=127.0.0.1 command=/bin/bash\n\n\nExpected Results:\nssh containers_ip\ncat /etc/resolv.conf\n\nnameserver 127.0.0.1 should appear at the top of the file\nActual Results:\nnameserver 127.0.0.1 does not appear"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ThStock"
                },
                "number": 7281,
                "resourcePath": "/ansible/ansible/issues/7281",
                "state": "CLOSED",
                "publishedAt": "2014-05-05T17:50:41Z",
                "closedAt": "2014-05-06T04:13:03Z",
                "title": "--limit does not work with intersecting hostgroups",
                "bodyText": "Issue Type:\nBug Report, Feature Idea\nAnsible Version:\nansible 1.5.5\nEnvironment:\nN/A\nSummary:\nI'll be nice, if the --limit switch, will limit task execution to the given hostgroup, also when both hostgroups consists of the same hosts\nSteps To Reproduce:\nInventory:\n[webservers]\nwebAA\n\n[webservers-with-cache]\nwebAA\nwebBB\n\nPlaybook:\n---\n- hosts: webservers\n  remote_user: a\n  tasks:\n  - shell: id\n- hosts: webservers-with-cache\n  remote_user: b\n  tasks:\n  - shell: uname -a\n$ ansible-playbook  -v -i ... --limit webservers playbook.yml\nExpected Results:\nPLAY [webservers] *************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell id] **************************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"id \", \n\"delta\": \"0:00:00.004137\", \"end\": \"2014-05-05 18:31:23.507347\", \"rc\": 0,\n\"start\": \"2014-05-05 18:31:23.503210\", \"stderr\": \"\", \n\"stdout\": \"uid=1000(a) gid=1000(a) groups=1000(a)\"}\n\nPLAY RECAP ********************************************************************\nwebAA              : ok=2    changed=1    unreachable=0    failed=0\n\nActual Results:\nPLAY [webservers] *************************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell id] **************************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"id \", \n\"delta\": \"0:00:00.004137\", \"end\": \"2014-05-05 18:31:23.507347\", \"rc\": 0, \n\"start\": \"2014-05-05 18:31:23.503210\", \"stderr\": \"\", \n\"stdout\": \"uid=1000(a) gid=1000(a) groups=1000(a)\"}\n\nPLAY [webservers-with-cache] **************************************************\n\nGATHERING FACTS ***************************************************************\nok: [webAA]\n\nTASK: [shell uname -a] ********************************************************\nchanged: [webAA] => {\"changed\": true, \"cmd\": \"uname -a \", \n\"delta\": \"0:00:00.002848\", \"end\": \"2014-05-05 18:31:25.982012\", \"rc\": 0, \n\"start\": \"2014-05-05 18:31:25.979164\", \"stderr\": \"\", \n\"stdout\": \"Linux ... 2.6.18-371.3.1.el5 #1 SMP Mon Nov 11 03:23:58 ES..\"}\n\nPLAY RECAP ********************************************************************\nwebAA              : ok=4    changed=2    unreachable=0    failed=0"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "nobita2041"
                },
                "number": 417,
                "resourcePath": "/ansible/ansible/issues/417",
                "state": "CLOSED",
                "publishedAt": "2012-05-28T07:37:46Z",
                "closedAt": "2012-05-29T12:43:02Z",
                "title": "sudo options in connection.py (CentOS 5.4, -u and -k incompatible?)",
                "bodyText": "「sudo: the `-u' and '-k' options may not be used together」\nI modified below, and fine.\nconnection.py\n        sudocmd = 'sudo -k -p \"%s\" -u %s -- \"$SHELL\" -c %s' % (prompt,\n\n        sudocmd = 'sudo -k;sudo -p \"%s\" -u %s -- \"$SHELL\" -c %s' % (prompt,"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tonk"
                },
                "number": 1610,
                "resourcePath": "/ansible/ansible/issues/1610",
                "state": "CLOSED",
                "publishedAt": "2012-11-13T10:38:54Z",
                "closedAt": "2012-11-13T12:46:00Z",
                "title": "Template variable stuff broken",
                "bodyText": "Yesterday everything was working fine, but after a git pull; make install my run borked big time.\nI got:\nAt revision 233.\nTraceback (most recent call last):\n  File \"/usr/bin/ansible-playbook\", line 172, in <module>\n    sys.exit(main(sys.argv[1:]))\n  File \"/usr/bin/ansible-playbook\", line 118, in main\n    subset=options.subset,\n  File \"/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py\", line 119, in __init__\n    (self.playbook, self.play_basedirs) = self._load_playbook_from_file(playbook)\n  File \"/usr/lib/python2.6/site-packages/ansible/playbook/__init__.py\", line 169, in     _load_playbook_from_file\n    p['vars'].update(incvars)\nAttributeError: 'list' object has no attribute 'update'\n\nInvestigating this pointed me to my issue playbook, containing\n# vim:ff=unix ts=4 sw=4 ai expandtab\n# $Id: init.yml 209 2012-11-11 13:26:06Z tonk $\n\n---\n- hosts: all\n  tasks:\n      - name: deploy issue file\n        template: src=issue.in dest=/etc/issue owner=root mode=0444\n\nAnd the template containing\n  ------------------------------------------------------------------------------\n                               -- W A R N I N G --\n                  UNAUTHORIZED ACCESS STRICTLY PROHIBITED!!\n  ------------------------------------------------------------------------------\n           System Name : {{ \"%-25s\"|format(ansible_hostname) }} Location : {{ location }}\n           Managed by  : {{ \"%-25s\"|format(name)             }} Room     : {{ room }}\n  ------------------------------------------------------------------------------\n{% if issueremarks is defined %}\n{{ issueremarks.center(80) }}\n  ------------------------------------------------------------------------------\n{% endif %}\n\nWhen I roll back to the code of yesterday, things work again.\nIt does look as if an undefined variable is used things break. But that's what the is defined is for, so there seems to be a bug in the template stuff of last night.\nCould you look into that, please?"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ThoTischner"
                },
                "number": 16946,
                "resourcePath": "/ansible/ansible/issues/16946",
                "state": "CLOSED",
                "publishedAt": "2016-08-04T09:39:11Z",
                "closedAt": "2016-08-08T15:04:06Z",
                "title": "Hipchat Callback not working",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\nansible 2.1.0.0\n  config file = /root/ansible/ansible.cfg\n  configured module search path = ['modules']\n\nCONFIGURATION\n[defaults]\nlibrary = modules\nlog_path = /tmp/ansible.log\nroles_path = roles\ncallback_plugins = callbacks/\ndeprecation_warnings=False\ncallback_whitelist = hipchat\n\nOS / ENVIRONMENT\nCentOS7\nSUMMARY\nHipchat Callback: https://github.com/ansible/ansible/blob/devel/lib/ansible/plugins/callback/hipchat.py\nis not working.\nVars can not be set.\nSTEPS TO REPRODUCE\nEnable hipchat callback via ansible.cfg whitelisting.\nConfigure the required Hipchat ENV-Vars.\nRun any playbook, following error occurs:\nPLAY [Staging Packages] ********************************************************\n [WARNING]: Failure using method (v2_playbook_on_play_start) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x31c4750>):\n'Play' object has no attribute 'playbook'\n [WARNING]: Failure using method (v2_playbook_on_stats) in callback plugin (</usr/lib/python2.7/site-packages/ansible/plugins/callback/hipchat.CallbackModule object at 0x2c4c750>):\n'CallbackModule' object has no attribute 'display'\n\nEXPECTED RESULTS\nMessage send to hipchat room.\nACTUAL RESULTS\nHipchat message not working\nMISC\nThe display error can be solved by changing the callback from:\nself.display.warning('\nto\nself._display.warning('"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "nickhammond"
                },
                "number": 10881,
                "resourcePath": "/ansible/ansible/issues/10881",
                "state": "CLOSED",
                "publishedAt": "2015-04-29T21:10:49Z",
                "closedAt": "2016-05-27T14:19:26Z",
                "title": "Add documentation for privilege_escalation/become to ansible.cfg",
                "bodyText": "By looking at the example file here it looks as though the new group would look similar to this:\n[privilege_escalation]\nbecome=True\nbecome_method='sudo'\nbecome_user='root'\nbecome_ask_pass=False\n\nI can create a pull request to add that section to the docs, just wanted to confirm that will be the correct setup moving forward.\nDocs:\n\nBecome docs\nAnsible configuration file docs"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "piffey"
                },
                "number": 8615,
                "resourcePath": "/ansible/ansible/issues/8615",
                "state": "CLOSED",
                "publishedAt": "2014-08-14T00:22:49Z",
                "closedAt": "2014-08-14T01:26:29Z",
                "title": "Delegated Hosts Don't Use Inventory Details or Confirm Existence In Inventory",
                "bodyText": "I wasn't sure if this was a bug or intended behavior. When I think about the issue, it seems more intuitive that you'd use the same name that you would for a host in your inventory and expect Ansible to use the same connection details when running the task.\nIssue Type: Bug Report\nAnsible Version: 1.7\nEnvironment: Arch to drive, managing Ubuntu 12.04\nSummary:\nWhen running a task with delegate_to it doesn't look the provided host up in the inventory. You can pass it anything and it will try to SSH to it. Since that's the case it also doesn't properly look up the ansible_ssh_port for the delegated host.\nSteps To Reproduce:\nHosts File:\n(Using 127.0.0.1 as an example, but assume that's a WAN IP with a bunch of NAT one-to-many forwards.)\n host1 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22\n host2 ansible_ssh_host=127.0.0.1 ansible_ssh_port=22222\n\nPlaybook:\n- name: A Most Noble Reproduction\n  hosts: host1\n  remote_user: root\n\n  tasks:\n    - name: Do The Delegationating\n      shell: hostname\n      delegate_to: host2\n\n    - name: Do The Delegationating, Part Deux\n      shell: echo 'If you see this in your syslog maybe someone should disable passwordless root logins at the googles.'\n      delegate_to: google.com\n\nExpected Results:\nchanged: [host1 -> host2] => {\"changed\": true, \"cmd\": \"hostname\", \"stderr\": \"\", \"stdout\": \"host2\"}\nfatal: [host1 -> google.com] => Host google.com ain't all up in your inventory.\n\nActual Results:\nchanged: [host1 -> host2] => {\"changed\": true, \"cmd\": \"hostname\", \"stderr\": \"\", \"stdout\": \"host1\"}\nfatal: [host1 -> google.com] => SSH encountered an unknown error during the connection. We recommend you re-run the command using -vvvv, which will enable SSH debugging output to help diagnose the issue\n\nEdit: Seems likely related to Issue 8224.\nEdit2: Made the title better. Seemed ambiguous/not as explanatory as it could be."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jyrkiput"
                },
                "number": 10975,
                "resourcePath": "/ansible/ansible/issues/10975",
                "state": "CLOSED",
                "publishedAt": "2015-05-11T09:52:19Z",
                "closedAt": "2016-11-01T16:20:21Z",
                "title": "SSH connection got stuck when IP and host have different keys in known_hosts",
                "bodyText": "In following situatation, I'd like to have some kind of error report, but now the connection just got stuck.\nI reinstalled one machine, and tried to use same IP and host after installation. The initial ansible connection reported nicely\n\nfatal: [node2-jenkins-slave.dev.sysart.fi] => Using a SSH password instead of a key is not possible because Host Key checking is enabled and sshpass does not support this.  Please add this host's fingerprint to your known_hosts file to manage this host.\n\nAfter adding doing this and trying to connect, the connection got stuck\n\nansible -i hosts node2-jenkins-slave.dev.sysart.fi -m ping -u root -k -vvvv\nSSH password:\n<node2-jenkins-slave.dev.sysart.fi> ESTABLISH CONNECTION FOR USER: root\n<node2-jenkins-slave.dev.sysart.fi> REMOTE_MODULE ping\n<node2-jenkins-slave.dev.sysart.fi> EXEC sshpass -d6 ssh -C -tt -vvv -o ControlMaster=auto -o ControlPersist=60s -o ControlPath=\"/home/jyrki/.ansible/cp/ansible-ssh-%h-%p-%r\" -o GSSAPIAuthentication=no -o PubkeyAuthentication=no -o User=root -o ConnectTimeout=10 node2-jenkins-slave.dev.sysart.fi /bin/sh -c 'mkdir -p $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678 && echo $HOME/.ansible/tmp/ansible-tmp-1431337315.21-244074993784678'\n\nI think that the reason for this had something to do with the warning I got with ssh\n\nssh node2-jenkins-slave.dev.sysart.fi\nWarning: the ECDSA host key for 'node2-jenkins-slave.dev.sysart.fi' differs from the key for the IP >address '192.168.179.43'\nOffending key for IP in /home/jyrki/.ssh/known_hosts:107\nMatching host key in /home/jyrki/.ssh/known_hosts:133\n\nAfter removing both keys and adding the host to known_hosts, everything worked."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "elouanKeryell-Even"
                },
                "number": 11039,
                "resourcePath": "/ansible/ansible/issues/11039",
                "state": "CLOSED",
                "publishedAt": "2015-05-20T12:20:36Z",
                "closedAt": "2015-05-21T02:03:27Z",
                "title": "Single * pattern not working",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\n$ ansible --version\nansible 1.9.1\nconfigured module search path = None\nAnsible Configuration:\nI think I changed nothing to the base configuration.\nEnvironment:\nCentos 7\nSummary:\nIn the docs it says it is possible to use a single * pattern to target every host, but it doesn't seem to work for me.\nSteps To Reproduce:\n$ ansible * -m ping\nExpected Results:\nSame result as $ ansible all -m ping, which is working fine for me.\nActual Results:\nUsage: ansible <host-pattern> [options]"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "buster"
                },
                "number": 8512,
                "resourcePath": "/ansible/ansible/issues/8512",
                "state": "CLOSED",
                "publishedAt": "2014-08-08T09:25:55Z",
                "closedAt": "2014-08-08T17:12:27Z",
                "title": "ansible 1.7 passing an argument with newlines to the shell module eats newlines (edited title)",
                "bodyText": "I'm  using quite a lot of multiline strings, and according to http://stackoverflow.com/questions/3790454/in-yaml-how-do-i-break-a-string-over-multiple-lines the way this works in YAML is to use | isntead of > to preserve newlines.\nThis worked in ansible 1.6.x without problems.\nNow, newlines are stripped away..\nExample:\n\n---\n- hosts: all\n  remote_user: root\n  tasks:\n  - name: bla\n    args:\n      executable: /bin/bash\n    shell: |\n      echo hi\n      echo hi2\n\nThis should print hi and hi2 in two lines but it doesn't anymore.\nIt breaks a lot of ansible code for me which, unfortunately, relies on shell scripts."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "anaselbouhali"
                },
                "number": 25350,
                "resourcePath": "/ansible/ansible/issues/25350",
                "state": "CLOSED",
                "publishedAt": "2017-06-05T09:58:58Z",
                "closedAt": "2017-06-05T12:13:46Z",
                "title": "C:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:76: Variable QMAKE_DEFAULT_INCDIRS is not defined.\nC:/Qt/5.9/mingw53_32/mkspecs/features/toolchain.prf:129: Variable QMAKE_CXX.COMPILER_MACROS is not defined.\nANSIBLE VERSION\n\n\n\nCONFIGURATION\n\nOS / ENVIRONMENT\n\nSUMMARY\n\nSTEPS TO REPRODUCE\n\n\n\n\nEXPECTED RESULTS\n\nACTUAL RESULTS"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tbielawa"
                },
                "number": 3099,
                "resourcePath": "/ansible/ansible/issues/3099",
                "state": "CLOSED",
                "publishedAt": "2013-06-02T20:26:03Z",
                "closedAt": "2013-06-02T21:20:30Z",
                "title": "Docs ansible-1.1: \"register\" example broken?",
                "bodyText": "[root@util01 ansible]# ansible-playbook --version\nansible-playbook 1.1\n[root@util01 ansible]# rpm -q ansible\nansible-1.1-1.el6.noarch\n[root@util01 ansible]# rpm -q --qf \"%{NAME} - %{VENDOR}\\n\" ansible\nansible - Fedora Project\n[root@util01 ansible]# which ansible-playbook\nalias ansible-playbook='ansible-playbook -v -i /root/ansible/hosts'\n        /usr/bin/ansible-playbook\n\nDescription:\nThe example [1] for register appears to be broken.\nSteps to reproduce:\n\nCopy example from [1] into a yaml file\nRun the file with ansible-playbook\n\n[root@util01 tmp]# grep \"hi\" /etc/motd\nThis motd has the word 'hi' in it\n[root@util01 tmp]# cat test_when.yaml\n- name: test play\n  hosts: util\n\n  tasks:\n\n      - action: shell cat /etc/motd\n        register: motd_contents\n\n      - action: shell echo \"motd contains the word hi\"\n        when: motd_contents.stdout.find('hi') != -1\n\nExpected Result:\nAnsible prints the message \"motd contains the word hi\"\nObserved result:\n[root@util01 tmp]# ansible-playbook test_when.yaml\nERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1\n\nThis is failing when it gets to the final 'else' clause in compile_when_to_only_if. I added some debugging statements:\n    else:\n        print \"Tokens: \" + str(tokens)\n        print map(lambda t: type(t), tokens)\n        raise errors.AnsibleError(\"invalid usage of when_ operator: %s\" % expression)\n\nwhich outputs:\n[root@util01 tmp]# ansible-playbook test_when.yaml\nTokens: [\"motd_contents.stdout.find('hi')\", '!=', '-1']\n[<type 'str'>, <type 'str'>, <type 'str'>]\nERROR: invalid usage of when_ operator: motd_contents.stdout.find('hi') != -1\n\n[1] http://ansible.cc/docs/playbooks2.html#register-variables"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "maxalbert"
                },
                "number": 10403,
                "resourcePath": "/ansible/ansible/issues/10403",
                "state": "CLOSED",
                "publishedAt": "2015-03-06T21:03:46Z",
                "closedAt": "2015-08-07T04:10:04Z",
                "title": "New keyword for 'with_sequence' in order to skip task if count < 0 or start > end",
                "bodyText": "Issue Type: Feature request\nAnsible Version: 1.8.4\nEnvironment: N/A\nSummary:\nWhen using with_sequence \"programmatically\" (i.e., in combination with variables), it would be useful to allow a negative value for the count argument, or a start value that is larger than end. Currently this results in an error (\"can't count backwards\"), but it could be useful to simply skip the task if the range is empty. This could be achieved by adding an extra keyword, e.g. skip_with_emtpy_range (although it should probably be less verbose), which would be False by default for backwards compatibility.\nExample:\n---\n- hosts: all\n  gather_facts: False\n  vars:\n    - M: 2\n    - N: 1\n  tasks:\n    - name: This taks should be skipped because 'skip_with_empty_range' is True.\n      command: echo \"Hello world\"\n      with_sequence: start={{ M }} end={{ N }} skip_with_empty_range=True\n      #with_sequence: count={{ M - N }} skip_with_empty_range=True\nExpected Results:\nThe task should be skipped because skip_with_empty_range is True:\nTASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** \nskipping: [localhost]\n\nActual Results:\nCurrently the task fails with the error message \"can't count backwards\" (of course, skip_with_emtpy_range needs to be omitted when running the example above because it is not supported yet).\nTASK: [This taks should be skipped because 'skip_with_empty_range' is True.] *** \nfatal: [localhost] => can't count backwards\n\nFATAL: all hosts have already failed -- aborting"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "kustodian"
                },
                "number": 16365,
                "resourcePath": "/ansible/ansible/issues/16365",
                "state": "CLOSED",
                "publishedAt": "2016-06-20T07:33:58Z",
                "closedAt": "2016-09-20T13:57:33Z",
                "title": "Could not create retry file '*.retry'. [Errno 2] No such file or directory: ''",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\nansible 2.1.1.0 (stable-2.1 4d4bbcbb33) last updated 2016/06/20 08:55:21 (GMT +200)\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nUbuntu\nSUMMARY\nIf retry_files_save_path isn't set in ansible.cfg, when a playbook fails, a retry files tries to be created in an empty directory.\nSTEPS TO REPRODUCE\nMake any playbook fail.\nEXPECTED RESULTS\nA try file should be created in the user home directory.\nACTUAL RESULTS\nA warning message is printed and a retry file is not created:\n [WARNING]: Could not create retry file 'test_fail.retry'.         [Errno 2] No such file or directory: ''\n\nIt looks like the default value for the retry files isn't set to be ~/ like it's mentioned in the documentation."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "0mmariano"
                },
                "number": 23489,
                "resourcePath": "/ansible/ansible/issues/23489",
                "state": "CLOSED",
                "publishedAt": "2017-04-11T11:18:27Z",
                "closedAt": "2017-04-13T00:40:39Z",
                "title": "Unable to use some environment variables",
                "bodyText": "ISSUE TYPE: Bug report\nCOMPONENT NAME: ansible_env\nANSIBLE VERSION:\nansible 2.2.1.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = ['/usr/share/my_modules/', '/etc/ansible/roles/glassfish/library']\nCONFIGURATION:\ndefined values under [default]\nOS: Debian Jessie\nSUMMARY: Ansible is unable to interpolate an environment variable when it is called from a hosts file or a group_vars/all file.\nSTEPS TO REPRODUCE:\nIn /etc/profile.d/required_env_vars.sh --\nDEPLOY_DIR=/myhome/deployments\nIn testPlaybook.yml\ndeploy_dir = \"{{ ansible_env.DEPLOY_DIR }}/RT{{rt_number}}\"\nwhere rt_number is defined via vars_prompt\nThis fails because DEPLOY_DIR is not found in the output of the setup command for the host (looked in the ansible_env), but HOME is. So it looks like Ansible is unable to use environment variables defined in a profile.d script.\nAs a work-around, I added an [all:vars] section the inventories/poc/hosts file --\n[all:vars]\ndeploy_dir = $HOME/deployments --> did not work\ndeploy_dir = /myhome/deployments --> worked\ndeploy_dir = $DEPLOY_DIR --> did not work\ndeploy_dir = \"{{ ansible_env.DEPLOY_DIR }}\" --> did not work\nSince the second one worked (hardcoded value), I removed the [all:vars] section and used an inventories/poc/group_vars/all file --\ndeploy_dir = /myhome/deployments\nThis resulted in the error below --\nERROR! Unexpected Exception: dictionary update sequence element #0 has length 1; 2 is required\nthe full traceback was:\nTraceback (most recent call last):\nFile \"/usr/bin/ansible-playbook\", line 103, in \nexit_code = cli.run()\nFile \"/usr/lib/python2.7/dist-packages/ansible/cli/playbook.py\", line 132, in run\ninventory = Inventory(loader=loader, variable_manager=variable_manager, host_list=self.options.inventory)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 98, in init\nself.parse_inventory(host_list)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 165, in parse_inventory\ngroup.vars = combine_vars(group.vars, self.get_group_variables(group.name))\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 555, in get_group_variables\nself._vars_per_group[groupname] = self._get_group_variables(groupname, vault_password=vault_password)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 573, in _get_group_variables\nvars = combine_vars(vars, self.get_group_vars(group))\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 775, in get_group_vars\nreturn self._get_hostgroup_vars(host=None, group=group, new_pb_basedir=new_pb_basedir, return_results=return_results)\nFile \"/usr/lib/python2.7/dist-packages/ansible/inventory/init.py\", line 839, in _get_hostgroup_vars\nhost_results = self._variable_manager.add_group_vars_file(base_path, self._loader)\nFile \"/usr/lib/python2.7/dist-packages/ansible/vars/init.py\", line 619, in add_group_vars_file\ndata = self._load_inventory_file(path, loader)\nFile \"/usr/lib/python2.7/dist-packages/ansible/vars/init.py\", line 577, in _load_inventory_file\nrval.update(data)\nValueError: dictionary update sequence element #0 has length 1; 2 is required\nOn my target server, I also tried to set up DEPLOY_DIR as an environment variable in various places --\n\n/etc/environment -- which is not preferred because it can't do variable interpolation (only key=val pairs, no variables on the right hand side). The output of the env command showed the variables I defined as expected.\n/etc/profile -- The output of the env command showed the variables defined also.\n/etc/profile.d/required_env_vars.sh -- this is preferred because I'd like to keep /etc/profile the same across all servers, and just add custom environment variables via profile.d script. The output of the env command DID NOT show the variables defined, but doing \"echo $variable\" showed the expected value\n\nNone of these affected the content of ansible_env (I thought ansible_env would get the variables if the were returned by the \"env\" command?).\nSo it looks like Ansible does not include all the environment variables defined in its \"environment\" (ansible_env).\nUPDATE 1\nOkay. My bad. I changed the content of group_vars/all to use a colon instead of an equal and it worked.\ndeploy_dir : /home/b013000915/deployments\nGot confused because in some cases (e.g. facts.d content, some modules), I had to use an equal sign.\nBut the other issue (not getting some of the environment variables defined) still persists.\nUPDATE 2\nI also tried using lookup to get the value of the following variables on the target node but it did not work for custom variables (defined via /etc/profile)\nIn /etc/profile\nDEPLOY_DIR=/myhome/deployments\nIn testPlaybook.yml\n\nname: check using lookup env\ndebug:\nmsg: \"deploy_dir is {{ lookup('env', 'DEPLOY_DIR') }}, home is {{ lookup('env', 'HOME') }}\"\n\nOutput was\nok: [IP address] => {\n\"msg\": \"deploy_dir is , home is /myhome\"\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "muffl0n"
                },
                "number": 12473,
                "resourcePath": "/ansible/ansible/issues/12473",
                "state": "CLOSED",
                "publishedAt": "2015-09-22T14:45:14Z",
                "closedAt": "2015-09-23T12:29:45Z",
                "title": "--limit isn't honored at all",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\n$ ansible --version\nansible 2.0.0 (devel c30e464388) last updated 2015/09/22 16:21:32 (GMT +200)\n  lib/ansible/modules/core: (detached HEAD 59afecace4) last updated 2015/09/22 16:21:46 (GMT +200)\n  lib/ansible/modules/extras: (detached HEAD dee690d7f4) last updated 2015/09/22 16:21:59 (GMT +200)\n  config file = \n  configured module search path = None\n\nAnsible Configuration:\nn/a\nEnvironment:\nn/a\nSummary:\nI have a simple inventory with four hosts (master1, master2, slave1, slave2) in two different groups (app-server, app-slave). When I use a host-pattern to select one group (e.g. app-server) and specify a host with --limit, ansible just selects all hosts. It does not matter if the host is contained in the selected group.\nSteps To Reproduce:\nhosts:\n[app-server]\nmaster1\nmaster2\n\n[app-slave]\nslave1\nslave2\n\nRun some module with this inventory, use a host-pattern and limit it with a host. E.g:\nansible -i hosts -m debug -a \"msg=foo\" app-slave -l master1\nansible -i hosts -m debug -a \"msg=foo\" app-slave -l slave1\nExpected Results:\nNo hosts matched\n\nActual Results:\n$ ansible -i hosts -m debug -a \"msg=foo\" app-slave -l master1\nslave1 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\nslave2 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\n\n$ ansible -i hosts -m debug -a \"msg=foo\" app-slave -l slave1\nslave1 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}\nslave2 | SUCCESS => {\n    \"changed\": false, \n    \"msg\": \"foo\"\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "sivel"
                },
                "number": 21028,
                "resourcePath": "/ansible/ansible/issues/21028",
                "state": "CLOSED",
                "publishedAt": "2017-02-03T22:06:56Z",
                "closedAt": "2019-02-08T17:26:45Z",
                "title": "Update, remove or migrate CODING_GUIDELINES.md",
                "bodyText": "ISSUE TYPE\n\nDocumentation Report\n\nCOMPONENT NAME\nCODING_GUIDELINES.md\nANSIBLE VERSION\nN/A\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nThe CODING_GUIDELINES.md file is extremely out of date.  It contains information about coding style, tests, etc.\nI bean looking into this document, but found the amount of information needing updating a bit daunting for me to undertake at the moment.\nThis file should either be updated, removed and potentially migrated into the docsite.\nSTEPS TO REPRODUCE\nN/A\nEXPECTED RESULTS\nN/A\nACTUAL RESULTS\nN/A"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ogai"
                },
                "number": 12186,
                "resourcePath": "/ansible/ansible/issues/12186",
                "state": "CLOSED",
                "publishedAt": "2015-09-01T15:02:42Z",
                "closedAt": "2019-06-24T17:43:19Z",
                "title": "search filter on an undefined variable returns a non-descriptive error",
                "bodyText": "ISSUE TYPE\nBug Report\nCOMPONENT NAME\ncore\nANSIBLE VERSION\n2.1\nCONFIGURATION\nOS / ENVIRONMENT\nSUMMARY\nIn templates\nTrying to use this template fails with a non-descriptive error when STRINGG is undefined:\n{% if STRINGG | search('abc') %}\nworks!\n{% endif %}\nThe error is:\nTypeError: expected string or buffer\n\nThe error should be:\nAnsibleUndefinedVariable: One or more undefined variables: 'STRINGG' is undefined\n\nIn plays\nwhen: \"undefined | search('abc')\"\nThe error is:\nFailed to template {% if undefined | search('abc') %} True {% else %} False {% endif %}: an unexpected type error occurred. Error was expected string or buffer\n\nSTEPS TO REPRODUCE\nEXPECTED RESULTS\nACTUAL RESULTS"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "maedox"
                },
                "number": 9966,
                "resourcePath": "/ansible/ansible/issues/9966",
                "state": "CLOSED",
                "publishedAt": "2015-01-09T14:12:53Z",
                "closedAt": "2017-01-23T17:41:47Z",
                "title": "apt_repository: Failed to validate the SSL certificate for launchpad.net:443",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\nansible 1.8.2\nconfigured module search path = /usr/share/ansible\nEnvironment:\nRunning from: Linux Mint 17.1 (based on Ubuntu 14.04)\nManaging: Ubuntu 10.04, 12.04\nSummary:\nAdding a PPA with the apt_repository module fails with certificate validation problems.\nManually running add-apt-repository on the host works.\nSteps To Reproduce:\n- name: \"PHP: Add PHP 5.4 PPA\"\n  apt_repository:\n    repo: ppa:ondrej/php5-oldstable\nExpected Results:\nPPA added under /etc/apt/sources.list.d/\nActual Results:\nTASK: [php | PHP: Add PHP 5.4 PPA] ******************************************** \nfailed: [host.example.com] => {\"failed\": true}\nmsg: Failed to validate the SSL certificate for launchpad.net:443. Use validate_certs=no or make sure your managed systems have a valid CA certificate installed. Paths checked for this platform: /etc/ssl/certs, /etc/pki/ca-trust/extracted/pem, /etc/pki/tls/certs, /usr/share/ca-certificates/cacert.org, /etc/ansible"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "Brian-Williams"
                },
                "number": 14668,
                "resourcePath": "/ansible/ansible/issues/14668",
                "state": "CLOSED",
                "publishedAt": "2016-02-25T21:03:52Z",
                "closedAt": "2016-03-28T15:16:47Z",
                "title": "scp_if_ssh parameter ignored in ansible 2.0.1.0",
                "bodyText": "Issue Type:\n\nBug Report\n\nAnsible Version:\nansible 2.0.1.0\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = /home/share/library\nAnsible Configuration:\n$ cat /etc/ansible/ansible.cfg | grep scp\nif True, make ansible use scp if the connection type is ssh\nscp_if_ssh = True\nEnvironment:\nN/A\nSummary:\nParameter scp_if_ssh is set to True in ansible.cfg. It fails to connect to host with error message \"unable to open an sftp connection\". It shouldn't be attempting an sftp connection.\nSteps To Reproduce:\nRun the following on a system that rejects sftp connections:\nansible  -m setup\n\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nfatal: [GoldenBoy]: FAILED! => {\"failed\": true, \"msg\": \"failed to open a SFTP connection (Channel closed.)\"}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @service-pack-6.2.x.retry\n\nPLAY RECAP *********************************************************************\nGoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY ***************************************************************************\n\nTASK [setup] *******************************************************************\nfatal: [GoldenBoy]: FAILED! => {\"failed\": true, \"msg\": \"failed to open a SFTP connection (Channel closed.)\"}\n\nNO MORE HOSTS LEFT *************************************************************\n    to retry, use: --limit @service-pack-6.2.x.retry\n\nPLAY RECAP *********************************************************************\nGoldenBoy                  : ok=0    changed=0    unreachable=0    failed=1\n\n\nWhen I uninstall ansible and reinstall 1.9.4 with no configuration changes setup works.\n**$ pip uninstall ansible\n<< ommitted >>\n$ pip install ansible==1.9.4\n<<ommitted>>\n$ ansible --version\nansible 1.9.4\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = /home/share/library\n$ ansible tigh -m setup\ntigh | success >> {\n    \"ansible_facts\": {\n        \"ansible_all_ipv4_addresses\": [\n...\n}, \n    \"changed\": false\n}\n\nPlaybook works as well\n\n$ ansible-playbook service-pack-6.2.x.yml \n\nPLAY [GoldenBoy] ************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [GoldenBoy]\n\nTASK: [debug var=inventory_hostname] ****************************************** \nok: [GoldenBoy] => {\n    \"var\": {\n        \"inventory_hostname\": \"GoldenBoy\"\n    }\n}\n\nTASK: [debug var=ansible_host] ************************************************ \nok: [GoldenBoy] => {\n    \"var\": {\n        \"ansible_host\": \"ansible_host\"\n    }\n}\n\nTASK: [revert to snapshot] **************************************************** \nchanged: [GoldenBoy -> 127.0.0.1]\n...\n\n\nExpected Results:\nIt uses scp to succeed.\nActual Results:\nIt attempts to connect with SFTP.\n$ ansible GoldenBoy -m setup\nGoldenBoy | FAILED! => {\n    \"failed\": true, \n    \"msg\": \"failed to open a SFTP connection (Channel closed.)\"\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "bcoca"
                },
                "number": 13243,
                "resourcePath": "/ansible/ansible/issues/13243",
                "state": "CLOSED",
                "publishedAt": "2015-11-20T23:59:27Z",
                "closedAt": "2017-07-28T19:26:17Z",
                "title": "Allow multiple vault passwords/files",
                "bodyText": "vault password could keep prompting until empty password is supplied, vault file could take a list of files\nThis allows for having multiple vault files with different keys, good for ops team having access to all vaults but qa or dev having access only to specific ones"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "mpdehaan"
                },
                "number": 1461,
                "resourcePath": "/ansible/ansible/issues/1461",
                "state": "CLOSED",
                "publishedAt": "2012-10-26T23:55:31Z",
                "closedAt": "2013-02-23T18:24:58Z",
                "title": "Let the inventory file location be a directory",
                "bodyText": "If the file is a directory, run all items within it, whether script or INI file, and blend the results.\nThis will allow the inventory directory to be used in conf.d form, or even one group per file.\nIt will also allow for hybrid EC2/local inventory, etc."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "luiseterc"
                },
                "number": 10471,
                "resourcePath": "/ansible/ansible/issues/10471",
                "state": "CLOSED",
                "publishedAt": "2015-03-16T12:10:10Z",
                "closedAt": "2015-03-18T00:50:54Z",
                "title": "Apt module does not work well when specifying version along with with_items loop",
                "bodyText": "When trying to install several packages with this task:\n- name: Install deb packages\n  apt: name={{ item }}=1.0*\n       update_cache=yes\n       state=present\n       force=yes\n  with_items:\n    - package1\n    - package2\n\nOnly package2 (last one in the loop) is forced to install with 1.0 version. Package1 is installed with the latest version found on the repository. Running the playbook with \"-vvvv\" I can se how the command is eventually executed:\nREMOTE_MODULE apt name=package1,package2=1.0* update_cache=yes state=present force=yes\nI got it working by rewriting the task as following:\n- name: Install deb packages\n  apt: name={{ item }}\n       update_cache=yes\n       state=present\n       force=yes\n  with_items:\n    - package1=1.0*\n    - package2=1.0*"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "agaffney"
                },
                "number": 13161,
                "resourcePath": "/ansible/ansible/issues/13161",
                "state": "CLOSED",
                "publishedAt": "2015-11-13T19:35:36Z",
                "closedAt": "2015-11-13T22:39:54Z",
                "title": "Debug task is run even though dependent task is skipped",
                "bodyText": "I found a case in ansible 1.9.4 (also present in 1.9.2) where a debug task with when: whatever|success runs even though the task that registers whatever was skipped.\nI was able to reproduce with this playbook:\n- hosts: all\n  user: cloud-user\n\n  tasks:\n    - name: command that always fails\n      shell: /bin/false\n      ignore_errors: yes\n      register: false_command\n\n    - name: dummy git task\n      git: accept_hostkey=yes\n      when: false_command|success\n      register: dummy_git\n\n    - name: dummy debug\n      debug:\n        msg: \"This should never happen\"\n      when: dummy_git|success\n\nwhich results in the following output:\n$ ansible-playbook -i 10.60.3.31, ansible_problem.yml \n\nPLAY [all] ******************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [10.60.3.31]\n\nTASK: [command that always fails] ********************************************* \nfailed: [10.60.3.31] => {\"changed\": true, \"cmd\": \"/bin/false\", \"delta\": \"0:00:00.002888\", \"end\": \"2015-11-13 11:26:25.460150\", \"rc\": 1, \"start\": \"2015-11-13 11:26:25.457262\", \"warnings\": []}\n...ignoring\n\nTASK: [dummy git task] ******************************************************** \nskipping: [10.60.3.31]\n\nTASK: [dummy debug] *********************************************************** \nok: [10.60.3.31] => {\n    \"msg\": \"This should never happen\"\n}\n\nPLAY RECAP ******************************************************************** \n10.60.3.31                 : ok=3    changed=1    unreachable=0    failed=0"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "wgj"
                },
                "number": 527,
                "resourcePath": "/ansible/ansible/issues/527",
                "state": "CLOSED",
                "publishedAt": "2012-07-02T21:43:58Z",
                "closedAt": "2012-07-02T22:55:18Z",
                "title": "yum: ansible not able to find package",
                "bodyText": "ansible-playbook/yum module isn't able to find a package that I can find manually. This issue 'goes away' if I install the package manually.\n[wes@mgmt001 ~]$ ssh root@host.domain yum clean all\nLoaded plugins: fastestmirror\nCleaning up Everything\nCleaning up list of fastest mirrors\n[wes@mgmt001 ~]$ ssh root@host.domain yum list epel-release\nLoaded plugins: fastestmirror\nLoading mirror speeds from cached hostfile\nAvailable Packages\nepel-release.noarch                        5-4                        private-repository\n[wes@mgmt001 ~]$ ansible-playbook -i /tmp/all_hosts_badtz -f 30 ~/ansible/playbooks/company/company.yml\n\nTASK: [yum: install epel-release] *********************\n\nfailed: [host.domain] => {\"changed\": false, \"failed\": true, \"msg\": \"No Package matching 'epel-release' found available, installed or updated\"}\nFrom playbook:\n\nname: \"yum: install epel-release\"\naction: yum pkg=epel-release state=latest"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "doiim"
                },
                "number": 24844,
                "resourcePath": "/ansible/ansible/issues/24844",
                "state": "CLOSED",
                "publishedAt": "2017-05-19T19:21:13Z",
                "closedAt": "2017-08-02T13:09:54Z",
                "title": "Wrong temp path OSX",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\ncore\nANSIBLE VERSION\nansible 2.3.0.0\n  config file = /Users/mateus/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Apr  4 2017, 08:47:57) [GCC 4.2.1 Compatible Apple LLVM 8.1.0 (clang-802.0.38)]\n\nCONFIGURATION\nask_become_pass=True\nask_sudo_pass=True\nlocal_tmp = /Users/mateus/.ansible/tmp\nOS / ENVIRONMENT\nmacOS 101.12.5\nSUMMARY\nFor some reason Ansible is trying to create a temp file on the wrong directory, since macOS uses /Users/ instead of /home/.\nMy $HOME variable is fine (/Users/mateus/)\nAlready tried setting local_tmp and remote_tmp.\nAs you can see below, the ControlPath is right, only the mkdir is wrong\nACTUAL RESULTS\n<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus\n<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=\"/Users/mateus/.ssh/id_rsa\"' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '\"'\"'echo ~ && sleep 0'\"'\"''\n<[HIDDEN_IP]> (0, '/home/mateus\\n', '')\n<[HIDDEN_IP]> ESTABLISH SSH CONNECTION FOR USER: mateus\n<[HIDDEN_IP]> SSH: EXEC ssh -C -o ControlMaster=auto -o ControlPersist=60s -o 'IdentityFile=\"/Users/mateus/.ssh/id_rsa\"' -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o User=mateus -o ConnectTimeout=10 -o ControlPath=/Users/mateus/.ansible/cp/6cf90d6e66 [HIDDEN_IP] '/bin/sh -c '\"'\"'( umask 77 && mkdir -p \"` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `\" && echo ansible-tmp-1495220924.36-187159279265629=\"` echo /home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629 `\" ) && sleep 0'\"'\"''\n<[HIDDEN_IP]> (1, '', 'mkdir: cannot create directory \\xe2\\x80\\x98/home/mateus/.ansible/tmp/ansible-tmp-1495220924.36-187159279265629\\xe2\\x80\\x99: No space left on device\\n')"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "yew011"
                },
                "number": 14081,
                "resourcePath": "/ansible/ansible/issues/14081",
                "state": "CLOSED",
                "publishedAt": "2016-01-22T19:24:08Z",
                "closedAt": "2016-01-22T22:01:07Z",
                "title": "async with command not working",
                "bodyText": "Issue Type:\n\nbug\n\nAnsible Version:\nansible 2.0.0.2\n\nAnsible Configuration:\ndefault\nEnvironment:\nOS X Yosemite 10.10.4\n\nSummary:\nWhen doing async reboot using ansible, if I call /sbin/shutdown directly, it works.\nHowever if I ran /bin/sleep first then, nothing happens.\nSteps To Reproduce:\n- name: Reboots machine to new kernel (async)\n  command: /bin/sleep 5 && /sbin/shutdown -r now \"Reboot triggered by Ansible\"\n  async: 1\n  poll:  0\n  ignore_errors: true\n\nsyslog:\nJan 22 19:21:44 example ansible-async_wrapper: Starting module and watcher\nJan 22 19:21:44 example ansible-async_wrapper: Start watching 1282 (1)\nJan 22 19:21:44 example ansible-async_wrapper: Start module (1282)\nJan 22 19:21:44 example ansible-command: Invoked with warn=True executable=None chdir=None _raw_params=/bin/sleep 5 && /sbin/shutdown -r now \"Reboot triggered by Ansible\" removes=None creates=None _uses_shell=False\nJan 22 19:21:44 example ansible-async_wrapper: Module complete (1282)"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "hachaboob"
                },
                "number": 17786,
                "resourcePath": "/ansible/ansible/issues/17786",
                "state": "CLOSED",
                "publishedAt": "2016-09-27T23:47:50Z",
                "closedAt": "2016-11-11T13:52:54Z",
                "title": "Allow group_vars and host_vars to override role variables",
                "bodyText": "ISSUE TYPE\n\n\nFeature Idea\n\nCOMPONENT NAME\n\nVariable Precedence\nANSIBLE VERSION\n\nansible 2.1.1.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\n\nN/A\nOS / ENVIRONMENT\n\nN/A\nSUMMARY\n\nChange Ansible's variable precedence so that groups_vars and host_vars can override all role variables. Roles should be considered standalone or isolated modules and maintain there own variable precedence. Roles can use conditions to set OS specific variables in their vars folder. All variables defined in a role should be able to be overridden outside of the role by group_vars and host_vars. This would take care of the use case for being able to set OS specific variable defaults in a role and being able to override them.\nSTEPS TO REPRODUCE\n\n\n\n\n\nEXPECTED RESULTS\n\nACTUAL RESULTS"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "maximede"
                },
                "number": 16775,
                "resourcePath": "/ansible/ansible/issues/16775",
                "state": "CLOSED",
                "publishedAt": "2016-07-20T17:18:49Z",
                "closedAt": "2019-02-22T16:30:38Z",
                "title": "Different behavior for variables used in an included task in a dependent role since 2.0.2.0",
                "bodyText": "ISSUE TYPE\nBug Report\nCOMPONENT NAME\nroles\nANSIBLE VERSION\nansible 2.2.0 (devel d8a3feb976) last updated 2016/07/19 150214 (GMT -700)\nlib/ansible/modules/core (detached HEAD 7de287237f) last updated 2016/07/15 125903 (GMT -700)\nlib/ansible/modules/extras (detached HEAD 68ca157f3b) last updated 2016/07/15 125903 (GMT -700)\nconfig file =\nconfigured module search path = Default w/o overrides\n\nCONFIGURATION\nOS / ENVIRONMENT\nN/A\nSUMMARY\nThis one is really similar to #16729\nWhen a dependency role (configured with allow_duplicates ) uses a variable set by a dependent role inside an included task and multiple role are using the same dependency role, the dependency role always use the variable defined in the last called role\nSTEPS TO REPRODUCE\nCreate a playbook\nAdd two role (role1 and role2) with a dependency on another role configured with allow_duplicates: yes (common-role)\nUse a variable ( app_name) in an included task in the common role which is defined in role1/vars/main.yml and role2/vars/main.yml\nDisplay the var value in a debug task inside the common role\nDirectory structure\n roles\n     common-role\n        meta\n           included.yml\n           main.yml\n        tasks\n            main.yml\n     role1\n        meta\n           main.yml\n        vars\n              main.yml\n     role2\n         meta\n            main.yml\n         vars\n             main.yml\n\nplaybook.yml\n  - hosts: localhost\n    roles:\n      - { role: role1}\n      - { role: role2 }\n\nrole/common-role/meta/main.yml\nallow_duplicates: yes\n\nroles/common-role/tasks/included.yml\n- name: \"debug {{app_name}}\"\n  debug: msg=\"apps_name --- {{app_name}}\"\n\nroles/common-role/tasks/main.yml\n- include: tasks/included.yml\n\nroles/role1/meta/main.yml\ndependencies:\n  - { role: common-role }\n\nroles/role1/vars/main.yml\napp_name: role1AppName\n\nroles/role2/meta/main.yml\ndependencies:\n  - { role: common-role }\n\nroles/role2/vars/main.yml\napp_name: role2AppName\n\nEXPECTED RESULTS\nansible 2.0.1.0 ( and the previous versions) was producing this :\nPLAY ***************************************************************************\nTASK [setup] *******************************************************************\nok: [localhost]\nTASK [common-role : include] ***************************************************\nincluded: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost\nTASK [common-role : debug included role1AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role1AppName\"\n}\nTASK [common-role : include] ***************************************************\nincluded: /Users/maximederavet/Development/temp/inheritancebug-ansible/roles/common-role/tasks/included.yml for localhost\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=5    changed=0    unreachable=0    failed=0\n\nACTUAL RESULTS\nansible 2.0.2+ produces this :\nPLAY [localhost] ***************************************************************\nTASK [setup] *******************************************************************\nok: [localhost]\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nTASK [common-role : debug included role2AppName] *******************************\nok: [localhost] => {\n    \"msg\": \"apps_name included --- role2AppName\"\n}\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=3    changed=0    unreachable=0    failed=0\n\ntested with ansible 2.0.2.0 , the stable-2.1 branch and the devel branch, same results."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "tidzo"
                },
                "number": 25020,
                "resourcePath": "/ansible/ansible/issues/25020",
                "state": "CLOSED",
                "publishedAt": "2017-05-25T10:06:40Z",
                "closedAt": "2018-11-20T19:19:17Z",
                "title": "Wait_for should return matches to groups in its search_regex",
                "bodyText": "ISSUE TYPE\n\nFeature Idea\n\nCOMPONENT NAME\nwait_for\nANSIBLE VERSION\n2.3\n\nCONFIGURATION\nn/a\nOS / ENVIRONMENT\nn/a\nSUMMARY\nWhen using the wait_for module to monitor, say, a file for a string matching a particular search_regex, it would be helpful if the matches to that regex were available in the module's output.\nIn my current use case, I'm monitoring a log file from a remote process which was started asynchronously and want to display certain pertinent information from it in the Ansible output.\nSTEPS TO REPRODUCE\n\n - name: Wait for message in log file\n    wait_for:\n        path: \"somefile.log'\n        search_regex: \"SOMETHING_HAPPENED start_delimiter(.*)end_delimiter\"\n    register: foo\n\n - name: show text between delimiters\n    debug: msg=\"The text between delimiters was {{foo.result.matches[0]}}\"\nEXPECTED RESULTS\nThe wait_for task will pause until text matching the search_regex is found in somefile.log (or timeout occurs).  The text between the parens in the regex will be available somewhere in the variable registered in wait_for task.\nACTUAL RESULTS\nThe matches are not available in the registered variable."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "jctanner"
                },
                "number": 16951,
                "resourcePath": "/ansible/ansible/issues/16951",
                "state": "CLOSED",
                "publishedAt": "2016-08-04T15:09:05Z",
                "closedAt": "2016-08-04T15:45:58Z",
                "title": "broken plugins cause UnboundLocalError: local variable 'obj' referenced before assignment",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nANSIBLE VERSION\n2.2 devel\n\nCONFIGURATION\nN/A\nOS / ENVIRONMENT\nN/A\nSUMMARY\nPluginLoader.all() creates a variable named \"obj\" in try/except, raises a warning if failed and then attempts to use the variable later even though it may not exist.\nSTEPS TO REPRODUCE\nCreate a bad plugin file in \"filter_plugins\" and run a playbook with a debug: var=\nmkdir filter_plugins\ntouch filter_plugins/foobar.py\n\nEXPECTED RESULTS\nNo traceback, just a warning.\nACTUAL RESULTS\ntask path: .../tasks/main.yml:57\n[WARNING]: Skipping plugin (....library/<BROKEMODULE>.py) as it seems to be invalid: 'module' object has no attribute 'FilterModule'\n\nAn exception occurred during task execution. The full traceback is:\nTraceback (most recent call last):\n File \"/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py\", line 124, in run\n   res = self._execute()\n File \"/usr/local/lib/python2.7/dist-packages/ansible/executor/task_executor.py\", line 401, in _execute\n   self._task.post_validate(templar=templar)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/playbook/task.py\", line 246, in post_validate\n   super(Task, self).post_validate(templar)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/playbook/base.py\", line 317, in post_validate\n   value = templar.template(getattr(self, name))\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 358, in template\n   d[k] = self.template(variable[k], preserve_trailing_newlines=preserve_trailing_newlines, fail_on_undefined=fail_on_undefined, overrides=overrides)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 330, in template\n   result = self._do_template(variable, preserve_trailing_newlines=preserve_trailing_newlines, escape_backslashes=escape_backslashes, fail_on_undefined=fail_on_undefined, overrides=overrides)\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 467, in _do_template\n   myenv.filters.update(self._get_filters())\n File \"/usr/local/lib/python2.7/dist-packages/ansible/template/__init__.py\", line 186, in _get_filters\n   plugins = [x for x in self._filter_loader.all()]\n File \"/usr/local/lib/python2.7/dist-packages/ansible/plugins/__init__.py\", line 388, in all\n   obj = obj(*args, **kwargs)\nUnboundLocalError: local variable 'obj' referenced before assignment"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dapf73"
                },
                "number": 23079,
                "resourcePath": "/ansible/ansible/issues/23079",
                "state": "CLOSED",
                "publishedAt": "2017-03-29T16:27:13Z",
                "closedAt": "2017-05-19T10:16:59Z",
                "title": "vmware_guest doesn't create a properly formatted /etc/resolv.conf file",
                "bodyText": "I'm trying Ansible 2.3 RC2\nISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nvmware_guest: customization: dns_servers\nANSIBLE VERSION\n/usr/lib64/python2.6/site-packages/cryptography/__init__.py:26: DeprecationWarning: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of cryptography will drop support for Python 2.6\n  DeprecationWarning\nansible 2.3.0.0\n  config file = /opt/ansible/.ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.6.6 (r266:84292, Aug 18 2016, 15:13:37) [GCC 4.4.7 20120313 (Red Hat 4.4.7-17)]\n\nCONFIGURATION\nforks=10\n\nOS / ENVIRONMENT\nCentos 6.8\n\nSUMMARY\nI want to provision a VMWare VM and  populate /etc/resolv.conf, so I've puth this in my playbook:\n- vmware_guests:\n      customization:\n        domain: \"{{ guest_domain }}\"\n        dns_servers: \"{{ guest_dns_servers }}\"\n        dns_suffix: \"{{ guest_dns_suffix }}\"\n\nAnd defined this variable in a group_vars file\nguest_dns_servers:\n- 10.75.228.65\n  10.75.228.66\n\nBut I get a wrongly formatted /etc/resolv.conf file:\n[ansible@ansclient111 etc]$ cat resolv.conf\nsearch  mydomain.com\nnameserver      10.75.228.65 10.75.228.66\n\nInstead of getting one nameserver line per server, I get all of them in a single line.\nSTEPS TO REPRODUCE\nProvision the VM using the aforementioned playbook/group_vars file\nEXPECTED RESULTS\nGet a resolv.conf file like this:\nsearch  mydomain.com\nnameserver 10.75.228.65\nnameserver 10.75.228.66\n\nACTUAL RESULTS\nGot a resolv.conf file like this:\nsearch  mydomain.com\nnameserver      10.75.228.65 10.75.228.66"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "ruimcfreitas"
                },
                "number": 21725,
                "resourcePath": "/ansible/ansible/issues/21725",
                "state": "CLOSED",
                "publishedAt": "2017-02-21T16:13:01Z",
                "closedAt": "2017-02-21T16:38:23Z",
                "title": "Unable to manage windows server after binding an IIS https site with option \"Require Server Name Identification\"",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\nANSIBLE VERSION\n\nansible 2.2.1.0\n  config file = /home/rui/test/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\n\ninventory      = ./hosts\nOS / ENVIRONMENT\nrunning Ansible from: Linux ubuntu16_Ansible 4.4.0-59-generic\nmanaging: Windows 2016 with IIS\nSUMMARY\nUnable to manage windows server after binding an IIS https site with option \"Require Server Name Identification\"\nSTEPS TO REPRODUCE\n\nOn the Windows 2016 machine there is a IIS web site with binding for https on port 443 using a certificate.\nThe ansible commands and playbooks are able to manage this machine.\nAfter I change the binding and select the option \"Require Server Name Identification\" (required to use a single IP address to service multiple sites with certificates using \"host name\") ansible stops communication with server.\n\nI can only restore communication after I remove the binding and restart the windows server.\n\nansible win2016gui2 -m win_ping -vvvv\n\nEXPECTED RESULTS\nwin2016gui2 | SUCCESS => {\n\"changed\": false,\n\"ping\": \"pong\"\n}\nACTUAL RESULTS\n\n\nUsing /home/rui/test/ansible.cfg as config file\nLoading callback plugin minimal of type stdout, v2.0 from /usr/lib/python2.7/dist-packages/ansible/plugins/callback/__init__.pyc\nUsing module file /usr/lib/python2.7/dist-packages/ansible/modules/core/windows/win_ping.ps1\n<192.168.233.141> ESTABLISH WINRM CONNECTION FOR USER: administrator on PORT 5986 TO 192.168.233.141\nwin2016gui2 | UNREACHABLE! => {\n    \"changed\": false,\n    \"msg\": \"ssl: (\\\"bad handshake: SysCallError(104, 'ECONNRESET')\\\",)\",\n    \"unreachable\": true\n}"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "defionscode"
                },
                "number": 8035,
                "resourcePath": "/ansible/ansible/issues/8035",
                "state": "CLOSED",
                "publishedAt": "2014-07-03T18:22:59Z",
                "closedAt": "2014-07-10T18:29:55Z",
                "title": "ansible_memfree_mb fact combines disk-cache use of memory",
                "bodyText": "Issue Type: Bug Report\nAnsible Version: ansible 1.6.3\nEnvironment: N/A\nSummary:\nI setup a play to alert whenever memory utilization is dangerously high. I immediately received a handful of alerts and was a bit concerned until I realized it takes into account memory being used by the disk cache and not 'real' memory utilization.\nSteps To Reproduce:\nTo reproduce just compare the ansible_memfree_mb fact, which in my example is\n        \"ansible_memfree_mb\": 10550,\n        \"ansible_memtotal_mb\": 15042,\n\nwith the return value of running\nfree -m\n\nExpected Results: I expected 'real' memory available\nActual Results:\nIn my case returns\n             total       used       free     shared    buffers     cached\nMem:         15042       4483      10559        139        231       2859\n-/+ buffers/cache:       1393      13649\nSwap:            0          0          0\n\nAs you can see, the ansible_facts are correct but misleading, in this case saying only 10gb are free when in fact I have 13gb that are useable.\nPerhaps this could be added as a ansible_nocache_memfree_mb fact?"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "slyall"
                },
                "number": 9686,
                "resourcePath": "/ansible/ansible/issues/9686",
                "state": "CLOSED",
                "publishedAt": "2014-12-01T22:48:38Z",
                "closedAt": "2016-02-27T16:37:37Z",
                "title": "Include inventory modules in distribution",
                "bodyText": "Currently inventory modules are only available on github.\nIt would be cool if they could be bundled in with the release so they can be used without needing to be separately downloaded"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "Val"
                },
                "number": 13140,
                "resourcePath": "/ansible/ansible/issues/13140",
                "state": "CLOSED",
                "publishedAt": "2015-11-12T15:08:23Z",
                "closedAt": "2015-11-12T16:09:39Z",
                "title": "Missing `xsltproc` Debian packaging README",
                "bodyText": "packaging/debian/README.md does not include installation of xsltproc which provides missing local eponym command.\nmake deb\n[...]\na2x -D docs/man/man1/ -d manpage -f manpage docs/man/man1/ansible.1.asciidoc\na2x: WARNING: --destination-dir option is only applicable to HTML based outputs\na2x: ERROR: \"xsltproc\"  --stringparam callout.graphics 0 --stringparam navig.graphics 0 --stringparam admon.textlabel 1 --stringparam admon.graphics 0  \"/etc/asciidoc/docbook-xsl/manpage.xsl\" \"<snip>/docs/man/man1/ansible.1.xml\" returned non-zero exit status 127\nMakefile:117: recipe for target 'docs/man/man1/ansible.1' failed\nmake: *** [docs/man/man1/ansible.1] Error 1\nrm docs/man/man1/ansible.1.asciidoc"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "willthames"
                },
                "number": 6545,
                "resourcePath": "/ansible/ansible/issues/6545",
                "state": "CLOSED",
                "publishedAt": "2014-03-18T06:42:32Z",
                "closedAt": "2017-03-13T21:41:26Z",
                "title": "Ansible inventory allows groups to have same name as hosts",
                "bodyText": "Issue Type\nBug Report\nAnsible Version:\nansible 1.6 (devel 9da26da) last updated 2014/03/18 11:15:04 (GMT +1000)\nEnvironment:\nN/A\nSummary:\nIf you define a group containing another group without the :children, then the intended group is a host, but will also become a group when defining its contents.\nThe net result is that the host that should inherit characteristics of several layers of parents will not inherit those characteristics as the definitions will be:\ngrandparent -> parent(host)\nparent(group) -> host\nrather than\ngrandparent -> parent -> host\nI believe that it should not be possible to define a group with the same name as a host (or at least a warning suggesting it might not be what you want should happen)\nSteps To Reproduce:\nSee https://gist.github.com/willthames/9614054\nExpected Results:\nSee https://gist.github.com/willthames/9614054\nActual Results:\nSee https://gist.github.com/willthames/9614054"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "rhaido"
                },
                "number": 5994,
                "resourcePath": "/ansible/ansible/issues/5994",
                "state": "CLOSED",
                "publishedAt": "2014-02-13T13:33:10Z",
                "closedAt": "2014-03-16T15:50:13Z",
                "title": "Inconsistent expansion of the variables in the lookup 'with_items'",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\n\nProduction:\n\n/usr/lib/python2.6/site-packages/ansible # ansible --version\nansible 1.4.1\n\n\nFresh clone (13.02.2014)\n\n /local/home/mike/dev/ansible-latest/ansible # ansible --version\nansible 1.5 (devel 6f405c8970) last updated 2014/02/13 13:14:25 (GMT +200)\n\nEnvironment:\n# uname -a\nLinux xxx.cern.ch 2.6.32-431.3.1.el6.x86_64 #1 SMP Mon Jan 6 11:34:51 CET 2014 x86_64 x86_64 x86_64 GNU/Linux\n\n # lsb_release -a\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\nDistributor ID: ScientificCERNSLC\nDescription:    Scientific Linux CERN SLC release 6.5 (Carbon)\nRelease:    6.5\nCodename:   Carbon\n\nSummary:\nDear Ansible Dev Team,\nWhile migrating code from old ${...} variable expansion to new Jinja2 style {{...}}, I've noticed some incompatibilities with the previous syntax, while I would expect {{...}} be a full equivalent of ${...}, i.e the following case should work just fine:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n     -   - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next }}\n\n\nwhere jdk_next is a list of values:\njdk_pro: 1.7.0_45\njdk_6: 1.6.0_43\njdk_7: 1.7.0_45\njdk_next:\n  - jdk1.7.0_45\n\nUnfortunately, it does not work.\nSteps To Reproduce:\n\nCreate the playbook, which uses roles, for example server.yml:\n\n-\n  hosts: server\n  gather_facts: yes\n  user: root\n  vars:\n    local_to_opt: yes\n  roles:\n    - base\n\nAnd put the mentioned vars in role's \"base\" vars/ and mentioned task in role's \"base\" tasks/, as it should be.\n\nExecute ansible playbook.\n\n ansible-playbook --connection=local server.yml\n\nExpected Results:\nWith previous syntax:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}\n\n- the result is just fine:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   \n\nActual Results:\nWith the code written in Jinja2 style, I've received with the bad result:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nfailed: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']) => {\"failed\": true, \"item\": \"jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']\"}\nmsg: ['jdk1.7.0_45'] packages has been failed for installation;\n\nFATAL: all hosts have already failed -- aborting\n\nPLAY RECAP ******************************************************************** \n           to retry, use: --limit @/root/server.retry\n\n127.0.0.1                  : ok=1    changed=0    unreachable=0    failed=1   \n\nThen, even more interesting: I found, that it's not possible anymore specify variable enclosed in {{..}} as a first member of iteration sequence like this:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }}\n\nThe result is a syntax error:\nERROR: Syntax Error while loading YAML script, /var/lib/ansible/roles/base/tasks/main.yml\nNote: The error may actually appear before this position: line 596, column 22\n\n  with_items:\n    - {{ jdk_next }} jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} ${jdk_next}\n\nWhile the old syntax was ok:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0   \n\nQuick debug & hacking showed the following things:\n\nif you use original anisble syntax, the file expansion string, which arrives in the variable varname of the template() function of utils/template.py, is [u'jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45'], which is correct\nif you use jinja2 syntax, the final expansion string, which arrives in the variable varname of the template() function of utils/template.py is [u\"jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 ['jdk1.7.0_45']\"] which is incorrect.\n\nI do not have comments about SyntaxError I've provided earlier as I have no time to debug it :(\nWorkaround\nThis hack is acceptable but still ugly - the idea is to use the join() Jinja2 filter, i.e. the following code:\n- name: Java JDK installation\n  action: yum2 name=\"{{ item }}\" enablerepo=acc-external-do state=installed\n  with_items:\n    - jdk{{ jdk_pro }} jdk{{ jdk_6 }} jdk{{ jdk_7 }} {{ jdk_next | join(' ') }}\n\nworks:\nPLAY [server] ***************************************************************** \n\nGATHERING FACTS *************************************************************** \nok: [127.0.0.1]\n\nTASK: [base | Java JDK installation] ****************************************** \nok: [127.0.0.1] => (item=jdk1.7.0_45 jdk1.6.0_43 jdk1.7.0_45 jdk1.7.0_45)\n\nPLAY RECAP ******************************************************************** \n127.0.0.1                  : ok=2    changed=0    unreachable=0    failed=0"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dagwieers"
                },
                "number": 1657,
                "resourcePath": "/ansible/ansible/issues/1657",
                "state": "CLOSED",
                "publishedAt": "2012-11-22T02:48:47Z",
                "closedAt": "2012-11-23T16:15:23Z",
                "title": "Implement RHEL5 python-dmidecode support",
                "bodyText": "For older systems lacking sysfs I plan to implement python-dmidecode support in setup as a backup."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "maxwo"
                },
                "number": 24373,
                "resourcePath": "/ansible/ansible/issues/24373",
                "state": "CLOSED",
                "publishedAt": "2017-05-08T13:23:49Z",
                "closedAt": "2018-04-26T21:44:39Z",
                "title": "include_once is skipped when required roles are partially skipped",
                "bodyText": "I use an include_role task in a role, but it is skipped when I use a conditional depency role.\nI tried to find more infos in the documentation, but there is nothing I can find to explain it if I do anything wrong...\nISSUE TYPE\nBug Report\nCOMPONENT NAME\ninclude_role or meta/dependencies\nANSIBLE VERSION\nansible 2.3.0.0\n  config file =\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Dec 18 2016, 07:03:39) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]\n\nCONFIGURATION\nDefault configuration\nOS / ENVIRONMENT\nMac OS X / Debian\nSUMMARY\nI use a include_role task which is skipped because I use conditional required roles.\nSTEPS TO REPRODUCE\nroles/shell/meta/main.yml\n# Role dependencies\ndependencies:\n  - { role: homebrew, when: ansible_os_family == 'Darwin' }\n  - { role: apt, when: ansible_distribution == 'Debian' }\nroles/shell/tasks/main.yml\n---\n# Tasks for roles\n\n- name: Ensure Oh My Zsh required packages are installed\n  package:\n    name: \"{{ item }}\"\n    state: latest\n  become: \"{{ ansible_os_family != 'Darwin' }}\"\n  with_items:\n   - curl\n   - git\n   - zsh\n\n- name: Ensure Zsh is the default shell\n  shell: \"chsh -s /bin/zsh {{ansible_user_id}}\"\n  become: yes\n  register: chsh_result\n  changed_when: \"chsh_result.stderr.find('no changes made') == -1\"\n\n- name: Ensure Oh My Zsh is installed\n  shell: \"sh -c \\\"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\\\"\"\n  args:\n    creates: \"{{ ansible_env.HOME}}/.oh-my-zsh\"\n    executable: /bin/zsh\n\n- name: Ensure auto-suggestions Zsh plugin is installed\n  git:\n    repo: 'https://github.com/zsh-users/zsh-autosuggestions.git'\n    dest: \"{{ ansible_env.HOME }}/.oh-my-zsh/custom/plugins/zsh-autosuggestions\"\n\n- name: Ensure dotfiles are installed\n  include_role:\n    name: dotfiles\nIn the dotfiles role, nothing special, no required role, just some unconditional tasks:\nroles/dotfiles/tasks/main.yml\n- name: Ensure repository is cloned\n  git:\n    repo: 'https://github.com/maxwo/dotfiles.git'\n    dest: \"{{ ansible_env.HOME }}/dotfiles\"\n\n- name: Ensure SSH dotfiles symlinks are set with correct permissions\n  file:\n    src: \"{{ ansible_env.HOME}}/dotfiles/ssh/{{ item }}\"\n    dest: \"{{ ansible_env.HOME }}/.ssh/{{ item }}\"\n    mode: 0600\n    state: link\n    force: yes\n  with_items:\n    - authorized_keys\n    - config\n\n- name: Ensure dotfiles standard symlinks are set\n  file:\n    src: \"{{ ansible_env.HOME }}/dotfiles/{{ item.src }}\"\n    dest: \"{{ ansible_env.HOME }}/{{ item.dest }}\"\n    state: link\n    force: yes\n  with_items:\n    - { src: 'gitconfig', dest: '.gitconfig' }\n    - { src: 'zshrc', dest: '.zshrc' }\nEXPECTED RESULTS\nThe last include_role should be played.\nACTUAL RESULTS\nThe last include_role is skipped, whether it is played on a Debian machine or a Mac OS machine.\nWhen I remove the role dependencies, everything is fine.\nTASK [icopp.homebrew : Install Homebrew] *********************************************************************************************************************************\nok: [aki.local]\n\nTASK [homebrew : Ensure Homebrew packages are up to date] ****************************************************************************************************************\nchanged: [aki.local]\n\nTASK [apt : Ensure APT packages are up to date] **************************************************************************************************************************\nskipping: [aki.local]\n\nTASK [shell : Ensure Oh My Zsh required packages are installed] **********************************************************************************************************\nok: [aki.local] => (item=curl)\nok: [aki.local] => (item=git)\nok: [aki.local] => (item=zsh)\n\nTASK [shell : Ensure Zsh is the default shell] ***************************************************************************************************************************\nok: [aki.local]\n\nTASK [shell : Ensure Oh My Zsh is installed] *****************************************************************************************************************************\nok: [aki.local]\n\nTASK [shell : Ensure auto-suggestions Zsh plugin is installed] ***********************************************************************************************************\nok: [aki.local]\n\nTASK [dotfiles : Ensure repository is cloned] ****************************************************************************************************************************\nskipping: [aki.local]\n\nTASK [dotfiles : Ensure SSH dotfiles symlinks are set with correct permissions] ******************************************************************************************\nskipping: [aki.local] => (item=authorized_keys)\nskipping: [aki.local] => (item=config)\n\nTASK [dotfiles : Ensure dotfiles standard symlinks are set] **************************************************************************************************************\nskipping: [aki.local] => (item={u'dest': u'.gitconfig', u'src': u'gitconfig'})\nskipping: [aki.local] => (item={u'dest': u'.zshrc', u'src': u'zshrc'})"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dan-mcdonald"
                },
                "number": 8547,
                "resourcePath": "/ansible/ansible/issues/8547",
                "state": "CLOSED",
                "publishedAt": "2014-08-11T16:44:19Z",
                "closedAt": "2014-09-29T20:21:30Z",
                "title": "`postgres_user` module doesn't work with AWS RDS databases",
                "bodyText": "Issue Type:\nBug report\nAnsible Version:\nansible 1.7.0\nEnvironment:\nOSX Mavericks 10.9.4\nSummary:\nWhen running against an AWS RDS Postgresql instance, the postgres_user module can't set up new users.\nSteps To Reproduce:\nRun this task:\n- name: Ensure user has access to the database\n  postgresql_user: login_host={{ db_host }}\n                   port={{ db_port }}\n                   login_user={{ db_admin_user }}\n                   login_password={{ db_admin_password }}\n                   db={{ db_name }}\n                   name={{ db_user }}\n                   password={{ db_password }}\n                   priv=ALL\n                   state=present \n\nExpected Results:\nok: [...] => {\"changed\": true, \"db\": \"database\"}\n\nActual Results:\nfailed: [...] => {\"failed\": true, \"parsed\": false}\ninvalid output was: SUDO-SUCCESS-qwpjepgvenunnlewzwjddpnrbfjyptxo\nTraceback (most recent call last):\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 1869, in <module>\n    main()\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 497, in main\n    changed = user_alter(cursor, module, user, password, role_attr_flags, encrypted, expires)\n  File \"/home/ubuntu/.ansible/tmp/ansible-tmp-1407775799.05-32923950289495/postgresql_user\", line 201, in user_alter\n    cursor.execute(select, {\"user\": user})\npsycopg2.ProgrammingError: permission denied for relation pg_authid\n\nApparently the pg_authid relation is not available in RDS.\nPossible workaround: if access denied for pg_authid, then always set the password."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "walterdolce"
                },
                "number": 22411,
                "resourcePath": "/ansible/ansible/issues/22411",
                "state": "CLOSED",
                "publishedAt": "2017-03-08T15:08:36Z",
                "closedAt": "2018-01-18T21:08:34Z",
                "title": "letsencrypt module does not create any cert when using DNS-based validation",
                "bodyText": "ISSUE TYPE\nBug Report\nCOMPONENT NAME\n\nletsencrypt\nANSIBLE VERSION\nAnsible version is 2.2.1.0\nSUMMARY\nIt looks like the Let's Encrypt Ansible module doesn't create any cert when running with the DNS-based validation.\nSTEPS TO REPRODUCE\n# vars\n---\naws:\n   access_key: \"some_access_key\"\n   secret_key: \"some_secret_key\"\nletsencrypt_account_email: \"some_email@example.com\"\nletsencrypt_key_filename: \"some_filename\"\nletsencrypt_dir: \"some/path\"\nletsencrypt_domains:\n   - \"foo\"\n   - \"bar\"\n\n# tasks\n---\n- name: Install Route53 dependencies\n  pip:\n    name: boto\n\n- name: Create folder\n  file:\n    path: \"{{ letsencrypt_dir }}\"\n    owner: root\n    group: root\n    mode: 0755\n    state: directory\n\n- name: Create key\n  shell: 'ssh-keygen -t rsa -b 2048 -C \"{{ letsencrypt_account_email }}\" -f ~/.ssh/{{ letsencrypt_key_filename }} -q -N \"\"'\n  args:\n    creates: \"~/.ssh/{{ letsencrypt_key_filename }}\"\n\n- name: Create domain key\n  shell: 'ssh-keygen -t rsa -b 2048 -C \"{{ letsencrypt_account_email }}\" -f ~/.ssh/id_rsa.{{ item }} -q -N \"\"'\n  args:\n    creates: '~/.ssh/id_rsa.{{ item }}'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create CSR (Certificate Signing Request)\n  shell: 'openssl req -new -nodes -key ~/.ssh/{{ letsencrypt_key_filename }} -out {{ letsencrypt_dir }}/{{ item }}.csr -subj \"/CN={{ item }}\"'\n  args:\n    creates: '{{ letsencrypt_dir }}/{{ item }}.csr'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create challenge\n  letsencrypt:\n    account_key: '~/.ssh/id_rsa.{{ item }}'\n    challenge: dns-01\n    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'\n    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'\n    remaining_days: 20\n  register: letsencrypt_challenge\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Create Route53 TXT record for DNS-based certificate validation\n  route53:\n    command: create\n    aws_access_key: \"{{ aws.access_key_id }}\"\n    aws_secret_key: \"{{ aws.secret_access_key }}\"\n    zone: 'some_hosted_zone.com'\n    record: \"{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com\"\n    retry_interval: 300\n    type: TXT\n    ttl: 7200\n    value: '\"{{ item.1.challenge_data[item.0][\"dns-01\"][\"resource_value\"] }}\"'\n    wait: yes\n  when: \"letsencrypt_challenge|changed\"\n  with_together:\n    - \"{{ letsencrypt_domains }}\"\n    - \"{{ letsencrypt_challenge['results'] }}\"\n  ignore_errors: yes\n\n- name: Validate challenge\n  letsencrypt:\n    account_key: '~/.ssh/id_rsa.{{ item }}'\n    challenge: dns-01\n    csr: '{{ letsencrypt_dir }}/{{ item }}.csr'\n    dest: '{{ letsencrypt_dir }}/{{ item }}.crt'\n    data: '{{ letsencrypt_challenge }}'\n    remaining_days: 20\n  when: '{{ letsencrypt_challenge|changed }}'\n  with_items: \"{{ letsencrypt_domains }}\"\n\n- name: Delete Route53 TXT record for DNS-based certificate validation\n  route53:\n    command: delete\n    aws_access_key: \"{{ aws.access_key_id }}\"\n    aws_secret_key: \"{{ aws.secret_access_key }}\"\n    zone: 'some_hosted_zone.com'\n    record: \"{{ item.1.challenge_data[item.0]['dns-01']['resource'] }}.some_hosted_zone.com\"\n    retry_interval: 300\n    type: TXT\n    ttl: 7200\n    value: '\"{{ item.1.challenge_data[item.0][\"dns-01\"][\"resource_value\"] }}\"'\n    wait: yes\n  when: \"letsencrypt_challenge|changed\"\n  with_together:\n    - \"{{ letsencrypt_domains }}\"\n    - \"{{ letsencrypt_challenge['results'] }}\"\n  ignore_errors: yes\n\n\nACTUAL RESULTS\nNo errors are being thrown or anything, it provisions just fine\nEXPECTED RESULTS\nThere should be a cert generated"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "hyperized"
                },
                "number": 23579,
                "resourcePath": "/ansible/ansible/issues/23579",
                "state": "CLOSED",
                "publishedAt": "2017-04-13T15:14:48Z",
                "closedAt": "2017-05-11T21:33:17Z",
                "title": "Memory load increased in 2.3.0 compared to 2.2.0.1 (high memory use, high ram use)",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nAnsible\nANSIBLE VERSION\n2.3.0\n\nCONFIGURATION\n[defaults]\nhostfile       = inventory/\nlibrary        = /usr/share/ansible\nremote_tmp     = $HOME/.ansible/tmp\npattern        = *\nforks          = 30\npoll_interval  = 15\nsudo_user      = root\nask_sudo_pass  = True\ntransport      = ssh\nremote_port    = 22\nmodule_lang    = C\nallow_world_readable_tmpfiles = True\ngathering = implicit\nhost_key_checking = False\nstdout_callback = actionable_debug\nsudo_exe = sudo\nsudo_flags = -i\ntimeout = 15\nansible_managed = This file is managed by Ansible.\naction_plugins     = /usr/share/ansible_plugins/action_plugins\ncallback_plugins   = /usr/share/ansible_plugins/callback_plugins:./callback_plugins\nconnection_plugins = /usr/share/ansible_plugins/connection_plugins\nlookup_plugins     = /usr/share/ansible_plugins/lookup_plugins\nvars_plugins       = /usr/share/ansible_plugins/vars_plugins\nfilter_plugins     = /usr/share/ansible_plugins/filter_plugins\nfact_caching = jsonfile\nfact_caching_connection = /tmp/$USER\nretry_files_enabled = False\n\n[privilege_escalation]\n\n[paramiko_connection]\n\n[ssh_connection]\nretries=10\npipelining = False\n\n[accelerate]\n\n[selinux]\n\n[colors]\n\nOS / ENVIRONMENT\nUbuntu 14.04.5 (exclusively) hosts & clients\nSUMMARY\nIncreased memory usage when 2.3.0 is rolled out: http://imgur.com/a/ZgUCe\nRecovery takes place when 2.2.0.1 is reverted.\nSTEPS TO REPRODUCE\n\nInstall Ansible 2.3.0\nRun regular job schedule (6x a day a full run of ~100 roles, several irregular jobs like backups), started from the Rundeck job scheduler over regular SSH.\n\nEXPECTED RESULTS\nSimilar memory usage.\nACTUAL RESULTS\nhttp://imgur.com/a/ZgUCe"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "dghubble"
                },
                "number": 6694,
                "resourcePath": "/ansible/ansible/issues/6694",
                "state": "CLOSED",
                "publishedAt": "2014-03-26T07:39:51Z",
                "closedAt": "2014-04-02T20:09:20Z",
                "title": "Homebrew module fails silently after fresh Homebrew installs",
                "bodyText": "Reproduce (done on a fresh OSX Mavericks install + Command line tools)\n\nRun the homebrew installer script.\nruby -e \"$(curl -fsSL https://raw.github.com/Homebrew/homebrew/go/install)\"\nCompletes. Run brew doctor. Everything looks good.\nNote that the installer does not (and should not) create /usr/local/Cellar, this is created when the first brew package is installed. So right now no Cellar exists.\n\nTry running a simple playbook with localhost inventory:\n\n---\n- name: My playbook\n  user: myusername\n  hosts: all\n  tasks:\n    - name: Install brew packages\n      homebrew: name=ack state=present\n\nansible-playbook myplaybook -i localhost_inventory\nPLAY [My playbook] ******************************************************\nGATHERING FACTS ***************************************************************\nok: [localhost]\nTASK: [Install brew packages] *********************************************\nok: [localhost]\nPLAY RECAP ********************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0\nYou'll get something that completes instantly, seems to indicate ack was installed(ok), but it wasn't. No Cellar exists. ack is seriously not installed.\nNow go ahead and make the /usr/local/Cellar directory. Run the config again. Boom, suddenly it actually works. There is a noticeable delay as ack is installed and ack immediately works after the playbook completes.\nTry deleting the Cellar completely and you're back to the original broken behavior.\nA clean install seems like a pretty simple edge case that should be supported. What's up with this? Lemme know if I'm going crazy."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "nirik"
                },
                "number": 6228,
                "resourcePath": "/ansible/ansible/issues/6228",
                "state": "CLOSED",
                "publishedAt": "2014-03-01T00:11:15Z",
                "closedAt": "2014-03-01T00:17:09Z",
                "title": "ansible 1.5 breaks gather_facts+accelerate",
                "bodyText": "Issue Type:·\nBug report\nAnsible Version:·\nansible 1.5\nEnvironment:\nBoth RHEL6 and Fedora rawhide.·\nSummary:\nAny plays with both:·\ngather_facts: true\naccelerate: true\nFail with:·\nPLAY [foo] ********************************************************************·\nGATHERING FACTS ***************************************************************·\nfatal: [td] => Incorrect permissions on ACCELERATE_KEYS_FILE (/home/kevin/.fireball.keys/td)\nansible never actually connects to the host that I can tell,·\nperhaps it's thinking accelerate is already started when it's not?\nThe directory it refers to doesn't exist (since it never connected to the host)\nSteps To Reproduce:\nCreate a playbook with:·\ngather_facts: true\naccelerate: true\nand at least one host. Run it.·\nExpected Results:\nFacts are gathered and rest of playbook runs.·\nActual Results:\nAnsible-playbook fails at gathering facts and errors out."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "Yajo"
                },
                "number": 24295,
                "resourcePath": "/ansible/ansible/issues/24295",
                "state": "CLOSED",
                "publishedAt": "2017-05-04T13:23:30Z",
                "closedAt": "2018-07-23T07:10:45Z",
                "title": "Clearing facts (to refresh them) does not work",
                "bodyText": "ISSUE TYPE\n\n\nBug Report\n\nCOMPONENT NAME\n\n\nmeta\nsetup\n\nANSIBLE VERSION\n\nansible 2.3.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, Jan 12 2017, 17:59:37) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]\n\nCONFIGURATION\n\nNone.\nOS / ENVIRONMENT\n\n➤ lsb_release -a\nLSB Version:\t:core-4.1-amd64:core-4.1-noarch\nDistributor ID:\tFedora\nDescription:\tFedora release 25 (Twenty Five)\nRelease:\t25\nCodename:\tTwentyFive\n\nSUMMARY\n\nIf you need to reload some facts (in my case I need the VPN IP after I configured it), it does not work\nSTEPS TO REPRODUCE\n\nOption A:\n\n- name: vpn client must be running\n  service:\n    name: openvpn@example\n    enabled: true\n    state: started\n\n- name: refresh facts\n  setup:\nOption B:\n- name: vpn client must be running\n  service:\n    name: openvpn@example\n    enabled: true\n    state: started\n\n- name: refresh facts\n  meta: clear_facts\n\nEXPECTED RESULTS\n\nFacts reloaded, VPN ip data available.\nACTUAL RESULTS\n\n\nOption A:\nTASK [vpn-client : refresh facts] ********************************************************************************************************************\nok: [server.example.com]\n\n... but later another task fails with:\nfatal: [server.example.com]: FAILED! => {\"changed\": false, \"failed\": true, \"msg\": \"AnsibleUndefinedVariable: 'ansible_tun0' is undefined\"}\n\nOption B:\nTASK [vpn-client : refresh facts] *********************************************************************************************************\nfatal: [server.example.com]: FAILED! => {\"changed\": false, \"failed\": true, \"module_stderr\": \"Shared connection to server.example.com closed.\\r\\n\", \"module_stdout\": \"\", \"msg\": \"MODULE FAILURE\", \"rc\": 0}\n\nWORKAROUND\nRun the playbook again 😞"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "greg-hellings"
                },
                "number": 26199,
                "resourcePath": "/ansible/ansible/issues/26199",
                "state": "CLOSED",
                "publishedAt": "2017-06-28T20:53:31Z",
                "closedAt": "2017-08-29T00:58:58Z",
                "title": "jenkins_plugin - incorrect \"changed\" and silent install failures",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\njenkins_plugin\nANSIBLE VERSION\nansible 2.3.1.0\n  config file = /home/ghelling/.ansible.cfg\n  configured module search path = Default w/o overrides\n  python version = 2.7.13 (default, May 10 2017, 20:04:28) [GCC 6.3.1 20161221 (Red Hat 6.3.1-1)]\n\nCONFIGURATION\ndefault configuration\nOS / ENVIRONMENT\nThis seems independent of versions, but I see it when running from Fedora while controlling Fedora, CentOS 7,  and RHEL 7 machines with Jenkins 1.651.3 running on them.\nSUMMARY\nSome plugins, after being installed, are either not installed to the latest version (despite no value being specified for the version) or are not installed at all, despite the module reporting success. On subsequent runs, these same plugins continue to report a changed/updated edition despite there being no change in the version being installed.\nAn example of some plugins where this behavior has been noticed:\n\nantisamy-markup-formatter (version 1.1 installed, despite 1.5 being available)\nscriptler (reports installed, but the plugin fails to be installed)\ndynamic-parameter (same as scriptler)\n\nAnd many others. However, the behavior does not affect all plugins.\nSTEPS TO REPRODUCE\nOn clean CentOS system, run the following playbook: https://gist.github.com/greg-hellings/14f58eb19a4992b910f27e53f63573b4\nEXPECTED RESULTS\nPlugins are installed to the latest version if version is specified as latest.\nSpurious \"changed\" values are not reported from the module when nothing gets updated.\nThe module errors when a plugin install error occurs.\nACTUAL RESULTS\nNo version specified results in both plugins reporting back \"changed\" when neither the version updates (antisamy-markup-formatter) or the plugin is not installed at all (scriptler).\nWith the line version: latest added to that final task in the sample file, the scriptler install fails while the antisamy-markup-formatter still reports \"changed\" without actually updating anything."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "brandond"
                },
                "number": 22471,
                "resourcePath": "/ansible/ansible/issues/22471",
                "state": "CLOSED",
                "publishedAt": "2017-03-10T00:47:19Z",
                "closedAt": "2017-08-22T15:11:38Z",
                "title": "ec2_group: add tags",
                "bodyText": "From @jbrockett on December 4, 2014 15:15\nIssue Type:\nFeature Idea\nComponent name:\n\nec2_group\nAnsible Version:\nansible 2.3\nEnvironment:\nN/A\nSummary:\nPlease add the ability to create and modify tags associated with the security group.  At least being able to set the Name tag would be helpful."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "oppianmatt"
                },
                "number": 9456,
                "resourcePath": "/ansible/ansible/issues/9456",
                "state": "CLOSED",
                "publishedAt": "2014-10-30T11:45:51Z",
                "closedAt": "2014-10-30T11:51:00Z",
                "title": "service: nginx enabled=yes not enabling service ansible 1.7.2",
                "bodyText": "Issue Type:\nBug Report\nAnsible Version:\n1.7.2\n1.6.6\nEnvironment:\nDestination: Ubuntu 12.04.4 LTS\nControl hosts: Mac OSX and Ubuntu 12.04.4 LTS\nSummary:\nWhen trying to enable nginx, which is a typical init.d startup script. It appears to not have any affect.\nHave tried with both ansible versions and with Mac OSX and Ubuntu 12.04 control hosts. Destination host is a Ubuntu 12.04\nSteps To Reproduce:\nInstall nginx from deb and it will create a /etc/init.d/nginx script\nIn a task write:\n  - name: start nginx on boot\n    service: name=nginx state=started enabled=yes\n    tags: nginx_boot\n\nRun the playbook.\nansible-playbook site.yaml -i inventory/prod -l host02.example.com -t nginx_boot\n\nExpected Results:\nService enabled\nActual Results:\nReports changed every time:\nTASK: [nginx | start nginx on boot] *******************************************\nchanged: [host02.example.com]\n\nchkconfig and rc.d directories unchanged:\nroot@host02:/etc# chkconfig nginx\nnginx  off\n\nroot@host02:/etc# ls rc*/*nginx*\nrc0.d/K00nginx  rc1.d/K00nginx  rc2.d/K00nginx  rc3.d/K00nginx  rc4.d/K00nginx  rc5.d/K00nginx  rc6.d/K00nginx"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "peterjanes"
                },
                "number": 20231,
                "resourcePath": "/ansible/ansible/issues/20231",
                "state": "OPEN",
                "publishedAt": "2017-01-13T15:44:50Z",
                "closedAt": null,
                "title": "Include all dependency roles in include search path for role",
                "bodyText": "ISSUE TYPE\n\nFeature Idea\n - Bug Report\n\nCOMPONENT NAME\ninclude\nANSIBLE VERSION\nansible 2.2.0.0\n  config file = /etc/ansible/ansible.cfg\n  configured module search path = Default w/o overrides\n\nCONFIGURATION\nroles_path set to both my local directory and left as default.\nOS / ENVIRONMENT\nFedora 25\nSUMMARY\nFiles present in a parent role's tasks directory are not found by child tasks' include statements.\nSTEPS TO REPRODUCE\nUnzip ansible-test.zip and execute ansible-playbook test.yml. Note that\n- include: roles/parent/tasks/parent-task.yml\nworks, but\n- include: parent-task.yml\nfails.\nEXPECTED RESULTS\nparent-task.yml should be found as if it were part of the child task that inherits from it. This would follow the standard (single) role behaviour where \"Any copy, script, template or include tasks (in the role) can reference files in roles/x/{files,templates,tasks}/ (dir depends on task) without having to path them relatively or absolutely\".\nACTUAL RESULTS\n$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv\nUsing /etc/ansible/ansible.cfg as config file\n [WARNING]: provided hosts list is empty, only localhost is available\n\nstatically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml\n[DEPRECATION WARNING]: Included file '/home/pjanes/ansible-test/parent-task.yml'\n not found, however since this include is not explicitly marked as 'static: \nyes', we will try and include it dynamically later. In the future, this will be \nan error unless 'static: no' is used on the include task. If you do not want \nmissing includes to be considered dynamic, use 'static: yes' on the include or \nset the global ansible.cfg options to make all inclues static for tasks and/or \nhandlers.\nThis feature will be removed in a future release. Deprecation warnings\n can be disabled by setting deprecation_warnings=False in ansible.cfg.\nLoading callback plugin default of type stdout, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc\nLoading callback plugin timestamp of type ek3, v2.0 from /usr/lib/python2.7/site-packages/ansible/plugins/callback/__init__.pyc\n\nPLAYBOOK: test.yml *************************************************************\n1 plays in test.yml\n\nPLAY [localhost] ***************************************************************\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.001)       0:00:00.001 ******** \n=============================================================================== \n\nTASK [child : debug] ***********************************************************\ntask path: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml:2\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.030)       0:00:00.032 ******** \nok: [localhost] => {\n    \"msg\": \"task from parent\"\n}\n\nTASK [child : include] *********************************************************\ntask path: /home/pjanes/ansible-test/roles/child/tasks/main.yml:3\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.010)       0:00:00.042 ******** \nfatal: [localhost]: FAILED! => {\n    \"failed\": true, \n    \"reason\": \"the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable\"\n}\n\tto retry, use: --limit @/home/pjanes/.ansible-retry/test.retry\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=1    changed=0    unreachable=0    failed=1   \n\nFriday 13 January 2017  10:38:41 -0500 (0:00:00.009)       0:00:00.051 ******** \n===============================================================================\n\nAdding static: yes to avoid the warning:\n$ ANSIBLE_NOCOWS=1 ansible-playbook test.yml -vvvv\nUsing /etc/ansible/ansible.cfg as config file\n [WARNING]: provided hosts list is empty, only localhost is available\n\nstatically included: /home/pjanes/ansible-test/roles/parent/tasks/parent-task.yml\nERROR! the file_name '/home/pjanes/ansible-test/parent-task.yml' does not exist, or is not readable"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "abadger"
                },
                "number": 17246,
                "resourcePath": "/ansible/ansible/issues/17246",
                "state": "CLOSED",
                "publishedAt": "2016-08-25T16:30:59Z",
                "closedAt": "2016-09-27T16:30:05Z",
                "title": "Vault bytes <=> text string API",
                "bodyText": "ISSUE TYPE\n\nBug Report\n\nCOMPONENT NAME\nlib/ansible/parsing/vault/init.py\nunittests for vault\nANSIBLE VERSION\n\ndevel\n\nSUMMARY\nThe new vault API is mixing bytes and text in inappropriate ways.\nIn working on enabling unittests for Python3, I came across the fact that vault unittests were skipping a lot of tests depending on whether they were running on python3 or python2.  Closer examination revealed that the tests were either giving different values to the API or receiving different values back depending on the version of Python that we are running on.  This is problematic API design that we need to change before release.\nGood API should follow one of these rules for input and output:\n\nTake bytes and return bytes\n\nThe old vault API attempted to use this strategy for \"internal\" functions.  Many of the functions inside of vault were not called from outside of parsing/vault/init.py.  Since they also dealt with operations involving byte strings (encoding, decoding, hexlifying, writing to disk, etc) it made sense that they dealt solely in bytes.\n\n\nTake text and return text\n\nIt is relatively easy to create this sort of API in python3 as combining text and bytes leads to immediate tracebacks.  In python2, it is easy to mix this up with one of the other strategies below as ascii-only bytes and text strings will combine.  Most of ansible's current API does not follow this strategy because we don't trust that the data coming in is going to be the string type we expect.  As we secure our borders (and with python3 tests throwing errors when these are combined inappropriately) we should be able to move more API to this model.\n\n\nTake either bytes or text, normalize internally, and return text\n\nThis is the strategy that a lot of the old external Vault API was taking.  We didn't trust that the rest of Ansible was properly sending us text strings so as the first step we used to_unicode and to_bytes to make sure that the string we were dealing with was the correct type.\n\n\nTake either bytes or text, normalize internally, and return bytes\n\nOld internal vault API may have taken this strategy.  For internal API we should know that our inputs are only text or only bytes but we may have been paranoid.  Since the data we passed around was often pure-ascii, this couldn't cause tracebacks.\n\n\nTake either bytes or text.  If bytes were input then output bytes.  If text was input then output text\n\nI would not recommend this for any of our Vault API.  You'll see this in some Python stdlib API like os.path.abspath() (a good usage of the strategy) or os.listdir() (a bad usage of the strategy).  It is appropriate when the purpose of the function is a straightforward text transformation and there is a need to give callers a version that works with text and a version that works with bytes.  This is not the case for Vault's internal API (where we can adapt the callers to use just one API) and it is not needed for Vault's external API (where we should probably always be returning text).  This strategy can also be used for functions which are operating inside of a larger \"native string\" data model as the code run on python2 should be taking in bytes and outputting bytes while the code on python3 is taking in text and outputting text.  Note, though, that this strategy does not validate the input or output while technically the native string model could validate that only bytes was accepted on python2 and only text was accepted on python3.\n\n\n\nWhichever strategy is followed, be sure that the same strategy is being applied for both Python2 and Python3.  Most projects (including Ansible) are working towards a single code base that runs on both python2 and python3.  Writing API that expects different types and returns different types on one or the other hampers this overall design.\nAlso note, it is tempting to do validation of the strings you receive.  I'd hesitate to do that because all checks have a cost.  Even asserts which are really meant for this purpose invokes the cost because no one really runs ansible in python optimized mode where asserts are stripped out.  My general stance has been if you trust the data coming in, there's no need to validate it.  If you don't trust the data, you should use to_bytes() or to_unicode() to accept either bytes or text and normalize to the type you want to operate on."
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "t2d"
                },
                "number": 1861,
                "resourcePath": "/ansible/ansible/issues/1861",
                "state": "CLOSED",
                "publishedAt": "2013-01-12T13:50:52Z",
                "closedAt": "2013-01-18T23:26:15Z",
                "title": "hacking/module_formatter build error in Debian [0.9]",
                "bodyText": "I tried to build ansible 0.9 on Debian Squeeze, but the make script fails.\n$ sudo aptitude install python-yaml2 python-jinja2 python-paramiko\n$ wget https://github.com/downloads/ansible/ansible/ansible-0.9.tar.gz\n$ tar -xvzf ansible-0.9.tar.gz\n$ cd ansible-0.9\n\n$ make debian\ncat: VERSION: No such file or directory\nfatal: Not a git repository (or any of the parent directories): .git\nCleaning up distutils stuff\nrm -rf build\nrm -rf dist\nCleaning up byte compiled python stuff\nfind . -type f -regex \".*\\.py[co]$\" -delete\nCleaning up editor backup files\nfind . -type f \\( -name \"*~\" -or -name \"#*\" \\) -delete\nfind . -type f \\( -name \"*.swp\" \\) -delete\nCleaning up manpage stuff\nfind ./docs/man -type f -name \"*.xml\" -delete\nfind ./docs/man -type f -name \"*.asciidoc\" -delete\nfind ./docs/man/man3 -type f -name \"*.3\" -delete\nCleaning up output from test runs\nrm -rf test/test_data\nCleaning up RPM building stuff\nrm -rf MANIFEST rpm-build\nCleaning up Debian building stuff\nrm -rf debian\nrm -rf deb-build\nrm -rf docs/json\nrm -rf docs/js\nPYTHONPATH=./lib hacking/module_formatter.py -A  -t man -o docs/man/man3/ --module-dir=library --template-dir=hacking/templates\n/bin/sh: hacking/module_formatter.py: not found\nmake: *** [modulepages] Error 127"
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "nathanhruby"
                },
                "number": 5270,
                "resourcePath": "/ansible/ansible/issues/5270",
                "state": "CLOSED",
                "publishedAt": "2013-12-12T17:56:19Z",
                "closedAt": "2013-12-15T16:33:31Z",
                "title": "Add support for \"build-dep\" to apt module",
                "bodyText": "As a system administrator I want to install build dependency packages for developer and buildhost machines using the same syntax I install normal packages with.\nIt would be useful if one could install build-deps via the apt module instead of mixing and matching apt module and commands for \"apt-get build-dep \""
            }
        }
    },
    {
        "repository": {
            "issue": {
                "author": {
                    "login": "berlic"
                },
                "number": 22988,
                "resourcePath": "/ansible/ansible/issues/22988",
                "state": "CLOSED",
                "publishedAt": "2017-03-27T07:49:11Z",
                "closedAt": "2017-03-29T23:11:20Z",
                "title": "sequence lookup shortcut syntax doesn't work and wrong docs",
                "bodyText": "ISSUE TYPE\n\nBug Report\nDocumentation Report\n\nCOMPONENT NAME\nsequence lookup plugin\nANSIBLE VERSION\nansible 2.2.1.0\n\n2.1, 2.3 and 2.4 have the same bug.\nCONFIGURATION\nStandard config\nOS / ENVIRONMENT\nN/A\nSUMMARY\nwith_sequence shortcut syntax [start-]end[/stride][:format] is not honoured.\nSTEPS TO REPRODUCE\n---\n- hosts: localhost\n  connection: local\n  gather_facts: no\n  tasks:\n    - debug:\n        msg: \"{{ item }}\"\n      with_sequence: '5-6'\n\nEXPECTED RESULTS\nLoop over [5,6]:\nTASK [debug] *******************************************************************\nok: [localhost] => (item=5) => {\n    \"item\": \"5\",\n    \"msg\": \"5\"\n}\nok: [localhost] => (item=6) => {\n    \"item\": \"6\",\n    \"msg\": \"6\"\n}\n\nACTUAL RESULTS\nParameter error:\nTASK [debug] *******************************************************************\nfatal: [localhost]: FAILED! => {\"failed\": true, \"msg\": \"unknown error parsing with_sequence arguments: u'5-6'. Error was: unrecognized arguments to with_sequence: [u'_raw_params']\"}\n\nAlso there is wrong parameters passing in the docs:\n    - user:\n        name: \"{{ item }}\"\n        state: present\n        groups: \"evens\"\n      with_sequence:\n        - start: 0\n        - end: 32\n        - format: testuser%02x\n\nParameters can be passed to with_sequence only as string, not as list or dict."
            }
        }
    }
]